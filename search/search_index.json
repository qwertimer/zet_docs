{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Hi there, welcome, i'm qwertimer, i am a mechatronic engineer working in the embedded linux space. I have a keen interest in Linux, Python, Go and Bash. I love learning new things and enjoy mucking around with homelabs, docker and anything related to linux. Check out my zets for all things related to my learning.","title":"Home"},{"location":"#home","text":"Hi there, welcome, i'm qwertimer, i am a mechatronic engineer working in the embedded linux space. I have a keen interest in Linux, Python, Go and Bash. I love learning new things and enjoy mucking around with homelabs, docker and anything related to linux. Check out my zets for all things related to my learning.","title":"Home"},{"location":"about/","text":"About Github - qwertimer","title":"About"},{"location":"about/#about","text":"Github - qwertimer","title":"About"},{"location":"zettel/","text":"Zettelkasten Welcome to my zettelkasten. My completely unfiltered learnings from across tech and other areas.","title":"Zettelkasten"},{"location":"zettel/#zettelkasten","text":"Welcome to my zettelkasten. My completely unfiltered learnings from across tech and other areas.","title":"Zettelkasten"},{"location":"zet/20210517010658/","text":"Getting started with Zets I needed a way to store all my random tidbits of research and information in a searchable simple setup that can be used cross platform and cross system. The current setup allows me to create zets on sny machine and it automatically pushes to github. If i am on any machine i can pull the git repo and add any new zets when i want. Following RWXRob on his beginner boost and his github repo - https://github.com/rwxrob/dot, I was able to add robs zet program to my path, the other dependencies the setup has is git, and robs program isosec. With this setup i can keep all my notes in one place.","title":"Getting started with Zets"},{"location":"zet/20210517010658/#getting-started-with-zets","text":"I needed a way to store all my random tidbits of research and information in a searchable simple setup that can be used cross platform and cross system. The current setup allows me to create zets on sny machine and it automatically pushes to github. If i am on any machine i can pull the git repo and add any new zets when i want. Following RWXRob on his beginner boost and his github repo - https://github.com/rwxrob/dot, I was able to add robs zet program to my path, the other dependencies the setup has is git, and robs program isosec. With this setup i can keep all my notes in one place.","title":"Getting started with Zets"},{"location":"zet/20210517011245/","text":"Testing git init This file is a test of the git push","title":"Testing git init"},{"location":"zet/20210517011245/#testing-git-init","text":"This file is a test of the git push","title":"Testing git init"},{"location":"zet/20210517012223/","text":"Configuring zet for my systems Working through robs initial zet program i modified the kn to zetdir. This zetdir points to a $HOME/.local/share\\zet location. I have not got any configuration set but it is possible in the .config/zet/config.yml . Once i had the zet program configured i added the folder location to path. I also needed to add isosec to the path however in the future i will bring that into the zet program. The next thing i did was add a symlink to $HOME/repos/github.com/qwertimer/zet to point to the zet location. ln -s ~/.local/share/zet ~/repos/github.com/qwertimer/ Finally with the configuration out of the way i initialised a repo in the zet folder. I used gh to create the github repo and i then ran an initial push. This setup is now configured for me to write zets when needed.","title":"Configuring zet for my systems"},{"location":"zet/20210517012223/#configuring-zet-for-my-systems","text":"Working through robs initial zet program i modified the kn to zetdir. This zetdir points to a $HOME/.local/share\\zet location. I have not got any configuration set but it is possible in the .config/zet/config.yml . Once i had the zet program configured i added the folder location to path. I also needed to add isosec to the path however in the future i will bring that into the zet program. The next thing i did was add a symlink to $HOME/repos/github.com/qwertimer/zet to point to the zet location. ln -s ~/.local/share/zet ~/repos/github.com/qwertimer/ Finally with the configuration out of the way i initialised a repo in the zet folder. I used gh to create the github repo and i then ran an initial push. This setup is now configured for me to write zets when needed.","title":"Configuring zet for my systems"},{"location":"zet/20210518001608/","text":"Working with SSH Often times additional configuration of an SSH communication to obfuscate your connection is time consuming and requires you to set up additional files to remember the configuration. The simplest solution for most of the setups is to disable password authentication, disable root login and enable ssh-key. With these simple configurations most of the work can be done.","title":"Working with SSH"},{"location":"zet/20210518001608/#working-with-ssh","text":"Often times additional configuration of an SSH communication to obfuscate your connection is time consuming and requires you to set up additional files to remember the configuration. The simplest solution for most of the setups is to disable password authentication, disable root login and enable ssh-key. With these simple configurations most of the work can be done.","title":"Working with SSH"},{"location":"zet/20210518002043/","text":"Why build a homelab Linuxserver.io has some great pre built docker images that allow you to do lots of fun things from a home server perspective. For instance you are able to setup a pihole (adblocker), reverse proxy (server protection from the web), duckdns (self hosted dns). The list goes on. The configurations are simple to use when incorporated with rancher (a kubernetes like docker orchestration tool) and heimdall ( a tool for viewing and access each of your services). The next thought in this process is to move the whole configuration to a raspberry pi and see how the configuration works on this.","title":"Why build a homelab"},{"location":"zet/20210518002043/#why-build-a-homelab","text":"Linuxserver.io has some great pre built docker images that allow you to do lots of fun things from a home server perspective. For instance you are able to setup a pihole (adblocker), reverse proxy (server protection from the web), duckdns (self hosted dns). The list goes on. The configurations are simple to use when incorporated with rancher (a kubernetes like docker orchestration tool) and heimdall ( a tool for viewing and access each of your services). The next thought in this process is to move the whole configuration to a raspberry pi and see how the configuration works on this.","title":"Why build a homelab"},{"location":"zet/20210528140148/","text":"Working with vim magics. Vim has an incredible feature built in that rwxrob has coined vimagics. These magics can be accessed using normal mode. To access normal mode we use esc , ctrl [ or jk if it is mapped. When in this mode we can use !! to send the current line to the program in question ie. !!bc will send the current line to the calculator. We can make use of this in many ways like adding the calendar to the current file. May 2021 Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 We can also use it with python, or any other language allowing in line display of file structure or any of these sorts of things. The biggest draw of vimagics is in the use of the !}cmt this comments out the current line. Allowing fast commenting and uncommenting of lines. To read more on vimagics check out rwx.gg/tools/editors/vi/how/magic/ where Rob gives an excellent overview of vimagics.","title":"Working with vim magics."},{"location":"zet/20210528140148/#working-with-vim-magics","text":"Vim has an incredible feature built in that rwxrob has coined vimagics. These magics can be accessed using normal mode. To access normal mode we use esc , ctrl [ or jk if it is mapped. When in this mode we can use !! to send the current line to the program in question ie. !!bc will send the current line to the calculator. We can make use of this in many ways like adding the calendar to the current file. May 2021 Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 We can also use it with python, or any other language allowing in line display of file structure or any of these sorts of things. The biggest draw of vimagics is in the use of the !}cmt this comments out the current line. Allowing fast commenting and uncommenting of lines. To read more on vimagics check out rwx.gg/tools/editors/vi/how/magic/ where Rob gives an excellent overview of vimagics.","title":"Working with vim magics."},{"location":"zet/20210530121615/","text":"Building up a log and todo zet Understanding robs code to allow multiple folders for zets is difficult. I have been working to get this completed. My goal is to use the zets as a way to keep a todo list up to date as well as a private repository for all my logs of each days work. This will allow me to get into the habit of writing up what i did each day ans building a todo list. This is then linked to a github repo that i can create issues and manage using the github kanban board or cli gh tool. With these tools i can create a usable pipeline to complete all my goals and tasks.","title":"Building up a log and todo zet"},{"location":"zet/20210530121615/#building-up-a-log-and-todo-zet","text":"Understanding robs code to allow multiple folders for zets is difficult. I have been working to get this completed. My goal is to use the zets as a way to keep a todo list up to date as well as a private repository for all my logs of each days work. This will allow me to get into the habit of writing up what i did each day ans building a todo list. This is then linked to a github repo that i can create issues and manage using the github kanban board or cli gh tool. With these tools i can create a usable pipeline to complete all my goals and tasks.","title":"Building up a log and todo zet"},{"location":"zet/20210530135725/","text":"Setting up multiple zets When i am looking to setup new zet repositories i have a couple of things that need doing. For now they are manual but could possibly be automated. The first is to create the folder in ~/.local/share/<folder> . Once the folder is created i link it to my repositories folder to make it easier to find all my stuff. This folder is stored at ~/repos/github.com/qwertimer . To link the new zets i run ln -s ~/.local/share/<folder> ~/repos/github.com/qwertimer/ . This adds the link to my main repos location. From here i cd into the new zet folder run git init and set the origin with git add origin https://www.github.com/qwertimer/<folder>.git if i haven't set up a repo i am able to do it from the command line using gh with this i can run gh repo create . Once completed i need to symlink the zet program to my new zet repository. This is done by moving into my scripts folder which is stored in my .dotfiles folder. By symlinking to a new name i am able to run that command and open the zet for that folder. For example linking zet to todo, when i run todo in the command line i begin writing a zet for that repo. This configuration makes it very easy to setup new zets and start storing everything on github.","title":"Setting up multiple zets"},{"location":"zet/20210530135725/#setting-up-multiple-zets","text":"When i am looking to setup new zet repositories i have a couple of things that need doing. For now they are manual but could possibly be automated. The first is to create the folder in ~/.local/share/<folder> . Once the folder is created i link it to my repositories folder to make it easier to find all my stuff. This folder is stored at ~/repos/github.com/qwertimer . To link the new zets i run ln -s ~/.local/share/<folder> ~/repos/github.com/qwertimer/ . This adds the link to my main repos location. From here i cd into the new zet folder run git init and set the origin with git add origin https://www.github.com/qwertimer/<folder>.git if i haven't set up a repo i am able to do it from the command line using gh with this i can run gh repo create . Once completed i need to symlink the zet program to my new zet repository. This is done by moving into my scripts folder which is stored in my .dotfiles folder. By symlinking to a new name i am able to run that command and open the zet for that folder. For example linking zet to todo, when i run todo in the command line i begin writing a zet for that repo. This configuration makes it very easy to setup new zets and start storing everything on github.","title":"Setting up multiple zets"},{"location":"zet/20210531004017/","text":"Working with Lynx Lynx is such a powerful web browser built into the CLI. It makes it super fast and simple to search for a question whilst in the terminal. One of the great things i found today is that after searching for a question you can use the comma key to open the current page in firefox. This allows the extension from written text to view the graphics in firefox. This simple workflow optimisation is incredibly useful.","title":"Working with Lynx"},{"location":"zet/20210531004017/#working-with-lynx","text":"Lynx is such a powerful web browser built into the CLI. It makes it super fast and simple to search for a question whilst in the terminal. One of the great things i found today is that after searching for a question you can use the comma key to open the current page in firefox. This allows the extension from written text to view the graphics in firefox. This simple workflow optimisation is incredibly useful.","title":"Working with Lynx"},{"location":"zet/20210531004309/","text":"TMUX key bindings for fast movement TMUX has many great keybindings. The simplest ones to keep in mind to begin with are ctrl-b starts command mode. Then w shows all open panes, ? will show available commands, c starts a new tab. <0-9> moves to that window, % splits the pane horizontally. , Will allow the change of tab names. There are many more commands available but this is a quick primer for use later.","title":"TMUX key bindings for fast movement"},{"location":"zet/20210531004309/#tmux-key-bindings-for-fast-movement","text":"TMUX has many great keybindings. The simplest ones to keep in mind to begin with are ctrl-b starts command mode. Then w shows all open panes, ? will show available commands, c starts a new tab. <0-9> moves to that window, % splits the pane horizontally. , Will allow the change of tab names. There are many more commands available but this is a quick primer for use later.","title":"TMUX key bindings for fast movement"},{"location":"zet/20210531005226/","text":"TMUX sessions and configuring startup scripts. TMUX has some powerful features available for use in the form of startup scripts. These allow the user to configure a shell script to set a tmux session and open panes for each workspace and folder that is needed for that particular project. When used alongside VIM this setup allows docmentation, coding and note taking to be done all in the same terminal session. This design allows the port of a startup script to be added to a remote container or system that is called when remotely logged in. The simple configuration can be done with a few commands. The first command is: tmux new-session -s <session-name> -n <new-window-name> -d We create a new session and name it using the -s flag. Then we create a new window with the -n flag, finally we detach from it whilst configuring the rest of the init. Next we can add additional windows/tabs with tmux new-window -t <session-name> -d -n <window-name> simply put we add a new window to <session-name> detach with the -d flag and name the window what ever we wish with -n <window-name> We are then able to send commands to the windows such as running a python script with: tmux send-keys -t <session-name>:<window-name> \"python3 hello-world.py\" Enter which starts our python script on the <window-name> tab. We are also able to call the splitting of windows if we wish with: tmux split-window -h or tmux split-window -v , This however is done after we select which window we are working on with: tmux select-window -t <session-name>:<window-name> Finally when the script is fully written we add: tmux -u attach -t <session-name> to start the tmux session. WARNING: It is important to note that if you rerun the script multiple times it will embed the new windows in the previous session. It will not create a new session. This may cause issues with the running scripts so be careful.","title":"TMUX sessions and configuring startup scripts."},{"location":"zet/20210531005226/#tmux-sessions-and-configuring-startup-scripts","text":"TMUX has some powerful features available for use in the form of startup scripts. These allow the user to configure a shell script to set a tmux session and open panes for each workspace and folder that is needed for that particular project. When used alongside VIM this setup allows docmentation, coding and note taking to be done all in the same terminal session. This design allows the port of a startup script to be added to a remote container or system that is called when remotely logged in. The simple configuration can be done with a few commands. The first command is: tmux new-session -s <session-name> -n <new-window-name> -d We create a new session and name it using the -s flag. Then we create a new window with the -n flag, finally we detach from it whilst configuring the rest of the init. Next we can add additional windows/tabs with tmux new-window -t <session-name> -d -n <window-name> simply put we add a new window to <session-name> detach with the -d flag and name the window what ever we wish with -n <window-name> We are then able to send commands to the windows such as running a python script with: tmux send-keys -t <session-name>:<window-name> \"python3 hello-world.py\" Enter which starts our python script on the <window-name> tab. We are also able to call the splitting of windows if we wish with: tmux split-window -h or tmux split-window -v , This however is done after we select which window we are working on with: tmux select-window -t <session-name>:<window-name> Finally when the script is fully written we add: tmux -u attach -t <session-name> to start the tmux session. WARNING: It is important to note that if you rerun the script multiple times it will embed the new windows in the previous session. It will not create a new session. This may cause issues with the running scripts so be careful.","title":"TMUX sessions and configuring startup scripts."},{"location":"zet/20210602012826/","text":"Blue eyes are strange Unrelated to computing but today i read about the rarity of Blue pigment in the world and how almost all blue comes from microstructures in the material reflecting back the blue light. This brought me to the question how are blue eyes made. What is striking with this is that blue eyes are caused by there being no pigment in the stroma and so the blue is reflected just like other species. What is interesting though is that blue eyes are unique in that your eye colour depends on the amount of light in the environment and can change throughout the day. Green or hazel eyes are caused by having light brown pigment in your eyes and can often vary throughout the iris. This is why hazel and green eyed people have such variation in their eye design. END unrelated post about eyes","title":"Blue eyes are strange"},{"location":"zet/20210602012826/#blue-eyes-are-strange","text":"Unrelated to computing but today i read about the rarity of Blue pigment in the world and how almost all blue comes from microstructures in the material reflecting back the blue light. This brought me to the question how are blue eyes made. What is striking with this is that blue eyes are caused by there being no pigment in the stroma and so the blue is reflected just like other species. What is interesting though is that blue eyes are unique in that your eye colour depends on the amount of light in the environment and can change throughout the day. Green or hazel eyes are caused by having light brown pigment in your eyes and can often vary throughout the iris. This is why hazel and green eyed people have such variation in their eye design. END unrelated post about eyes","title":"Blue eyes are strange"},{"location":"zet/20210602032711/","text":"Learning some bash - Part 1 These notes are mostly from Day 14 - 16 of the beginner boost from Rob... In bash you are able to run any command found in path. This is very important and you should never overwrite a linux program with your own names. This was seen when rob created in the past a program called main (changing to main git branch). When he tried to show what would happen if you moved the main call above the subprocess (functions) it tried to call the system main program and failed (VERY DANGEROUS). Other things of note. $1 $2 etc are used to move variables around the program. $1 will be the fist passed variable and so on. You can use $# to get the total number of variables that are passed at once. You can use $@ To get \"first\" \"second\" ... and never use $* unless you are meaning to concatenate all the values into one arguement string. You are able to return strings with a work around where you echo the result of what ever the script is and using $( ) you are able to capture the result of this and pass it on to other locations.","title":"Learning some bash - Part 1"},{"location":"zet/20210602032711/#learning-some-bash-part-1","text":"These notes are mostly from Day 14 - 16 of the beginner boost from Rob... In bash you are able to run any command found in path. This is very important and you should never overwrite a linux program with your own names. This was seen when rob created in the past a program called main (changing to main git branch). When he tried to show what would happen if you moved the main call above the subprocess (functions) it tried to call the system main program and failed (VERY DANGEROUS). Other things of note. $1 $2 etc are used to move variables around the program. $1 will be the fist passed variable and so on. You can use $# to get the total number of variables that are passed at once. You can use $@ To get \"first\" \"second\" ... and never use $* unless you are meaning to concatenate all the values into one arguement string. You are able to return strings with a work around where you echo the result of what ever the script is and using $( ) you are able to capture the result of this and pass it on to other locations.","title":"Learning some bash - Part 1"},{"location":"zet/20210602033401/","text":"Learning Bash - Part 2 This is mainly from Day 17 of the Boosts. - NOTE: Always use \"\" around your passing variables ... This stops nefarious code being run ?? -- check this. >/dev/null -- is used when you dont want to see the output. It sends output to dev null. >/dev/null 2>&1 -- sends the stderr to dev null In bash you can use &>/dev/null which is the same as the above command. One of robs great scripts is https://github.com/rwxrob/dot/blob/main/scripts/newx. This script creates a file, adds it to the scripts folder and adds to path. Then opens in vim. Try not to use else in any code. Place a return to return early. test is a language built in that returns true or false. --- there are many additional switches that are useful. check more man test elf is a compiled shell script. Really interesting thing is if you are checking for a file rather than outputting to stdout and possibly messing up logs. send it to stderr and it will still print to screen. example: if test $# -ne 1; then echo \"usage: has <path>\" >&2 exit 1 fi Note: robs cmt command is able to cmt specific lines using -- ctrl [ to go to normal mode, then !{ sets to run a command. Then he writes cmt . I need to learn how to select the write length of commenting. for loops are great in the command line.","title":"Learning Bash - Part 2"},{"location":"zet/20210602033401/#learning-bash-part-2","text":"This is mainly from Day 17 of the Boosts. - NOTE: Always use \"\" around your passing variables ... This stops nefarious code being run ?? -- check this. >/dev/null -- is used when you dont want to see the output. It sends output to dev null. >/dev/null 2>&1 -- sends the stderr to dev null In bash you can use &>/dev/null which is the same as the above command. One of robs great scripts is https://github.com/rwxrob/dot/blob/main/scripts/newx. This script creates a file, adds it to the scripts folder and adds to path. Then opens in vim. Try not to use else in any code. Place a return to return early. test is a language built in that returns true or false. --- there are many additional switches that are useful. check more man test elf is a compiled shell script. Really interesting thing is if you are checking for a file rather than outputting to stdout and possibly messing up logs. send it to stderr and it will still print to screen. example: if test $# -ne 1; then echo \"usage: has <path>\" >&2 exit 1 fi Note: robs cmt command is able to cmt specific lines using -- ctrl [ to go to normal mode, then !{ sets to run a command. Then he writes cmt . I need to learn how to select the write length of commenting. for loops are great in the command line.","title":"Learning Bash - Part 2"},{"location":"zet/20210603121623/","text":"Creating switches in shell When you want to add additional functionality to a script with flags or switches the simple solution is to read all the values in then use a case statement to create the fuctionality for the particular case. This setup is useful in adding command line arguments to modify the main functionality at run time.","title":"Creating switches in shell"},{"location":"zet/20210603121623/#creating-switches-in-shell","text":"When you want to add additional functionality to a script with flags or switches the simple solution is to read all the values in then use a case statement to create the fuctionality for the particular case. This setup is useful in adding command line arguments to modify the main functionality at run time.","title":"Creating switches in shell"},{"location":"zet/20210603232931/","text":"Building Jetson clusters There are a lot of tools out there to build small scale clusters such as a rpi cluster or a jetson cluster. The company seeed studio has a cluster board that would be excellent for dev testing and kubernetes learning. This blog post details setting up a jetson cluster with kubernetes. It also looks at running tensorflow on the cluster. This design could be utilised to test ML algorithms on the edge with compute nodes. https://medium.com/jit-team/building-a-gpu-enabled-kubernets-cluster-for-machine-learning-with-nvidia-jetson-nano-7b67de74172a. Further research into a board cluster is needed as the only available cluster that looks good is the jetson mate from seeed studio. This however has no m.2 slot for fast drive access. Jeff Geerling also has some articles and youtube videos on clusters and ansible for fast setup. These should be looked into...","title":"Building Jetson clusters"},{"location":"zet/20210603232931/#building-jetson-clusters","text":"There are a lot of tools out there to build small scale clusters such as a rpi cluster or a jetson cluster. The company seeed studio has a cluster board that would be excellent for dev testing and kubernetes learning. This blog post details setting up a jetson cluster with kubernetes. It also looks at running tensorflow on the cluster. This design could be utilised to test ML algorithms on the edge with compute nodes. https://medium.com/jit-team/building-a-gpu-enabled-kubernets-cluster-for-machine-learning-with-nvidia-jetson-nano-7b67de74172a. Further research into a board cluster is needed as the only available cluster that looks good is the jetson mate from seeed studio. This however has no m.2 slot for fast drive access. Jeff Geerling also has some articles and youtube videos on clusters and ansible for fast setup. These should be looked into...","title":"Building Jetson clusters"},{"location":"zet/20210605125400/","text":"Getting IRC running with weechat. Weechat provides an excellent command line interface for numerous chat groups. Notably it provides an excellent setup for a chat client to interface to the twitch community. This is great to keep the chat going without the video stream. The following zet is basically a short installation guide for future me when i inevitably break something and need to format and reinstall the OS. Installation To install weechat we can run the installation from the command line with sudo apt install weechat Once installed we can begin to configure it. This is used to allow the authentication fo the servers that are used for the chats. Twitch integration TO connect to the twitch server we run /server add twitch irc.chat.twitch.tv This will add the twitch server with a label such as twitch. Once this is done we need to add the oauth key to the server for authentication. To get an oauth key we can go to twitchapps.com/tmi. Once we have our oauth key we can add this to the server settings with /server add twitch irc.twitch.tv/6667 -password=oauth:*** -nicks=TWITCH_NAME -username=TWITCH_NAME Connecting and Chatting Once this is complete we are ready to start connecting to channels. The first step is to run /connect twitch Then we can join whichever channel we want with /join #channel_name Saving and closing When you wish to close down weechat we are able to save settings with /save and exit channel with /part #channel_name / Finally to close weechat we use /quit There is a quick gist created by noromanba -- https://gist.github.com/noromanba/df3d975613713f60e6ae. Which contains these steps in a more compact design for ease of reference.","title":"Getting IRC running with weechat."},{"location":"zet/20210605125400/#getting-irc-running-with-weechat","text":"Weechat provides an excellent command line interface for numerous chat groups. Notably it provides an excellent setup for a chat client to interface to the twitch community. This is great to keep the chat going without the video stream. The following zet is basically a short installation guide for future me when i inevitably break something and need to format and reinstall the OS.","title":"Getting IRC running with weechat."},{"location":"zet/20210605125400/#installation","text":"To install weechat we can run the installation from the command line with sudo apt install weechat Once installed we can begin to configure it. This is used to allow the authentication fo the servers that are used for the chats.","title":"Installation"},{"location":"zet/20210605125400/#twitch-integration","text":"TO connect to the twitch server we run /server add twitch irc.chat.twitch.tv This will add the twitch server with a label such as twitch. Once this is done we need to add the oauth key to the server for authentication. To get an oauth key we can go to twitchapps.com/tmi. Once we have our oauth key we can add this to the server settings with /server add twitch irc.twitch.tv/6667 -password=oauth:*** -nicks=TWITCH_NAME -username=TWITCH_NAME","title":"Twitch integration"},{"location":"zet/20210605125400/#connecting-and-chatting","text":"Once this is complete we are ready to start connecting to channels. The first step is to run /connect twitch Then we can join whichever channel we want with /join #channel_name","title":"Connecting and Chatting"},{"location":"zet/20210605125400/#saving-and-closing","text":"When you wish to close down weechat we are able to save settings with /save and exit channel with /part #channel_name / Finally to close weechat we use /quit There is a quick gist created by noromanba -- https://gist.github.com/noromanba/df3d975613713f60e6ae. Which contains these steps in a more compact design for ease of reference.","title":"Saving and closing"},{"location":"zet/20210607102038/","text":"Emailing me my public IP when it updates I have been working with ssh for a long time. One of the issues i face is losing the link to the home machine when i have not been on for a while and the homes public IP address changes. To counter this rather than requesting a static IP address i built a script that will pull the current ip address from opendns using dig +short myip.opendns.com @resolver1.opendns.com Once this is pulled it is saved to a variable. This value is compared to a previous IP address stored in a mail.txt file. The text file is read into a variable using cat. To compare these to variables i use an if statement and a simple logical test with != . If the ip address has changed i push the new ip address to the mail.txt file and then email this to me using curl. The command to send a message to someone with curl is incredible simple. However if you don't set it up correctly you are sending information such as your username and password over the internet. To counter this we are able to create a file called .netrc . In this file we place the location the email is being sent too. For example the gmail smtp server. Your username and Password. These are stored as follows. machine smtp.gmail.com user yourname password MySeQRpAsswrd Once done we can send a curl email with curl --ssl-reqd --netrc \\ --url 'smtps://smtp.gmail.com:465' \\ --mail-from 'myemail@gmail.com' \\ --mail-rcpt 'myemail@gmail.com' \\ --upload-file $HOME/Documents/mail.txt This script is excellent as is. But to fully benefit from this i added the script to a cronjob with crontab -e * * * * * /myprogram This runs the program every minute and if the IP has changed it will send an email to me telling me it has changed. There is more i might do with this script or pipeline. Possibilities include adding an email scraper that automatically checks for a change in my home IP address and pushes it to my config file so i never have to worry about updating the config file and it will seemlessly ssh into the system from anywhere. I also am not sure whether i need to run it every minute, as the script is quite small and only emails on a change i know it wont cause any issues so i am pretty happy with it at the moment.","title":"Emailing me my public IP when it updates"},{"location":"zet/20210607102038/#emailing-me-my-public-ip-when-it-updates","text":"I have been working with ssh for a long time. One of the issues i face is losing the link to the home machine when i have not been on for a while and the homes public IP address changes. To counter this rather than requesting a static IP address i built a script that will pull the current ip address from opendns using dig +short myip.opendns.com @resolver1.opendns.com Once this is pulled it is saved to a variable. This value is compared to a previous IP address stored in a mail.txt file. The text file is read into a variable using cat. To compare these to variables i use an if statement and a simple logical test with != . If the ip address has changed i push the new ip address to the mail.txt file and then email this to me using curl. The command to send a message to someone with curl is incredible simple. However if you don't set it up correctly you are sending information such as your username and password over the internet. To counter this we are able to create a file called .netrc . In this file we place the location the email is being sent too. For example the gmail smtp server. Your username and Password. These are stored as follows. machine smtp.gmail.com user yourname password MySeQRpAsswrd Once done we can send a curl email with curl --ssl-reqd --netrc \\ --url 'smtps://smtp.gmail.com:465' \\ --mail-from 'myemail@gmail.com' \\ --mail-rcpt 'myemail@gmail.com' \\ --upload-file $HOME/Documents/mail.txt This script is excellent as is. But to fully benefit from this i added the script to a cronjob with crontab -e * * * * * /myprogram This runs the program every minute and if the IP has changed it will send an email to me telling me it has changed. There is more i might do with this script or pipeline. Possibilities include adding an email scraper that automatically checks for a change in my home IP address and pushes it to my config file so i never have to worry about updating the config file and it will seemlessly ssh into the system from anywhere. I also am not sure whether i need to run it every minute, as the script is quite small and only emails on a change i know it wont cause any issues so i am pretty happy with it at the moment.","title":"Emailing me my public IP when it updates"},{"location":"zet/20210607115958/","text":"Configuring Mutt for email I have been putting off doing this for a long time because i have had no reason to actually have a terminal based email. However having set up an IP email for my home public IP address i have felt i might like to add mutt into the mix and increase my security. I won't write a step by step guide to setting up mutt as i feel https://linuxconfig.org/how-to-install-configure-and-use-mutt-with-a-gmail-account-on-linux did a great job on this. I do wish to note that there are a couple of requirements to setting this up. The first is that your gmail needs to be configured with 2-factor authentication. Once this is done it is very easy to configure the rest of the settings. This little side step will link to my email scraper to find the latest public IP address recieved...","title":"Configuring Mutt for email"},{"location":"zet/20210607115958/#configuring-mutt-for-email","text":"I have been putting off doing this for a long time because i have had no reason to actually have a terminal based email. However having set up an IP email for my home public IP address i have felt i might like to add mutt into the mix and increase my security. I won't write a step by step guide to setting up mutt as i feel https://linuxconfig.org/how-to-install-configure-and-use-mutt-with-a-gmail-account-on-linux did a great job on this. I do wish to note that there are a couple of requirements to setting this up. The first is that your gmail needs to be configured with 2-factor authentication. Once this is done it is very easy to configure the rest of the settings. This little side step will link to my email scraper to find the latest public IP address recieved...","title":"Configuring Mutt for email"},{"location":"zet/20210608040257/","text":"Working with Tar files An interesting side bit i found out today is that tar in itself is not a compression program. It is designed to join multiple files into one large file. The flags that are provided actually pipe the output of the tar join to underlying compression utilities such as gzip or bzip2. This is due to the origirnal design of tar being for tape archives and additional modification used the linux philosophy of being able to pipe stdout to other locations. Tar is an interesting tool that has the ability to uncompress some file formats it however does not have the ability to unzip zip files.","title":"Working with Tar files"},{"location":"zet/20210608040257/#working-with-tar-files","text":"An interesting side bit i found out today is that tar in itself is not a compression program. It is designed to join multiple files into one large file. The flags that are provided actually pipe the output of the tar join to underlying compression utilities such as gzip or bzip2. This is due to the origirnal design of tar being for tape archives and additional modification used the linux philosophy of being able to pipe stdout to other locations. Tar is an interesting tool that has the ability to uncompress some file formats it however does not have the ability to unzip zip files.","title":"Working with Tar files"},{"location":"zet/20210608100508/","text":"Fun with colours in the terminal We can do a lot with ANSI escape sequences in the terminal. These escape sequences are methods of changing how the terminal is shown including colourising, blinking, bold, underline and other things. To implement many of the colour changing and modifiers we use \\e followed by [ and a number. To stop the modification we use \\e[0. Some cool things we can do with this functionality is something like: printf \"\\e[38;5;33m DANGER\" this will make a DANGER warning in red with it blinking. Not all terminals work with this. This is quite useful to modify your prompt. Other normal escape sequences include \\n \\r and so on. Further learning on this will be used to customise the prompt","title":"Fun with colours in the terminal"},{"location":"zet/20210608100508/#fun-with-colours-in-the-terminal","text":"We can do a lot with ANSI escape sequences in the terminal. These escape sequences are methods of changing how the terminal is shown including colourising, blinking, bold, underline and other things. To implement many of the colour changing and modifiers we use \\e followed by [ and a number. To stop the modification we use \\e[0. Some cool things we can do with this functionality is something like: printf \"\\e[38;5;33m DANGER\" this will make a DANGER warning in red with it blinking. Not all terminals work with this. This is quite useful to modify your prompt. Other normal escape sequences include \\n \\r and so on. Further learning on this will be used to customise the prompt","title":"Fun with colours in the terminal"},{"location":"zet/20210608120528/","text":"Parameter Expansion Often times we want to capture certain parts of a variable. For instance when we call $PWD we may only want the current folder or the parents. Using parameter expansion we are able to get a part of the variable. This is instead of creating a subshell when calling a built in program such as basename. To complete parameter expansion and remove certain parts we can use many different keys. For example if we have the path of a file such as path=\"/usr/bin/bash\"; If we wish to remove all infomation up to and including the final /. The command to do this is ${path##*/} The result is bash . There is a lot more that parameter expansion can do. To learn more man dash . /Parameter expansion. Using this we can bypass using AWK or SED","title":"Parameter Expansion"},{"location":"zet/20210608120528/#parameter-expansion","text":"Often times we want to capture certain parts of a variable. For instance when we call $PWD we may only want the current folder or the parents. Using parameter expansion we are able to get a part of the variable. This is instead of creating a subshell when calling a built in program such as basename. To complete parameter expansion and remove certain parts we can use many different keys. For example if we have the path of a file such as path=\"/usr/bin/bash\"; If we wish to remove all infomation up to and including the final /. The command to do this is ${path##*/} The result is bash . There is a lot more that parameter expansion can do. To learn more man dash . /Parameter expansion. Using this we can bypass using AWK or SED","title":"Parameter Expansion"},{"location":"zet/20210608235223/","text":"Globbing in terminal Globbing is an excellent resource for working with folders and files in the terminal. One of the great things about globbing is that you can create a set of files using file{01...10}.{md,txt} will create 20 files with md and txt for 10 individual files. This is very useful when running tests or other such quick scripts. Globstar **/* will find all files in the directory. find is a better resource with regular expressions. Ranges ls f[ao-q]* will match anything with an f and a group of values a and range o to q. Match files that don't contain value To find all files that don't start with a value. using ls -d [^.]* will find all files that don't start with . CODE BLOCK: for i in [^.]*; do echo \"$i\"","title":"Globbing in terminal"},{"location":"zet/20210608235223/#globbing-in-terminal","text":"Globbing is an excellent resource for working with folders and files in the terminal. One of the great things about globbing is that you can create a set of files using file{01...10}.{md,txt} will create 20 files with md and txt for 10 individual files. This is very useful when running tests or other such quick scripts.","title":"Globbing in terminal"},{"location":"zet/20210608235223/#globstar","text":"**/* will find all files in the directory. find is a better resource with regular expressions.","title":"Globstar"},{"location":"zet/20210608235223/#ranges","text":"ls f[ao-q]* will match anything with an f and a group of values a and range o to q.","title":"Ranges"},{"location":"zet/20210608235223/#match-files-that-dont-contain-value","text":"To find all files that don't start with a value. using ls -d [^.]* will find all files that don't start with . CODE BLOCK: for i in [^.]*; do echo \"$i\"","title":"Match files that don't contain value"},{"location":"zet/20210609012706/","text":"Using vimagics to get cli interface support. One of the greatest things about vim is the ability to use what some people call vimagics. Some of the functions include: !! - To get into this mode from in normal mode if we type shift 11 we start line magic. From here we can send the line to whatever program we want. For instance we can write a python for loop and we can send this to python using !!python . %! - This command will send the whole file to whatever program you want. We can do so much with vimagics. !} - If we want to send a section to a command we use !} which will send the whole section to the next program. !13! - The command will send from the current line to line 13 to the command that will be typed next. # For example we sent this line and the following three lines to `cmt` a program that comments the lines. # This is done using !:16. # Line is commented It is of note that the :16 refers to the line the commenting will end before. ie. Up to but not including line number 16. This is incredbly useful when working with python and other programming languages.","title":"Using vimagics to get cli interface support."},{"location":"zet/20210609012706/#using-vimagics-to-get-cli-interface-support","text":"One of the greatest things about vim is the ability to use what some people call vimagics. Some of the functions include: !! - To get into this mode from in normal mode if we type shift 11 we start line magic. From here we can send the line to whatever program we want. For instance we can write a python for loop and we can send this to python using !!python . %! - This command will send the whole file to whatever program you want. We can do so much with vimagics. !} - If we want to send a section to a command we use !} which will send the whole section to the next program. !13! - The command will send from the current line to line 13 to the command that will be typed next. # For example we sent this line and the following three lines to `cmt` a program that comments the lines. # This is done using !:16. # Line is commented It is of note that the :16 refers to the line the commenting will end before. ie. Up to but not including line number 16. This is incredbly useful when working with python and other programming languages.","title":"Using vimagics to get cli interface support."},{"location":"zet/20210609020532/","text":"Idea for building a local search based utility for zets Zets are an excellent source for storing infomation. However there are a couple of issues that are faced when you don't want to integrate with github. If you need to search for a topic from the command line you will need to parse the contents of the directories to find the information. A program to allow this from the command line will be useful. One thing might be to utilise the gh like tool where you can select from a topic search or text search. One other program idea i wish to build is to reopen a previous zet but keep the commit message when re pushing it. The zet needs to be opened by either its heading or maybe pull last.","title":"Idea for building a local search based utility for zets"},{"location":"zet/20210609020532/#idea-for-building-a-local-search-based-utility-for-zets","text":"Zets are an excellent source for storing infomation. However there are a couple of issues that are faced when you don't want to integrate with github. If you need to search for a topic from the command line you will need to parse the contents of the directories to find the information. A program to allow this from the command line will be useful. One thing might be to utilise the gh like tool where you can select from a topic search or text search. One other program idea i wish to build is to reopen a previous zet but keep the commit message when re pushing it. The zet needs to be opened by either its heading or maybe pull last.","title":"Idea for building a local search based utility for zets"},{"location":"zet/20210615124539/","text":"TMUX has a plugin manager Similar to vim, tmux has a plugin manager. What is great about this is that i can utilise two powerful plugins tmux-resurrect and tmux-continuum. Resurrect allows me to reload the same configuration for the next time you restart tmux. This is handy when working on a certain workflow and needing to restart the computer or shutdown for the night. Continuum continually saves the current session, which is useful when linked with tmux-resurrect. There a some other small plugins that may or may not be useful to people. One i like and might integrate into my tmux setup is tmux-weather. This puts the weather in the status line of tmux. tmux-tpm the plugin manager also has a small two liner to automatically install tpm and added plugins to a new machine. Useful Links: https://github.com/tmux-plugins/tpm https://github.com/tmux-plugins/tmux-resurrect https://github.com/tmux-plugins/tmux-continuum For system admin on development boards: https://github.com/tmux-plugins/tmux-logging","title":"TMUX has a plugin manager"},{"location":"zet/20210615124539/#tmux-has-a-plugin-manager","text":"Similar to vim, tmux has a plugin manager. What is great about this is that i can utilise two powerful plugins tmux-resurrect and tmux-continuum. Resurrect allows me to reload the same configuration for the next time you restart tmux. This is handy when working on a certain workflow and needing to restart the computer or shutdown for the night. Continuum continually saves the current session, which is useful when linked with tmux-resurrect. There a some other small plugins that may or may not be useful to people. One i like and might integrate into my tmux setup is tmux-weather. This puts the weather in the status line of tmux. tmux-tpm the plugin manager also has a small two liner to automatically install tpm and added plugins to a new machine. Useful Links: https://github.com/tmux-plugins/tpm https://github.com/tmux-plugins/tmux-resurrect https://github.com/tmux-plugins/tmux-continuum For system admin on development boards: https://github.com/tmux-plugins/tmux-logging","title":"TMUX has a plugin manager"},{"location":"zet/20210621002121/","text":"lsio have created a container called webtop The company linuxserver.io have built a container that is a full linux system with a window manager and standard system utilities inside a web browser I plan on updating this dockerfile to incorporate my own configurations to have the fully fledged development environment for testing and experimentation. Some of the hurdles may be working with the jetson board. As the architecture is different. There are benefits to this design. We are able to create a solid starting point for an image and provide the ability to update the system. However the disadvantage is if the container breaks you have lost the additional configurations.","title":"lsio have created a container called webtop"},{"location":"zet/20210621002121/#lsio-have-created-a-container-called-webtop","text":"The company linuxserver.io have built a container that is a full linux system with a window manager and standard system utilities inside a web browser I plan on updating this dockerfile to incorporate my own configurations to have the fully fledged development environment for testing and experimentation. Some of the hurdles may be working with the jetson board. As the architecture is different. There are benefits to this design. We are able to create a solid starting point for an image and provide the ability to update the system. However the disadvantage is if the container breaks you have lost the additional configurations.","title":"lsio have created a container called webtop"},{"location":"zet/20210622234930/","text":"f-strings have some neat features In python, f-strings allow you to call the result of a variable inside the string quotation as follows print(f\"This is my value = {value}\") What is even cooler is the additional functionality added in python3.8. The first of these is using : print(f\"{num_value = }\") - This will grab the name of the variable and automatically do similar to the first line. This is incredibly useful when debugging. We can also do arbitrary maths inside the f string and view the results. {num_value % 2 = } . The ! character can be used to convert the result between alpha, repr and string outputs for debugging. One other really call feature is automatic string formatting using the : in an f string. For example: We have calculated the current time with datetime.utcnow(). Then we print the result in year month day format with: print(f'{now=:%Y-%m-%d}') We can also use the string formatting to getdecimal places using :2f for example. These features are incredibly helpful for debugging and testing purposes.","title":"f-strings have some neat features"},{"location":"zet/20210622234930/#f-strings-have-some-neat-features","text":"In python, f-strings allow you to call the result of a variable inside the string quotation as follows print(f\"This is my value = {value}\") What is even cooler is the additional functionality added in python3.8. The first of these is using : print(f\"{num_value = }\") - This will grab the name of the variable and automatically do similar to the first line. This is incredibly useful when debugging. We can also do arbitrary maths inside the f string and view the results. {num_value % 2 = } . The ! character can be used to convert the result between alpha, repr and string outputs for debugging. One other really call feature is automatic string formatting using the : in an f string. For example: We have calculated the current time with datetime.utcnow(). Then we print the result in year month day format with: print(f'{now=:%Y-%m-%d}') We can also use the string formatting to getdecimal places using :2f for example. These features are incredibly helpful for debugging and testing purposes.","title":"f-strings have some neat features"},{"location":"zet/20210628053612/","text":"Learning golang by building a website I have been looking at languages to learn and rob recommended learning GO. One of tthe first videos i looked at - https://www.youtube.com/watch?v=SqrbIlUwR0U Discussed setting up a website as a simple go project. I am thinking of integrating what i have been doing here with the zets and my interest in building a blogging site. The first plan is to just locally host it so i can view the site from my computer. The front end and queries will probably be all done in go whilst the backend will have markdown files stored in a structure and converted to html etc using pandoc. What i plan on doing is integrating this backend into my code development. At some stage i will look at using a style sheet and possibly begin to incorporate some of the functionality of the larger web devs such as jekkyl and github actions to run the server. I will need to learn more but this will be a way to test my go skills as well as my knowledge of web dev and learn a new skillset. Additional Resources: - https://www.youtube.com/watch?v=MsnXX12FbCc - https://www.youtube.com/watch?v=Efk2M77naFU","title":"Learning golang by building a website"},{"location":"zet/20210628053612/#learning-golang-by-building-a-website","text":"I have been looking at languages to learn and rob recommended learning GO. One of tthe first videos i looked at - https://www.youtube.com/watch?v=SqrbIlUwR0U Discussed setting up a website as a simple go project. I am thinking of integrating what i have been doing here with the zets and my interest in building a blogging site. The first plan is to just locally host it so i can view the site from my computer. The front end and queries will probably be all done in go whilst the backend will have markdown files stored in a structure and converted to html etc using pandoc. What i plan on doing is integrating this backend into my code development. At some stage i will look at using a style sheet and possibly begin to incorporate some of the functionality of the larger web devs such as jekkyl and github actions to run the server. I will need to learn more but this will be a way to test my go skills as well as my knowledge of web dev and learn a new skillset. Additional Resources: - https://www.youtube.com/watch?v=MsnXX12FbCc - https://www.youtube.com/watch?v=Efk2M77naFU","title":"Learning golang by building a website"},{"location":"zet/20210630071217/","text":"Getting Started with Go Go is a great language. It has some cool features. The rwxrob How to Start a Golang Project right is very useful -- https://www.youtube.com/watch?v=Ot9Em123Fz8&t=739s The codecademy go course is good as well - https://www.codecademy.com/courses/learn-go/lessons/ Other interesting Articles https://medium.com/rungo/everything-you-need-to-know-about-packages-in-go-b8bac62b74cc https://medium.com/rungo/anatomy-of-modules-in-go-c8274d215c16 https://medium.com/google-cloud/building-a-go-web-app-from-scratch-to-deploying-on-google-cloud-part-1-building-a-simple-go-aee452a2e654 Notes from youtube videos - Begin a project by doing go mod init github.com/ / . This will initialise a go module. package is what the program will be named when using it so be careful with what you name these fuctions. If you want to use a functipn from a library it needs to be imported just like python. If the package is not used GO will flag an error so be careful with this. We have variables and constants in go with the keywords var and const . To initialise a variable of type string we use : var message string To create a constant we use: const message string We can use the := operator to declare and assign the variable at the same time x := 2 If the variable has been initialised we can assign it with: x = 2 Functions are defined using func . They should be named with camelCase. takes zero or more arguments, which we can define the type. We can follow this with a return type declaration to explicitly indicate the type of the output variables. eg. func valueMod(a, b int)(int) { c = a % b return c } Unlike python we have if and else if Always surrond these control flow statements with {} switch and case are available. We have a single looping procedure with for this syntax is similar to c syntax. except we can use for _, i := range d for iterating over a range. Im sure there is more that we can do with this functionality like providing any named function.... When we want to use a function in another program we can export it by capitalising the first letter of the function. Anything that is capitalised is available in the package for other users. We have anonymous functions - basically a function that is defined and run inside the main function and is not accessible elsewhere. We also have Pointers, Methods and Go routines which i will add in a latter zet","title":"Getting Started with Go"},{"location":"zet/20210630071217/#getting-started-with-go","text":"Go is a great language. It has some cool features. The rwxrob How to Start a Golang Project right is very useful -- https://www.youtube.com/watch?v=Ot9Em123Fz8&t=739s The codecademy go course is good as well - https://www.codecademy.com/courses/learn-go/lessons/ Other interesting Articles https://medium.com/rungo/everything-you-need-to-know-about-packages-in-go-b8bac62b74cc https://medium.com/rungo/anatomy-of-modules-in-go-c8274d215c16 https://medium.com/google-cloud/building-a-go-web-app-from-scratch-to-deploying-on-google-cloud-part-1-building-a-simple-go-aee452a2e654 Notes from youtube videos - Begin a project by doing go mod init github.com/ / . This will initialise a go module. package is what the program will be named when using it so be careful with what you name these fuctions. If you want to use a functipn from a library it needs to be imported just like python. If the package is not used GO will flag an error so be careful with this. We have variables and constants in go with the keywords var and const . To initialise a variable of type string we use : var message string To create a constant we use: const message string We can use the := operator to declare and assign the variable at the same time x := 2 If the variable has been initialised we can assign it with: x = 2 Functions are defined using func . They should be named with camelCase. takes zero or more arguments, which we can define the type. We can follow this with a return type declaration to explicitly indicate the type of the output variables. eg. func valueMod(a, b int)(int) { c = a % b return c } Unlike python we have if and else if Always surrond these control flow statements with {} switch and case are available. We have a single looping procedure with for this syntax is similar to c syntax. except we can use for _, i := range d for iterating over a range. Im sure there is more that we can do with this functionality like providing any named function.... When we want to use a function in another program we can export it by capitalising the first letter of the function. Anything that is capitalised is available in the package for other users. We have anonymous functions - basically a function that is defined and run inside the main function and is not accessible elsewhere. We also have Pointers, Methods and Go routines which i will add in a latter zet","title":"Getting Started with Go"},{"location":"zet/20210630083124/","text":"There is a tiling window manager for mac Just found a twitch streamer was using a tiling window manager on the mac. - https://github.com/koekeishiya/yabai It looks pretty good for what it is. If you wanted to have the tiling WM style on your mac it is available. I do wonder how much customisability is available in this tool. If i get a mac in the future i may look into using this...","title":"There is a tiling window manager for mac"},{"location":"zet/20210630083124/#there-is-a-tiling-window-manager-for-mac","text":"Just found a twitch streamer was using a tiling window manager on the mac. - https://github.com/koekeishiya/yabai It looks pretty good for what it is. If you wanted to have the tiling WM style on your mac it is available. I do wonder how much customisability is available in this tool. If i get a mac in the future i may look into using this...","title":"There is a tiling window manager for mac"},{"location":"zet/20210703123958/","text":"Learning Go [part 2] We export go functions by making the function begin with a capital letter. The compiler also finds the main function of the main package when we execute go code. It is mandatory for a go program to be part of a package. Every package should be a distinct folder on $GOPATH . Go does not have any try catch, it uses one error type in the errors package. We need to handle our own errors. Enumerate functionality is done with the keyword iota . This starts at 0. Array is defined with: var arr [n]type . -- n is length, type is the type and [] to get or set elements. We can nest arrays in arrays. Main disadvantage of an array is we need to know the size. We can use a slice instead which is a dynamic array . like in python we can do arr[:n] to define a section of an array up to n . We can also define to the end with ar[n:] len - gets the length of the slice. cap - gets the max len of slice append - adds to the slice. copy - copys from one slice to another. map is a key value pair. ie a dictionary in python. map is unsorted. len - returns length of keys . like python to change a map we use numbers[\"one\"]=11 delete - deletes an element. make - memory allocation for built in modesl. - such as map , slice and channel . while new is for types memory allocation. ----------------------------------- Loops ---------------------------------- if doesnt need parentheses. if x > 10 { goto is available --- don't use. - however it is a goto to a name. for - is available but there is no while or do while . for is done the same way as in c.... break and continue is available. for can read from a slice: for k, v:= range map switch - case normal method. --------------------------------- functions -------------------------------- functions have arg ...int - which acts like *args in python. When x is passed the value doesn't change. If we actually want to change the variable we need to point to the location with a pointer using *int . We pass the value with & . defer - postpones the execution of a function till calling function has finished executing. defer works in reverse order when the program weaches the end. This would be useful when catching errors and closing the files. Functions are also variables in Go. Defined by a type . Functions are first class citizens. ---------------------------------- Imports --------------------------------- import by absolute path. $GOPATH/pkg/shorturl/model We can use a . when we improt to have the package name ommited when we call it. import (. \"fmt\") Printf rather than fmt.Printf We can alias f \"fmt ---------------------------------- Structs --------------------------------- We define structs similar to C. type Product struct { name string itemID int cost float32 } We init structs with var gobook Product and can put values in locations with . notation. gobook.name = \"web book We can also := assign. init struct with order book:= Product{name:\"book\", itemID:10025 We can have sub structs which essentially is inheritance like python. ----------------------- Methods (OO type paradigms) ----------------------- A method is affiliated with a type it contains a receiver . func (r ReceiverType) funcName(params) (results) We can use this to create multiple functions of the same name by calling a struct as the receivertype ------------------------------- Other things ------------------------------- defining custom types : type typename typeliteral --------------------------------- Out Notes -------------------------------- -- All these notes have been derived from the excelent book. -- https://thewhitetulip.gitbook.io/bo/","title":"Learning Go [part 2]"},{"location":"zet/20210703123958/#learning-go-part-2","text":"We export go functions by making the function begin with a capital letter. The compiler also finds the main function of the main package when we execute go code. It is mandatory for a go program to be part of a package. Every package should be a distinct folder on $GOPATH . Go does not have any try catch, it uses one error type in the errors package. We need to handle our own errors. Enumerate functionality is done with the keyword iota . This starts at 0. Array is defined with: var arr [n]type . -- n is length, type is the type and [] to get or set elements. We can nest arrays in arrays. Main disadvantage of an array is we need to know the size. We can use a slice instead which is a dynamic array . like in python we can do arr[:n] to define a section of an array up to n . We can also define to the end with ar[n:] len - gets the length of the slice. cap - gets the max len of slice append - adds to the slice. copy - copys from one slice to another. map is a key value pair. ie a dictionary in python. map is unsorted. len - returns length of keys . like python to change a map we use numbers[\"one\"]=11 delete - deletes an element. make - memory allocation for built in modesl. - such as map , slice and channel . while new is for types memory allocation.","title":"Learning Go [part 2]"},{"location":"zet/20210703123958/#-loops-","text":"if doesnt need parentheses. if x > 10 { goto is available --- don't use. - however it is a goto to a name. for - is available but there is no while or do while . for is done the same way as in c.... break and continue is available. for can read from a slice: for k, v:= range map switch - case normal method.","title":"----------------------------------- Loops ----------------------------------"},{"location":"zet/20210703123958/#-functions-","text":"functions have arg ...int - which acts like *args in python. When x is passed the value doesn't change. If we actually want to change the variable we need to point to the location with a pointer using *int . We pass the value with & . defer - postpones the execution of a function till calling function has finished executing. defer works in reverse order when the program weaches the end. This would be useful when catching errors and closing the files. Functions are also variables in Go. Defined by a type . Functions are first class citizens.","title":"--------------------------------- functions --------------------------------"},{"location":"zet/20210703123958/#-imports-","text":"import by absolute path. $GOPATH/pkg/shorturl/model We can use a . when we improt to have the package name ommited when we call it. import (. \"fmt\") Printf rather than fmt.Printf We can alias f \"fmt","title":"---------------------------------- Imports ---------------------------------"},{"location":"zet/20210703123958/#-structs-","text":"We define structs similar to C. type Product struct { name string itemID int cost float32 } We init structs with var gobook Product and can put values in locations with . notation. gobook.name = \"web book We can also := assign. init struct with order book:= Product{name:\"book\", itemID:10025 We can have sub structs which essentially is inheritance like python.","title":"---------------------------------- Structs ---------------------------------"},{"location":"zet/20210703123958/#-methods-oo-type-paradigms-","text":"A method is affiliated with a type it contains a receiver . func (r ReceiverType) funcName(params) (results) We can use this to create multiple functions of the same name by calling a struct as the receivertype","title":"----------------------- Methods (OO type paradigms)  -----------------------"},{"location":"zet/20210703123958/#-other-things-","text":"defining custom types : type typename typeliteral","title":"------------------------------- Other things -------------------------------"},{"location":"zet/20210703123958/#-out-notes-","text":"-- All these notes have been derived from the excelent book. -- https://thewhitetulip.gitbook.io/bo/","title":"--------------------------------- Out Notes --------------------------------"},{"location":"zet/20210704130642/","text":"Learning GO [part 3] Go programming basics $GOPATH is the environment variable that stores where the Go programs are stored. $GOROOT is where GO is installed. Packages are stored in distinct folders on GOPATH. Many directories but only one main function in one main file. Always call absolute path when importing. NOTE: GOPATH is being deprecated look into Go Modules. Additional Notes - Somewhat unrelated. Web Deployment WebApps are bound to a port ie 8888. We can make the webAPP accessible to the network with 0.0.0.0:8888.","title":"Learning GO [part 3]"},{"location":"zet/20210704130642/#learning-go-part-3","text":"","title":"Learning GO [part 3]"},{"location":"zet/20210704130642/#go-programming-basics","text":"$GOPATH is the environment variable that stores where the Go programs are stored. $GOROOT is where GO is installed. Packages are stored in distinct folders on GOPATH. Many directories but only one main function in one main file. Always call absolute path when importing. NOTE: GOPATH is being deprecated look into Go Modules. Additional Notes - Somewhat unrelated.","title":"Go programming basics"},{"location":"zet/20210704130642/#web-deployment","text":"WebApps are bound to a port ie 8888. We can make the webAPP accessible to the network with 0.0.0.0:8888.","title":"Web Deployment"},{"location":"zet/20210704135451/","text":"Lynx is a great program for reading long web articles One of the issues i was having with lynx was not understanding how to scroll in lynx. To do this all we need to use is ctrl - n for down and ctrl - p for up. We can go forward and back links with j and k as well.","title":"Lynx is a great program for reading long web articles"},{"location":"zet/20210704135451/#lynx-is-a-great-program-for-reading-long-web-articles","text":"One of the issues i was having with lynx was not understanding how to scroll in lynx. To do this all we need to use is ctrl - n for down and ctrl - p for up. We can go forward and back links with j and k as well.","title":"Lynx is a great program for reading long web articles"},{"location":"zet/20210705073723/","text":"Imports and packages in python We can use the __all__ in a module to define the import * scope. For example if we have a module, a python .py file we can define the functions that the import statement can see through using __all__ = ['foo', 'bar'] to provide access to the foo and bar modules. If we had a different function called apple the import * would not see this function. __init__.py is used to make a directory a python package. - init can be an empty file or execute initialisation code for the package or set the __all__ variable.","title":"Imports and packages in python"},{"location":"zet/20210705073723/#imports-and-packages-in-python","text":"We can use the __all__ in a module to define the import * scope. For example if we have a module, a python .py file we can define the functions that the import statement can see through using __all__ = ['foo', 'bar'] to provide access to the foo and bar modules. If we had a different function called apple the import * would not see this function. __init__.py is used to make a directory a python package. - init can be an empty file or execute initialisation code for the package or set the __all__ variable.","title":"Imports and packages in python"},{"location":"zet/20210705134019/","text":"Building webservices in Go There is an excellent talk by Mat Ryer about HTTP web services - https://www.youtube.com/watch?v=rWBSMsLG8po Create a routes.go file to handle all your routes. ie func (s *server) routes() { s.router.Get(\"/api/\", s.handleAPI()) s.router.Get(\"/About\", s.handleAbout()) s.router.Get(\"/\", s.handleIndex()) These handlers hang off the server. - func (s *sever) handleSomething() http.HanlerFunc { } Create handler namings handleTasksCreate handleTaskDone handleAuthLogin handleAuthLogout","title":"Building webservices in Go"},{"location":"zet/20210705134019/#building-webservices-in-go","text":"There is an excellent talk by Mat Ryer about HTTP web services - https://www.youtube.com/watch?v=rWBSMsLG8po Create a routes.go file to handle all your routes. ie func (s *server) routes() { s.router.Get(\"/api/\", s.handleAPI()) s.router.Get(\"/About\", s.handleAbout()) s.router.Get(\"/\", s.handleIndex()) These handlers hang off the server. - func (s *sever) handleSomething() http.HanlerFunc { } Create handler namings handleTasksCreate handleTaskDone handleAuthLogin handleAuthLogout","title":"Building webservices in Go"},{"location":"zet/20210706122957/","text":"Learning Go [Part 4] This zet outlines the development of the simple web server that i have developed using the go language. I have taken inspiration and development from: - whiteTulipGO https://thewhitetulip.gitbook.io/bo/, - rwxrob - go development, - Mat Ryer - https://www.youtube.com/watch?v=rWBSMsLG8po, - Tech with Tim - https://www.youtube.com/watch?v=75lJDVT1h0s&list=PLzMcBGfZo4-mtY_SE3HuzQJzuj4VlUG0q and others. The first step was to build a simple server using the http package. This is wrapped in a logger to catch any fatal issues. We can use log to time stamp to the terminal and indicate that the server has started by adding log.Print to the program. The tutorial by whitetulip goes into depth on using databases for building a task app. Much of the tutorial is out of my current interest scope but what is of interest is some of the subsequent sections. Routing is used to route the server to the specific functions. We can use a useful package called github.com/julienschmidt/httprouter which is very powerful and provides routing for GET and POST and is advantageous in allowign the user to be more free with their URL order. Middleware in webservers are essentially modular extensions to the website that doesn't know anything about the app but can provide additional functionality such as a login redirection middleware.","title":"Learning Go [Part 4]"},{"location":"zet/20210706122957/#learning-go-part-4","text":"This zet outlines the development of the simple web server that i have developed using the go language. I have taken inspiration and development from: - whiteTulipGO https://thewhitetulip.gitbook.io/bo/, - rwxrob - go development, - Mat Ryer - https://www.youtube.com/watch?v=rWBSMsLG8po, - Tech with Tim - https://www.youtube.com/watch?v=75lJDVT1h0s&list=PLzMcBGfZo4-mtY_SE3HuzQJzuj4VlUG0q and others. The first step was to build a simple server using the http package. This is wrapped in a logger to catch any fatal issues. We can use log to time stamp to the terminal and indicate that the server has started by adding log.Print to the program. The tutorial by whitetulip goes into depth on using databases for building a task app. Much of the tutorial is out of my current interest scope but what is of interest is some of the subsequent sections. Routing is used to route the server to the specific functions. We can use a useful package called github.com/julienschmidt/httprouter which is very powerful and provides routing for GET and POST and is advantageous in allowign the user to be more free with their URL order. Middleware in webservers are essentially modular extensions to the website that doesn't know anything about the app but can provide additional functionality such as a login redirection middleware.","title":"Learning Go [Part 4]"},{"location":"zet/20210706233243/","text":"Bash has dictionaries, or associative arrays [bash] [vim] we can make dictionaries in bash with arr[key]=value We access the keys with !arr and values with arr[@] An example that is really cool emoji filter while read -r line; do for k in ${!emoji[@]}; do line=${line//:$k:/${emoji[$k]}} done echo \"$line\" done To access it in command line echo 'some :smile: here and :pomo: there | toemoji To extend on this to be used in vim as an autocmd, we can create a autocmd with a vimleavepre. BufWritePost will run the au command on save. We can use this with explicit file location to update a file that is only in a certain location. `","title":"Bash has dictionaries, or associative arrays"},{"location":"zet/20210706233243/#bash-has-dictionaries-or-associative-arrays","text":"[bash] [vim] we can make dictionaries in bash with arr[key]=value We access the keys with !arr and values with arr[@] An example that is really cool","title":"Bash has dictionaries, or associative arrays"},{"location":"zet/20210706233243/#emoji-filter","text":"while read -r line; do for k in ${!emoji[@]}; do line=${line//:$k:/${emoji[$k]}} done echo \"$line\" done To access it in command line echo 'some :smile: here and :pomo: there | toemoji To extend on this to be used in vim as an autocmd, we can create a autocmd with a vimleavepre. BufWritePost will run the au command on save. We can use this with explicit file location to update a file that is only in a certain location. `","title":"emoji filter"},{"location":"zet/20210708121331/","text":"Learning Go [Part 4] ----------------------------- Interfaces in Go ----------------------------- An interface is a type that attaches to another type and describes what the type can do. A simple example is building a printer scanner fax interface. type Printer interface{ Print() string type Scanner interface { Scan() string } type Faxer interface { Fax() string } Interface joiners type PrinterScanner interface { Printer Scanner } type FaxerPrinterScanner interface { Faxer Printer Scanner } Functions using the interfaces type myPrinter struct {} func(mp myPrinter) Print() string{ return \"printing one page\" func (mp myPrinter) Scan() string { return \"Scanned One Page\" } func (my myPrinter) Fax() string { return \"Faxed One Pages\" } type secondPrinter struct{} func (sp secondPrinter) Print() string { return \"Printing five pages\" } func (sp secondPrinter) Scan() String { return \"Scanned five pages\" } func (sp secondPrinter) Fax() string { return \"Faxed Five Pages\" } func process(equipment FaxerPrinterScanner){ fmt.Println(\"Running Print, equipment.Print()) fmt.Println(\"Running Scan, equipment.Scan()) fmt.Println(\"Running Fax, equipment.Fax()) } func main() { printer := myPrinter{} otherPrinter := secondPrinter{} process(printer)","title":"Learning Go [Part 4]"},{"location":"zet/20210708121331/#learning-go-part-4","text":"","title":"Learning Go [Part 4]"},{"location":"zet/20210708121331/#-interfaces-in-go-","text":"An interface is a type that attaches to another type and describes what the type can do. A simple example is building a printer scanner fax interface. type Printer interface{ Print() string type Scanner interface { Scan() string } type Faxer interface { Fax() string } Interface joiners type PrinterScanner interface { Printer Scanner } type FaxerPrinterScanner interface { Faxer Printer Scanner } Functions using the interfaces type myPrinter struct {} func(mp myPrinter) Print() string{ return \"printing one page\" func (mp myPrinter) Scan() string { return \"Scanned One Page\" } func (my myPrinter) Fax() string { return \"Faxed One Pages\" } type secondPrinter struct{} func (sp secondPrinter) Print() string { return \"Printing five pages\" } func (sp secondPrinter) Scan() String { return \"Scanned five pages\" } func (sp secondPrinter) Fax() string { return \"Faxed Five Pages\" } func process(equipment FaxerPrinterScanner){ fmt.Println(\"Running Print, equipment.Print()) fmt.Println(\"Running Scan, equipment.Scan()) fmt.Println(\"Running Fax, equipment.Fax()) } func main() { printer := myPrinter{} otherPrinter := secondPrinter{} process(printer)","title":"----------------------------- Interfaces in Go -----------------------------"},{"location":"zet/20210712032748/","text":"Fun findings in vim Below are just some tidbit vim findings that may or may not help with my vim learning. Keeping the scroll in the middle of the screen We can set the sidescrolloff and scrolloff to 999 to keep the cursor in the middle ofthe screen. File completion and command completion If you want to have command completion or folder completion in vim you can use while in insert mode ctrl-x ctrl-f to complete files ctrl-x ctrl-p to give selection of completion This will give you a search bar which would link to different variables that you have already defined.","title":"Fun findings in vim"},{"location":"zet/20210712032748/#fun-findings-in-vim","text":"Below are just some tidbit vim findings that may or may not help with my vim learning. Keeping the scroll in the middle of the screen We can set the sidescrolloff and scrolloff to 999 to keep the cursor in the middle ofthe screen. File completion and command completion If you want to have command completion or folder completion in vim you can use while in insert mode ctrl-x ctrl-f to complete files ctrl-x ctrl-p to give selection of completion This will give you a search bar which would link to different variables that you have already defined.","title":"Fun findings in vim"},{"location":"zet/20210712041529/","text":"Expand multiple array strings with \"{A[@]}\" We can expand arrays in bash using multiple different methods. If we want to get all the string as one we can use the ${A[*]} to split by word. To get all the values of the array in one line. We can use \"${A[*]}\" . We can do \"${A[@]}\" to split by chucks. For example we have : A = [\"this is a string,\", \"and another one,\", \"last\"] - ${A[ ]} = This, is, a, string, and, another, one, last - \"${A[ ]}\" = \"this is a string, and another one, last\" - \"${A[@]}\" = this is a string and another one last","title":"Expand multiple array strings with \"{A[@]}\""},{"location":"zet/20210712041529/#expand-multiple-array-strings-with-a","text":"We can expand arrays in bash using multiple different methods. If we want to get all the string as one we can use the ${A[*]} to split by word. To get all the values of the array in one line. We can use \"${A[*]}\" . We can do \"${A[@]}\" to split by chucks. For example we have : A = [\"this is a string,\", \"and another one,\", \"last\"] - ${A[ ]} = This, is, a, string, and, another, one, last - \"${A[ ]}\" = \"this is a string, and another one, last\" - \"${A[@]}\" = this is a string and another one last","title":"Expand multiple array strings with \"{A[@]}\""},{"location":"zet/20210712044909/","text":"Useful bash tools If you want to use gzip to unzip a file in line and not actually unzip the file to a location, for example in a CTF you may wish to get the unzipped file and pipe it too another program you can use zcat.","title":"Useful bash tools"},{"location":"zet/20210712044909/#useful-bash-tools","text":"If you want to use gzip to unzip a file in line and not actually unzip the file to a location, for example in a CTF you may wish to get the unzipped file and pipe it too another program you can use zcat.","title":"Useful bash tools"},{"location":"zet/20210714023516/","text":"Fun things you can do in a jupyter notebook Jupyter notebook has some interesting shell integration. It is common knowledge that you can use the ! command to run a shell command. This is not limited to running ls and cd etc but any shell command that is on your path is able to be run. For example all my shell scripts are able to be run like the one below. !htitle title # ----------------------------------- title ---------------------------------- The benefits of this is that i can create scripts to do specific things for example working with repls on microcontrollers could be integrated into a standard python script using bash integration. Further to this any python variable is able to be piped to bash using the $<var> , this could then be used to pipe code blocks to a snippet file or similar to nbdev piping explicit code to a file to create python files from within jupyter notebooks. An example that i used to test this was calling a script to get a snippet of code. Then from there piped the output to a file using printf and >> -- !printf '%s\\n' $snippet > file.txt The possiblities of integrating with other programs and languages is quite useful.","title":"Fun things you can do in a jupyter notebook"},{"location":"zet/20210714023516/#fun-things-you-can-do-in-a-jupyter-notebook","text":"Jupyter notebook has some interesting shell integration. It is common knowledge that you can use the ! command to run a shell command. This is not limited to running ls and cd etc but any shell command that is on your path is able to be run. For example all my shell scripts are able to be run like the one below. !htitle title # ----------------------------------- title ---------------------------------- The benefits of this is that i can create scripts to do specific things for example working with repls on microcontrollers could be integrated into a standard python script using bash integration. Further to this any python variable is able to be piped to bash using the $<var> , this could then be used to pipe code blocks to a snippet file or similar to nbdev piping explicit code to a file to create python files from within jupyter notebooks. An example that i used to test this was calling a script to get a snippet of code. Then from there piped the output to a file using printf and >> -- !printf '%s\\n' $snippet > file.txt The possiblities of integrating with other programs and languages is quite useful.","title":"Fun things you can do in a jupyter notebook"},{"location":"zet/20210714235712/","text":"docker can set just internet access for the container If you want to set a docker container to give internet access you can set --network = bridge if you want complete access you can set --network = host . This is helpful if you want to provide open connectivity to the docker container and your computer acts as a router. Further research might be needed in this.","title":"docker can set just internet access for the container"},{"location":"zet/20210714235712/#docker-can-set-just-internet-access-for-the-container","text":"If you want to set a docker container to give internet access you can set --network = bridge if you want complete access you can set --network = host . This is helpful if you want to provide open connectivity to the docker container and your computer acts as a router. Further research might be needed in this.","title":"docker can set just internet access for the container"},{"location":"zet/20210718043805/","text":"Stat command will give the mode of a file We can use the stat command to check the mode of a file ie. 600 or 700. We can then use this programmatically for example if [ stat --format '%a' /swapfile -eg .... ] will allow us to check a files mode and run commands based on it.","title":"Stat command will give the mode of a file"},{"location":"zet/20210718043805/#stat-command-will-give-the-mode-of-a-file","text":"We can use the stat command to check the mode of a file ie. 600 or 700. We can then use this programmatically for example if [ stat --format '%a' /swapfile -eg .... ] will allow us to check a files mode and run commands based on it.","title":"Stat command will give the mode of a file"},{"location":"zet/20210718050805/","text":"Removing files or directories neatly Just found that you are able to use a nice shell snippet to remove either files, directories or symbolic links using some short shell scripting. for Dir in *; { [ -d \"$Dir\" ] || continue; rm \"$Dir\"; } This one liner checks all files in the directory and if it is a directory using the -d command or -f for files or -l for links. run the remove command otherwise ( || ) skip the file","title":"Removing files or directories neatly"},{"location":"zet/20210718050805/#removing-files-or-directories-neatly","text":"Just found that you are able to use a nice shell snippet to remove either files, directories or symbolic links using some short shell scripting. for Dir in *; { [ -d \"$Dir\" ] || continue; rm \"$Dir\"; } This one liner checks all files in the directory and if it is a directory using the -d command or -f for files or -l for links. run the remove command otherwise ( || ) skip the file","title":"Removing files or directories neatly"},{"location":"zet/20210718083855/","text":"Check and do something with a file based on its UUID. From Terminalforlife on youtube i found that you can write a shell script one liner to find files in a directory belonging to a specific user ID and do something with them. The example is shown below. sudo bash -c 'for File in *; { UID2=`stat -c \"%u\" \"$File\"`; [ $UID2 -eg 1001 ] && rm \"$File\"; } This script searches for all files in the folder and checks using stat for the UUID using the %u flag. Then tests against the required UUID . If this matches then do something. ie rm \"$File\"","title":"Check and do something with a file based on its UUID."},{"location":"zet/20210718083855/#check-and-do-something-with-a-file-based-on-its-uuid","text":"From Terminalforlife on youtube i found that you can write a shell script one liner to find files in a directory belonging to a specific user ID and do something with them. The example is shown below. sudo bash -c 'for File in *; { UID2=`stat -c \"%u\" \"$File\"`; [ $UID2 -eg 1001 ] && rm \"$File\"; } This script searches for all files in the folder and checks using stat for the UUID using the %u flag. Then tests against the required UUID . If this matches then do something. ie rm \"$File\"","title":"Check and do something with a file based on its UUID."},{"location":"zet/20210718142027/","text":"Shell variables can be assigned and given a value inside a printf It is quite simple to create a variable and give it a value directly inside a printf statement. printf -v Num '%02d' $Count will create a shell variable named Num and assign it the value of count. -- from Terminalforlife youtube, My shell Notes: Ep11","title":"Shell variables can be assigned and given a value inside a printf"},{"location":"zet/20210718142027/#shell-variables-can-be-assigned-and-given-a-value-inside-a-printf","text":"It is quite simple to create a variable and give it a value directly inside a printf statement. printf -v Num '%02d' $Count will create a shell variable named Num and assign it the value of count. -- from Terminalforlife youtube, My shell Notes: Ep11","title":"Shell variables can be assigned and given a value inside a printf"},{"location":"zet/20210719135154/","text":"bash for python There is a library in python called bash. This allows you to run bash commands in python. This will be beneficial when i want to create simple bash scripts to integrate with python. The library is [https://github.com/alexcouper/bash]","title":"bash for python"},{"location":"zet/20210719135154/#bash-for-python","text":"There is a library in python called bash. This allows you to run bash commands in python. This will be beneficial when i want to create simple bash scripts to integrate with python. The library is [https://github.com/alexcouper/bash]","title":"bash for python"},{"location":"zet/20210722005646/","text":"testing zet mods","title":"testing zet mods"},{"location":"zet/20210722005646/#testing-zet-mods","text":"","title":"testing zet mods"},{"location":"zet/20210722021227/","text":"Bash one liner to search zet titles and send to a selector. This bash one liner will search through the zet titles and open the folder to that directory. This one liner will be put in a command that allows me to search my zet folder for a specific topic and open that folder. I can also extend it to automatically open a vim instance. Will need to automate the uploading to github as the commit message should be reset to the title. `IFS=$'\\n'; select title in `zet titles`; do cd $KN/zet/${title%% *}; break; done`","title":"Bash one liner to search zet titles and send to a selector."},{"location":"zet/20210722021227/#bash-one-liner-to-search-zet-titles-and-send-to-a-selector","text":"This bash one liner will search through the zet titles and open the folder to that directory. This one liner will be put in a command that allows me to search my zet folder for a specific topic and open that folder. I can also extend it to automatically open a vim instance. Will need to automate the uploading to github as the commit message should be reset to the title. `IFS=$'\\n'; select title in `zet titles`; do cd $KN/zet/${title%% *}; break; done`","title":"Bash one liner to search zet titles and send to a selector."},{"location":"zet/20210722043533/","text":"add a newline to a variable. We can add a newline to the variable using subshell commands. nl=\"\\n\" title= ${desc%%$nl*}","title":"add a newline to a variable."},{"location":"zet/20210722043533/#add-a-newline-to-a-variable","text":"We can add a newline to the variable using subshell commands. nl=\"\\n\" title= ${desc%%$nl*}","title":"add a newline to a variable."},{"location":"zet/20210722060126/","text":"Again vim integration with bash for the win I was writing a command in the terminal to test some scripts. The script is below. I added piping to xclip. Then inside vim i used !!xclip -o to paste the output in line. Integrating vim with terminal is shining through again. Very much loving this process. One thing of issue is that i had to modify some of the resulting xclip as print and echo both did parameter expansion print url=$(cat stoprecord | curl -s -F 'f:1=<-' ix.io) | lynx $url | xclip","title":"Again vim integration with bash for the win"},{"location":"zet/20210722060126/#again-vim-integration-with-bash-for-the-win","text":"I was writing a command in the terminal to test some scripts. The script is below. I added piping to xclip. Then inside vim i used !!xclip -o to paste the output in line. Integrating vim with terminal is shining through again. Very much loving this process. One thing of issue is that i had to modify some of the resulting xclip as print and echo both did parameter expansion print url=$(cat stoprecord | curl -s -F 'f:1=<-' ix.io) | lynx $url | xclip","title":"Again vim integration with bash for the win"},{"location":"zet/20210722064031/","text":"Need to learn more about find I had a pile of zet folders with .index in them when i tried to integrate zets to nb. When i gave up on that i realised all of these had a .index file but i didnt want all these so i decided to use find to remove them. Using the below command i found all files in the directory with .index and piped to rm using xargs. find . -name '.index' | xargs rm However a twitch chat guy suggested doing it all with find as there is a delete flag in find. The below command is : find -type f -name '.index' -print -delete This is a fast and simple way to remove all the files.","title":"Need to learn more about find"},{"location":"zet/20210722064031/#need-to-learn-more-about-find","text":"I had a pile of zet folders with .index in them when i tried to integrate zets to nb. When i gave up on that i realised all of these had a .index file but i didnt want all these so i decided to use find to remove them. Using the below command i found all files in the directory with .index and piped to rm using xargs. find . -name '.index' | xargs rm However a twitch chat guy suggested doing it all with find as there is a delete flag in find. The below command is : find -type f -name '.index' -print -delete This is a fast and simple way to remove all the files.","title":"Need to learn more about find"},{"location":"zet/20210722132212/","text":"# Figured out what noremap means. In vim all mappings are recursive. If you map y to h using nmap for normal mode mapping. Each time you press y it will map to h and move you. However if you have a mapping that also maps h to something else like e , everytime you press y or h it will actually press e . This is where noremap comes in. It will stop vim from running the other remappings. So if we do noremap y h y will remap to h but if we had previously used a mapping it wont remap y to that mapping.","title":"Figured out what noremap means."},{"location":"zet/20210724044854/","text":"Line number wand !: magics If you would like to send a set of lines to a command for example sending from line 4 to 12 to a bash command. We can use !:13<enter> followed by bash or what ever command we wish to send the lines to that command. This is useful if i want to create a gist from a function.","title":"Line number wand !:<linenumber> magics"},{"location":"zet/20210724044854/#line-number-wand-magics","text":"If you would like to send a set of lines to a command for example sending from line 4 to 12 to a bash command. We can use !:13<enter> followed by bash or what ever command we wish to send the lines to that command. This is useful if i want to create a gist from a function.","title":"Line number wand !: magics"},{"location":"zet/20210725050018/","text":"Useful commands to memorize and use vim [part 1] I have been using vim for a while now and i am feeling i need to improve my vim commands to reduce overhead of keystrokes and wasted time. I am not too worried about time but i can be more efficient at writing code and notes then i dont have to stuff around with deleting letter by letter with x or j , k to move up and down. The commands to first learn are: <number>G to move to that location in the file. This is useful rather than using j and k to move. CTRL-O takes you to older positions in the file. Useful for when using searches. :w <filename> can be used with a selection to copy the contents to a location. :r <filename> will read the file in at the current line.","title":"Useful commands to memorize and use vim [part 1]"},{"location":"zet/20210725050018/#useful-commands-to-memorize-and-use-vim-part-1","text":"I have been using vim for a while now and i am feeling i need to improve my vim commands to reduce overhead of keystrokes and wasted time. I am not too worried about time but i can be more efficient at writing code and notes then i dont have to stuff around with deleting letter by letter with x or j , k to move up and down. The commands to first learn are: <number>G to move to that location in the file. This is useful rather than using j and k to move. CTRL-O takes you to older positions in the file. Useful for when using searches. :w <filename> can be used with a selection to copy the contents to a location. :r <filename> will read the file in at the current line.","title":"Useful commands to memorize and use vim [part 1]"},{"location":"zet/20210725054437/","text":"Learning vim copy, paste and registers to stop deleting things Often times i am editing a file and yank a line only to press dd and overwrite the yanked line. To overcome this i need to use \" followed by a letter. This can be used with y to yank and p to paste. This allows me to save multiple items to an array of clipboard locations and call them back later. The registers are used to store cut and copied information as well as macros. It is wise to use some level of design to keep and use both macros and yanks without overwriting one or the other.","title":"Learning vim copy, paste and registers to stop deleting things"},{"location":"zet/20210725054437/#learning-vim-copy-paste-and-registers-to-stop-deleting-things","text":"Often times i am editing a file and yank a line only to press dd and overwrite the yanked line. To overcome this i need to use \" followed by a letter. This can be used with y to yank and p to paste. This allows me to save multiple items to an array of clipboard locations and call them back later. The registers are used to store cut and copied information as well as macros. It is wise to use some level of design to keep and use both macros and yanks without overwriting one or the other.","title":"Learning vim copy, paste and registers to stop deleting things"},{"location":"zet/20210725055000/","text":"Create vim batch scripts to run multiple commands in a row. If we have multiple commands that we wish to run in ex mode we can place them in a script and source the script on the current buffer using :source <filename> . This is handy if we want to do multiple things on the same file.","title":"Create vim batch scripts to run multiple commands in a row."},{"location":"zet/20210725055000/#create-vim-batch-scripts-to-run-multiple-commands-in-a-row","text":"If we have multiple commands that we wish to run in ex mode we can place them in a script and source the script on the current buffer using :source <filename> . This is handy if we want to do multiple things on the same file.","title":"Create vim batch scripts to run multiple commands in a row."},{"location":"zet/20210725055321/","text":"We can operate on search matches using gn in vim If we search for a word in a file we can work on the search with gn we can then use the . command to repeat the last edit on each match. This could be useful to add a word as part of the search. For example if we search for files and want to change certain matches we can use gn to add local to the name.","title":"We can operate on search matches using gn in vim"},{"location":"zet/20210725055321/#we-can-operate-on-search-matches-using-gn-in-vim","text":"If we search for a word in a file we can work on the search with gn we can then use the . command to repeat the last edit on each match. This could be useful to add a word as part of the search. For example if we search for files and want to change certain matches we can use gn to add local to the name.","title":"We can operate on search matches using gn in vim"},{"location":"zet/20210725133814/","text":"chroot, namespaces and cgroups. In linux we can isolate a process and its children using chroot which effectively isolates the system from accessing anything at all. For example we may want to isolate an unknown program from the main system or fix a broken install we can use chroot. However it is of note that this jail has no access to any files including things like ls and grep so we need to add them in and their dependencies by cp the files from bin . We also need to check dependencies with ldd . This doesnt completely block the user as the jailed root can still see the processes on the system. To fix this we can create namespaces. To do this we install debootstrap and run unshare --mount --uts --ipc --net --pid --fork --user --map-root-user chroot /better-root bash and mount individually mounr proc , sys and tmp to create an isolated user. However this doesnt completely stop issues on a multi user server. If we want to truly isolate users we can use cgroups which set the amount of processes available for the user including CPU and RAM etc. To configure this checkout [https://btholt.github.io/complete-intro-to-containers/cgroups]","title":"chroot, namespaces and cgroups."},{"location":"zet/20210725133814/#chroot-namespaces-and-cgroups","text":"In linux we can isolate a process and its children using chroot which effectively isolates the system from accessing anything at all. For example we may want to isolate an unknown program from the main system or fix a broken install we can use chroot. However it is of note that this jail has no access to any files including things like ls and grep so we need to add them in and their dependencies by cp the files from bin . We also need to check dependencies with ldd . This doesnt completely block the user as the jailed root can still see the processes on the system. To fix this we can create namespaces. To do this we install debootstrap and run unshare --mount --uts --ipc --net --pid --fork --user --map-root-user chroot /better-root bash and mount individually mounr proc , sys and tmp to create an isolated user. However this doesnt completely stop issues on a multi user server. If we want to truly isolate users we can use cgroups which set the amount of processes available for the user including CPU and RAM etc. To configure this checkout [https://btholt.github.io/complete-intro-to-containers/cgroups]","title":"chroot, namespaces and cgroups."},{"location":"zet/20210725134549/","text":"Docker runs chroot, namespaces and cgroups along with other things. Docker essentially runs these tools to isolate the container from the main OS. It also sets up networking and other things. But essentially you can play around with this idea by exporting a docker container and untaring the filesystem. Once done you can chroot into the space and play around with it. [https://btholt.github.io/complete-intro-to-containers/docker-images-without-docker]","title":"Docker runs chroot, namespaces and cgroups along with other things."},{"location":"zet/20210725134549/#docker-runs-chroot-namespaces-and-cgroups-along-with-other-things","text":"Docker essentially runs these tools to isolate the container from the main OS. It also sets up networking and other things. But essentially you can play around with this idea by exporting a docker container and untaring the filesystem. Once done you can chroot into the space and play around with it. [https://btholt.github.io/complete-intro-to-containers/docker-images-without-docker]","title":"Docker runs chroot, namespaces and cgroups along with other things."},{"location":"zet/20210726053841/","text":"Integrating ? lynx search into vim and tmux I have been looking at a way to integrate lynx searching into my vim workflow by having the ability to open a search from normal mode. The script is seen below. This script checks the current window for whether or not it is a vim window. ( Careful if you change the window name this wont completely work). It also checks whether you have an active window at window 9, if so it will exit the window. It will then create a new window in the current session at window 9. Then exec lynx and the search. Once done it checks whether the window that lynx was executed form was a vim session if so it sends enter to the vim window to fix the issue with \"Press enter to continue\" message that vim sends when you open a new program. #!/bin/sh val=`tmux lsw -F '#{window_name}#{window_active}'|sed -n 's|^\\(.*\\)1$|\\1|p'` tmux has-session -t $session:9 2>/dev/null if [ $? = 0 ]; then tmux send-keys -t $session:9 \"q exit\" fi tmux new-window -t $session:9 tmux send -t $session:9 \"exec lynx \\ 'https://lite.duckduckgo.com/lite?kd=-1&kp=-1&q=$*'\" Enter tmux select-window -t $session:9 if [ $val = \"vim\" ]; then echo \"vim is here\" tmux send-keys -t $session:$val \"C-m\" fi","title":"Integrating ? lynx search into vim and tmux"},{"location":"zet/20210726053841/#integrating-lynx-search-into-vim-and-tmux","text":"I have been looking at a way to integrate lynx searching into my vim workflow by having the ability to open a search from normal mode. The script is seen below. This script checks the current window for whether or not it is a vim window. ( Careful if you change the window name this wont completely work). It also checks whether you have an active window at window 9, if so it will exit the window. It will then create a new window in the current session at window 9. Then exec lynx and the search. Once done it checks whether the window that lynx was executed form was a vim session if so it sends enter to the vim window to fix the issue with \"Press enter to continue\" message that vim sends when you open a new program. #!/bin/sh val=`tmux lsw -F '#{window_name}#{window_active}'|sed -n 's|^\\(.*\\)1$|\\1|p'` tmux has-session -t $session:9 2>/dev/null if [ $? = 0 ]; then tmux send-keys -t $session:9 \"q exit\" fi tmux new-window -t $session:9 tmux send -t $session:9 \"exec lynx \\ 'https://lite.duckduckgo.com/lite?kd=-1&kp=-1&q=$*'\" Enter tmux select-window -t $session:9 if [ $val = \"vim\" ]; then echo \"vim is here\" tmux send-keys -t $session:$val \"C-m\" fi","title":"Integrating ? lynx search into vim and tmux"},{"location":"zet/20210727002705/","text":"pywal is a nice tool to generate terminal colour schemes I have set up pywal to autogenerate a new terminal scheme everytime i run the command nwal. This is integrated with urxvt to create transparent terminals. I have an alias for wal to be automatically linked to my wallpapers folder and to output to /dev/null the stdout. he script i have autoruns pywal on the wallpaper folder with -a set to 70% for terminal transparency. To install run python3 -m pip install pywal The nwal script is seen below. #!/bin/sh wal -a 70 -i ~/wallpapers/wallpapers >>/dev/null","title":"pywal is a nice tool to generate terminal colour schemes"},{"location":"zet/20210727002705/#pywal-is-a-nice-tool-to-generate-terminal-colour-schemes","text":"I have set up pywal to autogenerate a new terminal scheme everytime i run the command nwal. This is integrated with urxvt to create transparent terminals. I have an alias for wal to be automatically linked to my wallpapers folder and to output to /dev/null the stdout. he script i have autoruns pywal on the wallpaper folder with -a set to 70% for terminal transparency. To install run python3 -m pip install pywal The nwal script is seen below. #!/bin/sh wal -a 70 -i ~/wallpapers/wallpapers >>/dev/null","title":"pywal is a nice tool to generate terminal colour schemes"},{"location":"zet/20210727083410/","text":"Use \"$(<\"file\")\" instead of cat inside scripts If you want to reduce the need for spawning subprocesses and piping the results of cat into a variable you can use \"$(<\"file\")\" to cat out the file. If you wish to read to an array you can use IFS=$'\\n' read -d \"\" -ra file_data < \"file\" or in later versions of bash mapfile -t file_data < \"file\"","title":"Use `\"$(<\"file\")\"` instead of cat inside scripts"},{"location":"zet/20210727083410/#use-file-instead-of-cat-inside-scripts","text":"If you want to reduce the need for spawning subprocesses and piping the results of cat into a variable you can use \"$(<\"file\")\" to cat out the file. If you wish to read to an array you can use IFS=$'\\n' read -d \"\" -ra file_data < \"file\" or in later versions of bash mapfile -t file_data < \"file\"","title":"Use \"$(&lt;\"file\")\" instead of cat inside scripts"},{"location":"zet/20210727084401/","text":"script to get a list of all the functions in a script This script i found may be useful to add to completion to scrape for the files. Not sure if it works with completion. Needs to be tested. get_functions() { # Usage: get_functions IFS=$'\\n' read -d \"\" -ra functions < <(declare -F) printf '%s\\n' \"${functions[@]//declare -f }\" }","title":"script to get a list of all the functions in a script"},{"location":"zet/20210727084401/#script-to-get-a-list-of-all-the-functions-in-a-script","text":"This script i found may be useful to add to completion to scrape for the files. Not sure if it works with completion. Needs to be tested. get_functions() { # Usage: get_functions IFS=$'\\n' read -d \"\" -ra functions < <(declare -F) printf '%s\\n' \"${functions[@]//declare -f }\" }","title":"script to get a list of all the functions in a script"},{"location":"zet/20210728050509/","text":"test of updated zet I added completion and other features to my zet and modified robs scripts. Hopefully this modification helps.","title":"test of updated zet"},{"location":"zet/20210728050509/#test-of-updated-zet","text":"I added completion and other features to my zet and modified robs scripts. Hopefully this modification helps.","title":"test of updated zet"},{"location":"zet/20210729102812/","text":"IO redirection in linux I often make a mistake when using these redirection commands. I have on numerous occasions accidentally overwrote a file with the result of a command. So i have outlined them below to remember them for the future. The first for standard out. We have two symbols. The first is > which writes the result of the left hand side into the right hand side. This is used for creating files mostly. The second symbol allows us to append to a file. This symbol is >> . The second set of symbols is for standard input.These symbols are the < and << symbol. The first is used in commands that accept input from stdin. We can redirect a file for example into the stdin. For example if we have a file with a list of values we can send it to sort with sort < file_list.txt . The next symbol is known as the here script or here document. This is denoted by << . This is used in conjunction with a token to denote the start and end of the information being fed into the command on the left hand side. For example we can run: cat << EOF This is a rhyme It doesn't need a line because we are out of time. damn fine. EOF In bash we also have <<< which means the here string, which will pass the words on the right to the standard input command on the left. We also have pipes | which send the standard out of one command to the stdin of another command. For example we can send: cat \"apple.txt\" | lolcat which will send the output of apple.txt to lolcat.","title":"IO redirection in linux"},{"location":"zet/20210729102812/#io-redirection-in-linux","text":"I often make a mistake when using these redirection commands. I have on numerous occasions accidentally overwrote a file with the result of a command. So i have outlined them below to remember them for the future. The first for standard out. We have two symbols. The first is > which writes the result of the left hand side into the right hand side. This is used for creating files mostly. The second symbol allows us to append to a file. This symbol is >> . The second set of symbols is for standard input.These symbols are the < and << symbol. The first is used in commands that accept input from stdin. We can redirect a file for example into the stdin. For example if we have a file with a list of values we can send it to sort with sort < file_list.txt . The next symbol is known as the here script or here document. This is denoted by << . This is used in conjunction with a token to denote the start and end of the information being fed into the command on the left hand side. For example we can run: cat << EOF This is a rhyme It doesn't need a line because we are out of time. damn fine. EOF In bash we also have <<< which means the here string, which will pass the words on the right to the standard input command on the left. We also have pipes | which send the standard out of one command to the stdin of another command. For example we can send: cat \"apple.txt\" | lolcat which will send the output of apple.txt to lolcat.","title":"IO redirection in linux"},{"location":"zet/20210729110512/","text":"Process substitution or <() The <() is known as process substitution which creates a file of the form /dev/fd/63 instead of using stdin , this makes a file called a named pipe. which redirects the output of the command to the named pipe. Often this is used in conjunction with a < to send the result of this into stdin of another function. This can also be used in a while loop where the results of the process substitution is fed into the loop. This would usually be fed into the read function. An example of this is: while read -r line; do [[ $line =~ ^declare\\ -f\\ x_ ]] || continue COMMANDS+=( ${line##declare -f x_} ) done < <(declare -F) This function is dense but basically it reads in all the functions of a file using <(declare -F), technically being saved to /dev/fd/63 or similar. This is piped in as stdin to the while loop. Inside the while loop we test whether the line starts with declare -f x_ if so then remove declare -f x_ from the line and save it into the COMMANDS array.","title":"Process substitution or <()"},{"location":"zet/20210729110512/#process-substitution-or","text":"The <() is known as process substitution which creates a file of the form /dev/fd/63 instead of using stdin , this makes a file called a named pipe. which redirects the output of the command to the named pipe. Often this is used in conjunction with a < to send the result of this into stdin of another function. This can also be used in a while loop where the results of the process substitution is fed into the loop. This would usually be fed into the read function. An example of this is: while read -r line; do [[ $line =~ ^declare\\ -f\\ x_ ]] || continue COMMANDS+=( ${line##declare -f x_} ) done < <(declare -F) This function is dense but basically it reads in all the functions of a file using <(declare -F), technically being saved to /dev/fd/63 or similar. This is piped in as stdin to the while loop. Inside the while loop we test whether the line starts with declare -f x_ if so then remove declare -f x_ from the line and save it into the COMMANDS array.","title":"Process substitution or &lt;()"},{"location":"zet/20210729112655/","text":"Brace expansion could be used for making files in directories Not just making files in directories but we can use brace expansion {} along with prefixes and suffixes to create names and strings. The main idea of this is that you begin with a prefix pre{ before the brace followed by comma separated strings or numbers. You can add [incr] inside to increment the values by the incr mark. Finally you can have a suffix following the brace }suff . Below is an example. echo b{a,e}n{a,u}n{a,e} resulting in: banana banane banuna banune benana benane benuna benune","title":"Brace expansion could be used for making files in directories"},{"location":"zet/20210729112655/#brace-expansion-could-be-used-for-making-files-in-directories","text":"Not just making files in directories but we can use brace expansion {} along with prefixes and suffixes to create names and strings. The main idea of this is that you begin with a prefix pre{ before the brace followed by comma separated strings or numbers. You can add [incr] inside to increment the values by the incr mark. Finally you can have a suffix following the brace }suff . Below is an example. echo b{a,e}n{a,u}n{a,e} resulting in: banana banane banuna banune benana benane benuna benune","title":"Brace expansion could be used for making files in directories"},{"location":"zet/20210729114017/","text":"Using lynx to its full potential. I have mentioned in the past about using Ctrl-n and Ctrl-p to go down and up respectively in lynx. We close lynx using q . There are many other things lynx can do much like using a normal browser, we can add sites to a bookmark file using A and when we open lynx again we can view these with V . If we want to open a specific site we can use g to go to the site. We can spacebar to go to the next screen and B to go back. The left arrow goes to the previously viewed link the right arrow goes to the selected link. The Up and Down arrows go to the links in either direction. The = sign will show the current website.","title":"Using lynx to its full potential."},{"location":"zet/20210729114017/#using-lynx-to-its-full-potential","text":"I have mentioned in the past about using Ctrl-n and Ctrl-p to go down and up respectively in lynx. We close lynx using q . There are many other things lynx can do much like using a normal browser, we can add sites to a bookmark file using A and when we open lynx again we can view these with V . If we want to open a specific site we can use g to go to the site. We can spacebar to go to the next screen and B to go back. The left arrow goes to the previously viewed link the right arrow goes to the selected link. The Up and Down arrows go to the links in either direction. The = sign will show the current website.","title":"Using lynx to its full potential."},{"location":"zet/20210729235836/","text":"Working with arrays in bash I found this awesome summary for working with arrays in bash. Syntax Result arr=() Create an empty array arr=(1 2 3) Initialize array ${arr[2]} Retrieve third element ${arr[@]} Retrieve all elements ${!arr[@]} Retrieve array indices ${#arr[@]} Calculate array size arr[0]=3 Overwrite 1st element arr+=(4) Append value(s) str=$(ls) Save ls output as a string arr=( $(ls) ) Save ls output as an array of files ${arr[@]:s:n} Retrieve n elements starting at index s","title":"Working with arrays in bash"},{"location":"zet/20210729235836/#working-with-arrays-in-bash","text":"I found this awesome summary for working with arrays in bash. Syntax Result arr=() Create an empty array arr=(1 2 3) Initialize array ${arr[2]} Retrieve third element ${arr[@]} Retrieve all elements ${!arr[@]} Retrieve array indices ${#arr[@]} Calculate array size arr[0]=3 Overwrite 1st element arr+=(4) Append value(s) str=$(ls) Save ls output as a string arr=( $(ls) ) Save ls output as an array of files ${arr[@]:s:n} Retrieve n elements starting at index s","title":"Working with arrays in bash"},{"location":"zet/20210730075532/","text":"Rob has an awesome help command that outputs as a man page While watching rwxrob today i noticed he had created an amazing help tool for use with his zettelkasten project. I had to borrow it. This tool will sit in my snippets and be added to any larger code base that i write. I also plan on pythonifying this code at some stage to use it as part of my python pipeline. The code is below. This should be linked with completion and delegation which i have stored as scripts in my snippets repo. These scripts will form part of a base for all my bash scripts. This first block creates a pandoc designed man page for the command that is called. _make_html() { local name=${1:-main} local title=\"$EXE $name\" [[ $name = main ]] && title=\"$EXE\" pandoc -s --metadata title=\"$title\" \\ -o \"/tmp/$name-help.html\" <<< \"${help[$name]}\" } The next command opens the web page for the designated command when called. This can be done by sending help . x_help() { local name=${1:-main} local file=\"/tmp/$name-help.html\" if [[ -n \"$HELP_BROWSER\" ]];then _make_html \"$name\" exec \"$HELP_BROWSER\" \"$file\" else pandoc -s -t plain <<< \"${help[${1:-main}]}\" | more fi } To write a help description we initially declare help with declare -A help and populate it with help[command]='This is the help for this command' We can also add a little bit of fanciness to the default call of a script with the below command. x_usage() { local cmds=\"${COMMANDS[*]}\" printf \"usage: %s (%s)\\n\" \"${0##*/}\" \"${cmds// /|}\" } It will produce a nice usage list of all the methods.","title":"Rob has an awesome help command that outputs as a man page"},{"location":"zet/20210730075532/#rob-has-an-awesome-help-command-that-outputs-as-a-man-page","text":"While watching rwxrob today i noticed he had created an amazing help tool for use with his zettelkasten project. I had to borrow it. This tool will sit in my snippets and be added to any larger code base that i write. I also plan on pythonifying this code at some stage to use it as part of my python pipeline. The code is below. This should be linked with completion and delegation which i have stored as scripts in my snippets repo. These scripts will form part of a base for all my bash scripts. This first block creates a pandoc designed man page for the command that is called. _make_html() { local name=${1:-main} local title=\"$EXE $name\" [[ $name = main ]] && title=\"$EXE\" pandoc -s --metadata title=\"$title\" \\ -o \"/tmp/$name-help.html\" <<< \"${help[$name]}\" } The next command opens the web page for the designated command when called. This can be done by sending help . x_help() { local name=${1:-main} local file=\"/tmp/$name-help.html\" if [[ -n \"$HELP_BROWSER\" ]];then _make_html \"$name\" exec \"$HELP_BROWSER\" \"$file\" else pandoc -s -t plain <<< \"${help[${1:-main}]}\" | more fi } To write a help description we initially declare help with declare -A help and populate it with help[command]='This is the help for this command' We can also add a little bit of fanciness to the default call of a script with the below command. x_usage() { local cmds=\"${COMMANDS[*]}\" printf \"usage: %s (%s)\\n\" \"${0##*/}\" \"${cmds// /|}\" } It will produce a nice usage list of all the methods.","title":"Rob has an awesome help command that outputs as a man page"},{"location":"zet/20210731064656/","text":"vim copy and paste the right way The best way to copy a section of code is to save the section to a /tmp/<file> using :w /tmp/foo1 and then we write it back out using :r /tmp/foo . Another useful shortcut is to just use !<number>! and remove the last ! rather than use : We can also use the viminfo file to save the snippet. However i do like the idea of making temp files for things i want to do.","title":"vim copy and paste the right way"},{"location":"zet/20210731064656/#vim-copy-and-paste-the-right-way","text":"The best way to copy a section of code is to save the section to a /tmp/<file> using :w /tmp/foo1 and then we write it back out using :r /tmp/foo . Another useful shortcut is to just use !<number>! and remove the last ! rather than use : We can also use the viminfo file to save the snippet. However i do like the idea of making temp files for things i want to do.","title":"vim copy and paste the right way"},{"location":"zet/20210731124101/","text":"Use zz in vim to recenter your screen I found out today that you can recenter your screen with zz . This is helpful if you are working over a large code base and you want the focus to be in the center. zz seems like a simple command to add into the memorization process.","title":"Use `zz` in vim to recenter your screen"},{"location":"zet/20210731124101/#use-zz-in-vim-to-recenter-your-screen","text":"I found out today that you can recenter your screen with zz . This is helpful if you are working over a large code base and you want the focus to be in the center. zz seems like a simple command to add into the memorization process.","title":"Use zz in vim to recenter your screen"},{"location":"zet/20210803045213/","text":"Got popos installed on my surface pro I found a community project that configured linux for surface touch devices. This is great i was able to get my old unused surface pro back to a useful state with this configuration. The git repo i followed is https://github.com/linux-surface/linux-surface/wiki/Installation-and-Setup","title":"Got popos installed on my surface pro"},{"location":"zet/20210803045213/#got-popos-installed-on-my-surface-pro","text":"I found a community project that configured linux for surface touch devices. This is great i was able to get my old unused surface pro back to a useful state with this configuration. The git repo i followed is https://github.com/linux-surface/linux-surface/wiki/Installation-and-Setup","title":"Got popos installed on my surface pro"},{"location":"zet/20210804064214/","text":"Understanding what !} does If i have a group of lines that i want to send to a shell script if i type !} it will send the current line and the rest of the lines in a 'paragraph'. ie same level of indentation. The command converts to ed commands where .,.+8 . is current line , is too .+8 lines and send to !(script) This allows you to really benefit from the Unix philosophy where you can pipe inputs and outputs to other commands.","title":"Understanding what !} does"},{"location":"zet/20210804064214/#understanding-what-does","text":"If i have a group of lines that i want to send to a shell script if i type !} it will send the current line and the rest of the lines in a 'paragraph'. ie same level of indentation. The command converts to ed commands where .,.+8 . is current line , is too .+8 lines and send to !(script) This allows you to really benefit from the Unix philosophy where you can pipe inputs and outputs to other commands.","title":"Understanding what !} does"},{"location":"zet/20210804084552/","text":"Modifying robs latest zet for my pipeline. Robs zet is becoming really impressive. Some functionality i don't need but i did have some issues moving his code to my base. The first issue was that i didnt have /usr/bin/bash This was simple to fix. The second more subtle issue was the default path for zet. For some reason my zet doesn't need /zet at the end of zetdir . The next issue i noticed was that rob has hardcoded main into the web access scripts. Instead i updated the script to use the GITBRANCH env variable. For reference the git commit is https://github.com/rwxrob/dot/commit/9ab62255d7d95eb72f01bf0d3f4d03f5453d8143","title":"Modifying robs latest zet for my pipeline."},{"location":"zet/20210804084552/#modifying-robs-latest-zet-for-my-pipeline","text":"Robs zet is becoming really impressive. Some functionality i don't need but i did have some issues moving his code to my base. The first issue was that i didnt have /usr/bin/bash This was simple to fix. The second more subtle issue was the default path for zet. For some reason my zet doesn't need /zet at the end of zetdir . The next issue i noticed was that rob has hardcoded main into the web access scripts. Instead i updated the script to use the GITBRANCH env variable. For reference the git commit is https://github.com/rwxrob/dot/commit/9ab62255d7d95eb72f01bf0d3f4d03f5453d8143","title":"Modifying robs latest zet for my pipeline."},{"location":"zet/20210804123008/","text":"Using read correctly in bash read is a powerful tool in bash. It will read from stdin if you don't provide anything to the call. To read from something else we can use < <file> which will be read into the variable in the while loop. example: while read -r line; do done < /etc/resolv.conf The -r makes sure it doesn't read ``` as an escape character which is useful when reading files.","title":"Using read correctly in bash"},{"location":"zet/20210804123008/#using-read-correctly-in-bash","text":"read is a powerful tool in bash. It will read from stdin if you don't provide anything to the call. To read from something else we can use < <file> which will be read into the variable in the while loop. example: while read -r line; do done < /etc/resolv.conf The -r makes sure it doesn't read ``` as an escape character which is useful when reading files.","title":"Using read correctly in bash"},{"location":"zet/20210804125600/","text":"Using recursive descent in bash We can use the continue statement to break out of logic if true. This is useful if you are parsing some lines of input. For example you may want to first check if it contains a # . If it doesn't then we continue . If it does then we check the next character and follow that logic. For more explanation about this checkout [https://www.youtube.com/watch?v=o_U76-7LmTQ] ~45mins.","title":"Using recursive descent in bash"},{"location":"zet/20210804125600/#using-recursive-descent-in-bash","text":"We can use the continue statement to break out of logic if true. This is useful if you are parsing some lines of input. For example you may want to first check if it contains a # . If it doesn't then we continue . If it does then we check the next character and follow that logic. For more explanation about this checkout [https://www.youtube.com/watch?v=o_U76-7LmTQ] ~45mins.","title":"Using recursive descent in bash"},{"location":"zet/20210804130056/","text":"Using =~ in bash starts regex matching If you want to use regex in bash we write the variable the result is being sent to followed by =~ to be able to do regex. It is of note that bash only as extended regex and not PCRE .","title":"Using `=~` in bash starts regex matching"},{"location":"zet/20210804130056/#using-in-bash-starts-regex-matching","text":"If you want to use regex in bash we write the variable the result is being sent to followed by =~ to be able to do regex. It is of note that bash only as extended regex and not PCRE .","title":"Using =~ in bash starts regex matching"},{"location":"zet/20210804131951/","text":"Rather than using true and false in bash use is empty Watching through robs scripting of dns resolv swap. It was mentioned that it is better in bash and sh to use is empty and contains stuff to show true and false. This can be done with the -n flag, for example: var=\"y\" [[ -n $var ]] && echo \"Contains text\" var=\"\" [[ -n $var ]] && echo \"empty\" Will print Contains text after the first var is initialised. Once var is changed the statement is false and empty wont be shown.","title":"Rather than using true and false in bash use is empty"},{"location":"zet/20210804131951/#rather-than-using-true-and-false-in-bash-use-is-empty","text":"Watching through robs scripting of dns resolv swap. It was mentioned that it is better in bash and sh to use is empty and contains stuff to show true and false. This can be done with the -n flag, for example: var=\"y\" [[ -n $var ]] && echo \"Contains text\" var=\"\" [[ -n $var ]] && echo \"empty\" Will print Contains text after the first var is initialised. Once var is changed the statement is false and empty wont be shown.","title":"Rather than using true and false in bash use is empty"},{"location":"zet/20210805030535/","text":"Deciphering robs filter function Rob has a filter function that is used in his scripts. From what i can tell it allows the script to run multiple calls recursively. I will need to think on this function some more. The function is: _filter(){ (( $# > 0 )) && return 0 while IFS= read -ra args; do \"${FUNCNAME[1]}\" \"${args[@]}\" done } Robs run through of the script is below. if args greater than 0 just return. if you don't have an argument loop through standard input put into array FUNCNAME[1] calls the caller and passes the arguments into it. Uses recursive function call","title":"Deciphering robs filter function"},{"location":"zet/20210805030535/#deciphering-robs-filter-function","text":"Rob has a filter function that is used in his scripts. From what i can tell it allows the script to run multiple calls recursively. I will need to think on this function some more. The function is: _filter(){ (( $# > 0 )) && return 0 while IFS= read -ra args; do \"${FUNCNAME[1]}\" \"${args[@]}\" done } Robs run through of the script is below. if args greater than 0 just return. if you don't have an argument loop through standard input put into array FUNCNAME[1] calls the caller and passes the arguments into it. Uses recursive function call","title":"Deciphering robs filter function"},{"location":"zet/20210805064701/","text":"Using Ctrl-a z to focus pane in tmux Often times i am working in tmux and need to focus on one pane. The quick shortcut key for this is Ctrl-a z where Ctrl-a is the prefix key in my case. This shortcut makes it easy to work with multiple panes on the one window.","title":"Using Ctrl-a z to focus pane in tmux"},{"location":"zet/20210805064701/#using-ctrl-a-z-to-focus-pane-in-tmux","text":"Often times i am working in tmux and need to focus on one pane. The quick shortcut key for this is Ctrl-a z where Ctrl-a is the prefix key in my case. This shortcut makes it easy to work with multiple panes on the one window.","title":"Using Ctrl-a z to focus pane in tmux"},{"location":"zet/20210805073450/","text":"Issues with setting up different ssh keys. I was trying to set up an ssh connection to my newly formatted surface pro from my main workstation. Every time i tried to connect the system would send an authentication error message, too many login attempts or similar messages. This is issue as it turns out was caused by me having too many different ssh keys on my main machine. I was using multiple different keys for different interactions with some services. However following this i think i may remove the other ssh keys and just have one. You are able to get around this issue in a couple of ways as seen on (https://superuser.com/questions/187779/too-many-authentication-failures-for-username). You can set up in your ssh/config file two lines the first being the Identity file you are giving and a flag saying send only this key. Eg: Host www.myawesomesite.com IdentityFile ~/.ssh/id_key IdentitiesOnly yes Port xx If you don't use this method you are able to use flags in the ssh call. We can use -i to indicate the identity file and -o IdentitiesOnly=yes This will stop the issue from occurring. If you wish to see when this actually happens you are able to use -v for verbose mode.","title":"Issues with setting up different ssh keys."},{"location":"zet/20210805073450/#issues-with-setting-up-different-ssh-keys","text":"I was trying to set up an ssh connection to my newly formatted surface pro from my main workstation. Every time i tried to connect the system would send an authentication error message, too many login attempts or similar messages. This is issue as it turns out was caused by me having too many different ssh keys on my main machine. I was using multiple different keys for different interactions with some services. However following this i think i may remove the other ssh keys and just have one. You are able to get around this issue in a couple of ways as seen on (https://superuser.com/questions/187779/too-many-authentication-failures-for-username). You can set up in your ssh/config file two lines the first being the Identity file you are giving and a flag saying send only this key. Eg: Host www.myawesomesite.com IdentityFile ~/.ssh/id_key IdentitiesOnly yes Port xx If you don't use this method you are able to use flags in the ssh call. We can use -i to indicate the identity file and -o IdentitiesOnly=yes This will stop the issue from occurring. If you wish to see when this actually happens you are able to use -v for verbose mode.","title":"Issues with setting up different ssh keys."},{"location":"zet/20210806114039/","text":"Nested tmux sessions can be accessed by pressing the prefix key again In the tao-of-tmux a note on nested tmux sessions is given. For example if you are in a tmux session and ssh into another computer. If that machine has tmux running aswell to send tmux commands to it send the prefix key again. i.e. If i want to send a key press to the ssh session i would do Ctrl -b b .","title":"Nested tmux sessions can be accessed by pressing the prefix key again"},{"location":"zet/20210806114039/#nested-tmux-sessions-can-be-accessed-by-pressing-the-prefix-key-again","text":"In the tao-of-tmux a note on nested tmux sessions is given. For example if you are in a tmux session and ssh into another computer. If that machine has tmux running aswell to send tmux commands to it send the prefix key again. i.e. If i want to send a key press to the ssh session i would do Ctrl -b b .","title":"Nested tmux sessions can be accessed by pressing the prefix key again"},{"location":"zet/20210806114626/","text":"TMUX uses Prefix + [ to enter copy mode, Prefix + ] for paste tmux has a copy mode similar to vim and you need to enter the mode by pressing Ctrl-a [ Once inside you press space to enter copy mode. Use the arrow keys to select text and Enter to copy. To paste we use Ctrl-a ] .","title":"TMUX uses `Prefix + [` to enter copy mode, `Prefix + ]` for paste"},{"location":"zet/20210806114626/#tmux-uses-prefix-to-enter-copy-mode-prefix-for-paste","text":"tmux has a copy mode similar to vim and you need to enter the mode by pressing Ctrl-a [ Once inside you press space to enter copy mode. Use the arrow keys to select text and Enter to copy. To paste we use Ctrl-a ] .","title":"TMUX uses Prefix + [ to enter copy mode, Prefix + ] for paste"},{"location":"zet/20210806115250/","text":"tmux session basics We can create new sessions with tmux new-session . We can also switch sessions from inside tmux using. Prefix ( to attach to previous session. Prefix ) to attach to next session. Prefix L to attach to last session and Prefix s to select a new session for the attached client. Prefix s is similar to Prefix w to look at windows. To name a session we can use Prefix $ . We can also rename from the command line with tmux rename-session -t 1 \"my session\"","title":"tmux session basics"},{"location":"zet/20210806115250/#tmux-session-basics","text":"We can create new sessions with tmux new-session . We can also switch sessions from inside tmux using. Prefix ( to attach to previous session. Prefix ) to attach to next session. Prefix L to attach to last session and Prefix s to select a new session for the attached client. Prefix s is similar to Prefix w to look at windows. To name a session we can use Prefix $ . We can also rename from the command line with tmux rename-session -t 1 \"my session\"","title":"tmux session basics"},{"location":"zet/20210806120133/","text":"tmux windows basics To create a new window in tmux we use Prefix c , windows will be added where aothers have been removed. We can move between windows using Prefix p and Prefix n to move back and forward, we can use numbering instead and if the numbering is above 10 we can use Prefix ' to prompt for the window. If we wish to change the window layouts we use Prefix space which will change the layouts through the presets. We can set layouts in the config with commands such as bind m set-window-option main-pane-height 60; select-layout main-horizontal .","title":"tmux windows basics"},{"location":"zet/20210806120133/#tmux-windows-basics","text":"To create a new window in tmux we use Prefix c , windows will be added where aothers have been removed. We can move between windows using Prefix p and Prefix n to move back and forward, we can use numbering instead and if the numbering is above 10 we can use Prefix ' to prompt for the window. If we wish to change the window layouts we use Prefix space which will change the layouts through the presets. We can set layouts in the config with commands such as bind m set-window-option main-pane-height 60; select-layout main-horizontal .","title":"tmux windows basics"},{"location":"zet/20210807124152/","text":"How to save a file that required sudo Just found this awesome tip for saving a file that needed sudo. We can use :w !sudo tee % , which will ask for the sudo password and save the file for you.","title":"How to save a file that required sudo"},{"location":"zet/20210807124152/#how-to-save-a-file-that-required-sudo","text":"Just found this awesome tip for saving a file that needed sudo. We can use :w !sudo tee % , which will ask for the sudo password and save the file for you.","title":"How to save a file that required sudo"},{"location":"zet/20210808020011/","text":"When using EOF line input use -. If you want to wrap a large amount of text in something like a printf statement rather than writing it all in one line use this: while read -r; do printf '%b\\n' \"$REPLY\" done <<-EOF Here is some text and some more Would you like them apples EOF This makes the text so much more readable.","title":"When using EOF line input use -."},{"location":"zet/20210808020011/#when-using-eof-line-input-use-","text":"If you want to wrap a large amount of text in something like a printf statement rather than writing it all in one line use this: while read -r; do printf '%b\\n' \"$REPLY\" done <<-EOF Here is some text and some more Would you like them apples EOF This makes the text so much more readable.","title":"When using EOF line input use -."},{"location":"zet/20210809122108/","text":"Use () to turn the output of a function into an array. When i was trying to get the output of a grep command, essentially being used as ls. I wanted to feed the result into an array but couldn't get it to work. The simplest solution was to (``) the whole subroutine. This gave me the output i needed.","title":"Use () to turn the output of a function into an array."},{"location":"zet/20210809122108/#use-to-turn-the-output-of-a-function-into-an-array","text":"When i was trying to get the output of a grep command, essentially being used as ls. I wanted to feed the result into an array but couldn't get it to work. The simplest solution was to (``) the whole subroutine. This gave me the output i needed.","title":"Use () to turn the output of a function into an array."},{"location":"zet/20210810052344/","text":"Found an excellent theme to use in my blog posts https://github.com/gyorb/hugo-dusk is an excellent simple theming for use on my blog platform. I however need to learn about hugo and a bit more about GO to see which is the best way to go.","title":"Found an excellent theme to use in my blog posts"},{"location":"zet/20210810052344/#found-an-excellent-theme-to-use-in-my-blog-posts","text":"https://github.com/gyorb/hugo-dusk is an excellent simple theming for use on my blog platform. I however need to learn about hugo and a bit more about GO to see which is the best way to go.","title":"Found an excellent theme to use in my blog posts"},{"location":"zet/20210811015906/","text":"Working with the bash completion tools from rwxrob One of the commands rob uses will check if the input is the executable name which will then run that command. This is useful for setting a default option. For example i have an ix2me command that will run the command at command line with ix2me <url> . This can also be run by ix2me ix2me <url> . The benefit of expanding these programs is to generate a help functionality into all the scripts. The function is shown below: for c in \"${COMMANDS[@]}\"; do if [[ $c == \"$EXE\" ]]; then \"x_$EXE\" \"$@\" exit $? fi done","title":"Working with the bash completion tools from rwxrob"},{"location":"zet/20210811015906/#working-with-the-bash-completion-tools-from-rwxrob","text":"One of the commands rob uses will check if the input is the executable name which will then run that command. This is useful for setting a default option. For example i have an ix2me command that will run the command at command line with ix2me <url> . This can also be run by ix2me ix2me <url> . The benefit of expanding these programs is to generate a help functionality into all the scripts. The function is shown below: for c in \"${COMMANDS[@]}\"; do if [[ $c == \"$EXE\" ]]; then \"x_$EXE\" \"$@\" exit $? fi done","title":"Working with the bash completion tools from rwxrob"},{"location":"zet/20210812113523/","text":"I have reached a point in my linux understanding Today i was working through cleaning up CDPATH when i realised when i CD 'd into scripts i had somehow moved into the scripts symlinked inside the scripts folder. I didn't know whether to remove it or not and was hesitant about removing it, i decided for safety i would confirm the file i was going to delete was symlinked and used short circuit logic to check that it was a symlink and if so remove the folder. This was done with [[ -L ./scripts ]] && rm -rf ./scripts A simple in line shell script to confirm i was deleting the write file and not my scripts directory.","title":"I have reached a point in my linux understanding"},{"location":"zet/20210812113523/#i-have-reached-a-point-in-my-linux-understanding","text":"Today i was working through cleaning up CDPATH when i realised when i CD 'd into scripts i had somehow moved into the scripts symlinked inside the scripts folder. I didn't know whether to remove it or not and was hesitant about removing it, i decided for safety i would confirm the file i was going to delete was symlinked and used short circuit logic to check that it was a symlink and if so remove the folder. This was done with [[ -L ./scripts ]] && rm -rf ./scripts A simple in line shell script to confirm i was deleting the write file and not my scripts directory.","title":"I have reached a point in my linux understanding"},{"location":"zet/20210812114216/","text":"Working with CDPATH. For a while i couldn't understand why CDPATH wasn't working in my bashrc export CDPATH=.:\\ ~/repos/github.com:\\ ~/repos/github.com/$GITUSER:\\ ~/repos/github.com/$GITUSER/dot:\\ ~/repos:\\ /media/$USER:\\ /mnt/SSD:\\ ~/.local/bin:\\ ~","title":"Working with CDPATH."},{"location":"zet/20210812114216/#working-with-cdpath","text":"For a while i couldn't understand why CDPATH wasn't working in my bashrc export CDPATH=.:\\ ~/repos/github.com:\\ ~/repos/github.com/$GITUSER:\\ ~/repos/github.com/$GITUSER/dot:\\ ~/repos:\\ /media/$USER:\\ /mnt/SSD:\\ ~/.local/bin:\\ ~","title":"Working with CDPATH."},{"location":"zet/20210814113122/","text":"Thought i'd lost all my photos on a drive today I tried the solution suggested online sudo ntfsfix /dev/sda1 and it worked. This not is here in case i need to fix it again in the future.","title":"Thought i'd lost all my photos on a drive today"},{"location":"zet/20210814113122/#thought-id-lost-all-my-photos-on-a-drive-today","text":"I tried the solution suggested online sudo ntfsfix /dev/sda1 and it worked. This not is here in case i need to fix it again in the future.","title":"Thought i'd lost all my photos on a drive today"},{"location":"zet/20210816090512/","text":"ls -1 to list individual files per line Simple command addition to list one file per line we can use ls -1 to output the files in the folder one item at a time.","title":"ls -1 to list individual files per line"},{"location":"zet/20210816090512/#ls-1-to-list-individual-files-per-line","text":"Simple command addition to list one file per line we can use ls -1 to output the files in the folder one item at a time.","title":"ls -1 to list individual files per line"},{"location":"zet/20210816090915/","text":"Working with find and xargs Rather than using ls when scripting it is better to use the find command which will output the files in the current folder if the command is scripted correctly. If we use find -maxdepth 1 -type f -print0 we can get all files in the folder delimited by a null command. The term maxdepth stops recursion. type f will give only files and print0 outputs the results as null delimited. We can then send this into xargs using the command xargs -0 -n1 which will send the results of find one at a time to xargs The whole pipe: find -maxdepth 1 -type f -print0 | -xargs -0 -n1 -t","title":"Working with find and `xargs`"},{"location":"zet/20210816090915/#working-with-find-and-xargs","text":"Rather than using ls when scripting it is better to use the find command which will output the files in the current folder if the command is scripted correctly. If we use find -maxdepth 1 -type f -print0 we can get all files in the folder delimited by a null command. The term maxdepth stops recursion. type f will give only files and print0 outputs the results as null delimited. We can then send this into xargs using the command xargs -0 -n1 which will send the results of find one at a time to xargs The whole pipe: find -maxdepth 1 -type f -print0 | -xargs -0 -n1 -t","title":"Working with find and xargs"},{"location":"zet/20210817065558/","text":"Bash [ -* ] flags It is almost impossible to google the answer for these. However the man page is pretty good. Here are the answers -z contains values -n contains no value -f file -d directory -x executable","title":"Bash [ -* ] flags"},{"location":"zet/20210817065558/#bash-flags","text":"It is almost impossible to google the answer for these. However the man page is pretty good. Here are the answers -z contains values -n contains no value -f file -d directory -x executable","title":"Bash [ -* ] flags"},{"location":"zet/20210817234512/","text":"Use pipx for any python program Often times i find programs written in python that are installed using pip. This tool allows you to install the software as a completely encapsulated package and thus removing issues of dependencies and python version issues, * pipx","title":"Use `pipx` for any python program"},{"location":"zet/20210817234512/#use-pipx-for-any-python-program","text":"Often times i find programs written in python that are installed using pip. This tool allows you to install the software as a completely encapsulated package and thus removing issues of dependencies and python version issues, * pipx","title":"Use pipx for any python program"},{"location":"zet/20210817235043/","text":"You should use commonmark for markdown I have been writing somewhat incorrect markdown over the last couple of months especially when writing my zets. I have decided i need to know markdown correctly. I found a couple of good resources for markdown. Commonmark tutorial Commonmark cheatsheet","title":"You should use commonmark for markdown"},{"location":"zet/20210817235043/#you-should-use-commonmark-for-markdown","text":"I have been writing somewhat incorrect markdown over the last couple of months especially when writing my zets. I have decided i need to know markdown correctly. I found a couple of good resources for markdown. Commonmark tutorial Commonmark cheatsheet","title":"You should use commonmark for markdown"},{"location":"zet/20210818224321/","text":"Linux bash expansions There are many bash expansions available to help speed up movements and function calls though out linux and bash scripts. Below are some of these with their explanation. * - expands the search to include all characters that are available. For example if there is a folder with foo, bar and baz in it. If we run ls b* it will expand to ls bar baz . ~ - expands to the $HOME variable which is usually /home/ . ~+ - expands to $PWD which is the current working directory. ~- - expands to $OLDPWD which is the previous directory you were in. You can also cd to this with cd - {} - will expand what is inside the braces and add it to what is on the outside of the braces. For example if you wish to create a directory with multiple txt files you could use touch {foo,bar,baz}.txt , will expand to touch foo.txt bar.txt baz.txt , this also works with the suffixes or prefixes. {1..9} - will fill in the values in between 1 and 9. This could be used to create directories with numbers or letters at the start. We can multiple these together, for example {a..z}{a..z} will expand to a pair of letter combinations starting at aa and ending with zz. !! - Will rewrite the last command to the shell, this is called history expansion. This can be useful to pipe results into another program. {01..10} - will expand to 00 01 .. 10. {01..10..2} - will output only odd numbers starting at one. 01 03 05 07 09.","title":"Linux bash expansions"},{"location":"zet/20210818224321/#linux-bash-expansions","text":"There are many bash expansions available to help speed up movements and function calls though out linux and bash scripts. Below are some of these with their explanation. * - expands the search to include all characters that are available. For example if there is a folder with foo, bar and baz in it. If we run ls b* it will expand to ls bar baz . ~ - expands to the $HOME variable which is usually /home/ . ~+ - expands to $PWD which is the current working directory. ~- - expands to $OLDPWD which is the previous directory you were in. You can also cd to this with cd - {} - will expand what is inside the braces and add it to what is on the outside of the braces. For example if you wish to create a directory with multiple txt files you could use touch {foo,bar,baz}.txt , will expand to touch foo.txt bar.txt baz.txt , this also works with the suffixes or prefixes. {1..9} - will fill in the values in between 1 and 9. This could be used to create directories with numbers or letters at the start. We can multiple these together, for example {a..z}{a..z} will expand to a pair of letter combinations starting at aa and ending with zz. !! - Will rewrite the last command to the shell, this is called history expansion. This can be useful to pipe results into another program. {01..10} - will expand to 00 01 .. 10. {01..10..2} - will output only odd numbers starting at one. 01 03 05 07 09.","title":"Linux bash expansions"},{"location":"zet/20210818225907/","text":"bash variable expansions Here are some notes on variable expansions from the Youtuber nixcasts. We always want to wrap variable results in quotes as without it new lines and other things wont get expanded. Bash separates fields based on the internal field separator which is IFS . Arrays should be wrapped in () otherwise for example if we define an array with * for example in a directory with a.txt b.txt c.txt we actually expand to * b.txt c.txt which is incorrect. When we call the array we use \"${files[0]}\" where files is given as files=(*) We wrap variables in {} to be able to add other extensions to be added. For example we can add an s to the variable by having \"{file}s\" which will expand the file variable and add the s at the end. If this wasn't done we would end up with a different variable that it is trying to expand. We can use the {} to allow us to slice the variable. For example we can use a variable such as test=\"check it\" and call the variable with \"${test:0:5}\" which will only output check , this syntax is start at value 0 and output the next 5 characters. This is also seen with \"${test:6:2}\" which will show it The : will allow us to get certain elements of an array as well. We can have an array called files and \"${files[@]:3:6}\" will output 6 array values starting at array number 3. Bash is 0 start so it will start with the 4th value in the array. # the hash sign at the start of the array will output the size of the array. \"${#files[@]}\" . This also works to output length of variable. % will remove the value shown after the symbol. For example with the test variable above if we do \"${test% it}\" , the output will just show check . Percent works with the suffix. Using the # after the variable name will remove the prefix listed after the symbol. For example with test we can remove check with \"${files#check }\" where the trailing whitespace is part of the removal. The difference between % and %% is the % will remove the first match of the value following the % . eg. \"${test%c*}\" will result in che , remembering that the % matches from the end back. While %% will remove the last match that it finds. Which in this situation will be starting at the first letter and therefore there will be no values left. It is important to note that the * is matching any character. If we didn't have the * it wouldn't match and remove anything. We can upper case parts of the variable with numerous modifiers. The first ^ will upper case the first letter, if we do ^^ it will upper case all letters and if the value is followed by a character that letter will be upper cased through out the variable. If we use the ^ followed by a letter it will upper case the first instance of that letter. Alternatively we can do the , in the same way and it will lowercase the letters. we can swap cases using the ~ symbol, and all symbols with ~~ . If we want to do in place replacement of a variable rather than piping to sed we can do it directly in bash for example we can change our test variable with \"${test/hec/hoc}\" Similarly to sed if we want to find all occureances like in sed with the /g flag we can use // at the start. So to find and replace all c's in the test variable we can use \"${test//c/t}\" . We can also change the first value found in the line with \"${test/#check/black}\" which will change check it to black it. All of these can be done on arrays. If we wish to check if a variable is defined otherwise output a value. We can use the :- to output a different value if it is undefined. For example we have a variable set by empty= this variable is empty. We can provide a value or a response with echo \"${empty:-Not here.}\" to output that the variable is empty. The :+ does the opposite if the value is undefined use that otherwise use the value on the right side of the :+ . If the value is defined use the value on the right side. Using the :? will exit the script throwing the error written on the right side of the ? if the value is undefined. := will assign the value on the right to the variable on the left overwriting the current value. There is a lot available in bash to do fast and cool things and this resource is incredibly useful. To find out more watch the video Bash variable expansion","title":"bash variable expansions"},{"location":"zet/20210818225907/#bash-variable-expansions","text":"Here are some notes on variable expansions from the Youtuber nixcasts. We always want to wrap variable results in quotes as without it new lines and other things wont get expanded. Bash separates fields based on the internal field separator which is IFS . Arrays should be wrapped in () otherwise for example if we define an array with * for example in a directory with a.txt b.txt c.txt we actually expand to * b.txt c.txt which is incorrect. When we call the array we use \"${files[0]}\" where files is given as files=(*) We wrap variables in {} to be able to add other extensions to be added. For example we can add an s to the variable by having \"{file}s\" which will expand the file variable and add the s at the end. If this wasn't done we would end up with a different variable that it is trying to expand. We can use the {} to allow us to slice the variable. For example we can use a variable such as test=\"check it\" and call the variable with \"${test:0:5}\" which will only output check , this syntax is start at value 0 and output the next 5 characters. This is also seen with \"${test:6:2}\" which will show it The : will allow us to get certain elements of an array as well. We can have an array called files and \"${files[@]:3:6}\" will output 6 array values starting at array number 3. Bash is 0 start so it will start with the 4th value in the array. # the hash sign at the start of the array will output the size of the array. \"${#files[@]}\" . This also works to output length of variable. % will remove the value shown after the symbol. For example with the test variable above if we do \"${test% it}\" , the output will just show check . Percent works with the suffix. Using the # after the variable name will remove the prefix listed after the symbol. For example with test we can remove check with \"${files#check }\" where the trailing whitespace is part of the removal. The difference between % and %% is the % will remove the first match of the value following the % . eg. \"${test%c*}\" will result in che , remembering that the % matches from the end back. While %% will remove the last match that it finds. Which in this situation will be starting at the first letter and therefore there will be no values left. It is important to note that the * is matching any character. If we didn't have the * it wouldn't match and remove anything. We can upper case parts of the variable with numerous modifiers. The first ^ will upper case the first letter, if we do ^^ it will upper case all letters and if the value is followed by a character that letter will be upper cased through out the variable. If we use the ^ followed by a letter it will upper case the first instance of that letter. Alternatively we can do the , in the same way and it will lowercase the letters. we can swap cases using the ~ symbol, and all symbols with ~~ . If we want to do in place replacement of a variable rather than piping to sed we can do it directly in bash for example we can change our test variable with \"${test/hec/hoc}\" Similarly to sed if we want to find all occureances like in sed with the /g flag we can use // at the start. So to find and replace all c's in the test variable we can use \"${test//c/t}\" . We can also change the first value found in the line with \"${test/#check/black}\" which will change check it to black it. All of these can be done on arrays. If we wish to check if a variable is defined otherwise output a value. We can use the :- to output a different value if it is undefined. For example we have a variable set by empty= this variable is empty. We can provide a value or a response with echo \"${empty:-Not here.}\" to output that the variable is empty. The :+ does the opposite if the value is undefined use that otherwise use the value on the right side of the :+ . If the value is defined use the value on the right side. Using the :? will exit the script throwing the error written on the right side of the ? if the value is undefined. := will assign the value on the right to the variable on the left overwriting the current value. There is a lot available in bash to do fast and cool things and this resource is incredibly useful. To find out more watch the video Bash variable expansion","title":"bash variable expansions"},{"location":"zet/20210821114620/","text":"Using chroot to run my arcolinux drive I wanted to build an alci iso from arcolinux but it required me to have my arch system which is currently not working. Instead of setting up a live cd and running with that or working with a vm i thought i could run chroot on the /root directory of this other drive and am able to open up and look at files. I plan on looking to see if i can build the iso from ubuntu inside the chrooted drive.","title":"Using chroot to run my arcolinux drive"},{"location":"zet/20210821114620/#using-chroot-to-run-my-arcolinux-drive","text":"I wanted to build an alci iso from arcolinux but it required me to have my arch system which is currently not working. Instead of setting up a live cd and running with that or working with a vm i thought i could run chroot on the /root directory of this other drive and am able to open up and look at files. I plan on looking to see if i can build the iso from ubuntu inside the chrooted drive.","title":"Using chroot to run my arcolinux drive"},{"location":"zet/20210821115307/","text":"If needing network access copy resolv.conf for chroot If you are working in chroot and you want to download something. cp /etc/resolv.conf to the /etc/resolve inside your chrooted environment. #network #chroot","title":"If needing network access copy resolv.conf for chroot"},{"location":"zet/20210821115307/#if-needing-network-access-copy-resolvconf-for-chroot","text":"If you are working in chroot and you want to download something. cp /etc/resolv.conf to the /etc/resolve inside your chrooted environment. #network #chroot","title":"If needing network access copy resolv.conf for chroot"},{"location":"zet/20210826012326/","text":"Using Grep in bash without actually calling grep Watching Robs steam today he wrote an excellent grep tool in bash. We can apply this to many setups and it may be useful to make this into a filter. The script is: while IFS= read -r line; do [[ $line =~ ^RE ]] && echo \"$line\" done < <(ls -1) This helps to remove subprocess calls to grep and will be useful for so many things. #bash #grep #scripting","title":"Using Grep in bash without actually calling grep"},{"location":"zet/20210826012326/#using-grep-in-bash-without-actually-calling-grep","text":"Watching Robs steam today he wrote an excellent grep tool in bash. We can apply this to many setups and it may be useful to make this into a filter. The script is: while IFS= read -r line; do [[ $line =~ ^RE ]] && echo \"$line\" done < <(ls -1) This helps to remove subprocess calls to grep and will be useful for so many things. #bash #grep #scripting","title":"Using Grep in bash without actually calling grep"},{"location":"zet/20210827063552/","text":"!$ will give you the last argument of the last command. Looking at the linux cast. He mentions the command !$ , which he incorrectly states is the output of the last command the value is actually the last argument of the last command. Not sure whether this is useful in any situation but it is added to the zet just in case.","title":"!$ will give you the last argument of the last command."},{"location":"zet/20210827063552/#will-give-you-the-last-argument-of-the-last-command","text":"Looking at the linux cast. He mentions the command !$ , which he incorrectly states is the output of the last command the value is actually the last argument of the last command. Not sure whether this is useful in any situation but it is added to the zet just in case.","title":"!$ will give you the last argument of the last command."},{"location":"zet/20210831115623/","text":"Sourcing programs with source and . The reason we use the source command including the more portable command . (posix) is to change the current processes environment and variables. Normally when we execute a program from bash it spawns a subprocess. This process is not able to change the parent process in any way and so if we change the environment variables the parent process won't see the change. We see this when we source the bashrc file and similarly when we source /venv/bin/activate which runs a group of modifications on the command line and sets some environment variables. It is convention to never make your sourceable files executable and if you wish to use syntax highlighting place a dummy shebang to indicate bash but a fake directory structure.","title":"Sourcing programs with source and ."},{"location":"zet/20210831115623/#sourcing-programs-with-source-and","text":"The reason we use the source command including the more portable command . (posix) is to change the current processes environment and variables. Normally when we execute a program from bash it spawns a subprocess. This process is not able to change the parent process in any way and so if we change the environment variables the parent process won't see the change. We see this when we source the bashrc file and similarly when we source /venv/bin/activate which runs a group of modifications on the command line and sets some environment variables. It is convention to never make your sourceable files executable and if you wish to use syntax highlighting place a dummy shebang to indicate bash but a fake directory structure.","title":"Sourcing programs with source and ."},{"location":"zet/20210831121935/","text":"Multicall or omi-executable programs. The most famous omni executable program is busybox which is a single binary containing a large portion of the base linux programs in it. The design works by reading in the sys.argv[0] (python) or $0 (bash), into the main program. This program is symlinked to different names for example we could have the program (called myprog) symlinked to calculator and time. This program will read in the executable name, i.e. calculator and feed this into a switch case to determine the code block to run. Below is a simple omni-executable seen on Anthonywritescode, written in python #!/usr/bin/env python3 import os.path import sys def read_link(path): read = os.readlink(path) return os.path.join(os.path.dirname(path), read) def main(): exe = sys.argv[0] while ( os.path.islink(exe) and os.path.islink(read_link(exe)) ): exe = read_link(exe) if exe == 'uniq': seen = {} for line in sys.stdin: seen[line] = True for line in seen: print(line, end = '') return 0 else raise Not ImplementedError(exe) if __name__ == \"__main__\": raise SystemExit(main())","title":"Multicall or omi-executable programs."},{"location":"zet/20210831121935/#multicall-or-omi-executable-programs","text":"The most famous omni executable program is busybox which is a single binary containing a large portion of the base linux programs in it. The design works by reading in the sys.argv[0] (python) or $0 (bash), into the main program. This program is symlinked to different names for example we could have the program (called myprog) symlinked to calculator and time. This program will read in the executable name, i.e. calculator and feed this into a switch case to determine the code block to run. Below is a simple omni-executable seen on Anthonywritescode, written in python #!/usr/bin/env python3 import os.path import sys def read_link(path): read = os.readlink(path) return os.path.join(os.path.dirname(path), read) def main(): exe = sys.argv[0] while ( os.path.islink(exe) and os.path.islink(read_link(exe)) ): exe = read_link(exe) if exe == 'uniq': seen = {} for line in sys.stdin: seen[line] = True for line in seen: print(line, end = '') return 0 else raise Not ImplementedError(exe) if __name__ == \"__main__\": raise SystemExit(main())","title":"Multicall or omi-executable programs."},{"location":"zet/20210831124324/","text":"Use python -i <program.py> to drop into REPL after execution It is simple to view the variable values at time of program crashing with the -i value. This drops the user into the REPL after the program exits. It will allow you to inspect the variables at the time of the crash. Useful tool for debugging without using pdb.","title":"Use `python -i <program.py>` to drop into REPL after execution"},{"location":"zet/20210831124324/#use-python-i-programpy-to-drop-into-repl-after-execution","text":"It is simple to view the variable values at time of program crashing with the -i value. This drops the user into the REPL after the program exits. It will allow you to inspect the variables at the time of the crash. Useful tool for debugging without using pdb.","title":"Use python -i &lt;program.py&gt; to drop into REPL after execution"},{"location":"zet/20210831124932/","text":"If you ever need resources on how to exit python programs this is it Reading through David Beazleys article on exiting python with raise SystemExit(0) i found some excellent gems the article in question is SystemExit . Not only does David talk about using -i to keep an interpreter alive. (see previous post) but he also gives a nice overview of how to write a context manager to use with a with statement. This is done simply with the methods __enter__ and __exit__ .","title":"If you ever need resources on how to exit python programs this is it"},{"location":"zet/20210831124932/#if-you-ever-need-resources-on-how-to-exit-python-programs-this-is-it","text":"Reading through David Beazleys article on exiting python with raise SystemExit(0) i found some excellent gems the article in question is SystemExit . Not only does David talk about using -i to keep an interpreter alive. (see previous post) but he also gives a nice overview of how to write a context manager to use with a with statement. This is done simply with the methods __enter__ and __exit__ .","title":"If you ever need resources on how to exit python programs this is it"},{"location":"zet/20210903013429/","text":"Use \"${EDITOR:=vi}\" to create a shorthand for EDITOR=${EDITOR:-vi} This doesn't work directly in a script. We can't just write \"{EDITOR...\" we need to include a command at the start which sets the value and returns true. Commands that can do this are : , [ , test and [[ . This was tested on rwxrob.tv where we worked through what : does at start of these commands in robs scripts. See robs zet for more info - https://github.com/rwxrob/zet/tree/main/20210825172754","title":"Use `\"${EDITOR:=vi}\"` to create a shorthand for `EDITOR=${EDITOR:-vi}`"},{"location":"zet/20210903013429/#use-editorvi-to-create-a-shorthand-for-editoreditor-vi","text":"This doesn't work directly in a script. We can't just write \"{EDITOR...\" we need to include a command at the start which sets the value and returns true. Commands that can do this are : , [ , test and [[ . This was tested on rwxrob.tv where we worked through what : does at start of these commands in robs scripts. See robs zet for more info - https://github.com/rwxrob/zet/tree/main/20210825172754","title":"Use \"${EDITOR:=vi}\" to create a shorthand for EDITOR=${EDITOR:-vi}"},{"location":"zet/20210903024104/","text":"<(something) is a process substitution and creates a named pipe To further understand the command above the process substitution creates what is known as a pipe using /dev/fd/ and sends the result of the process within the parentheses to another process. Process substitution can compare the contents of two directories. Check out Process Substitution .","title":"<(something) is a process substitution and creates a named pipe"},{"location":"zet/20210903024104/#something-is-a-process-substitution-and-creates-a-named-pipe","text":"To further understand the command above the process substitution creates what is known as a pipe using /dev/fd/ and sends the result of the process within the parentheses to another process. Process substitution can compare the contents of two directories. Check out Process Substitution .","title":"&lt;(something) is a process substitution and creates a named pipe"},{"location":"zet/20210903102800/","text":"Working with virtual Environments correctly. Python has many different virtual environment packagers including venv, virtualenv, conda and pipenv. All of which do very similar things. Venv is shipped with python from 3.3 onwards so is available at all times. The disadvantage to venv is that it is python version specific, ie you are only able to make virtual environments for python3.8 in the 3.8 version. This zet outlines how to use virtualenv on your system There are numerous ways to setup a virtual environment configuration. We can download from pip and install globally or user space. If we want to remove it from any system conflicts we can use a bootstraper from bootstrap . This can be called with curl -O https://bootstrap.pypa.io/virtualenv.pyz We can then initialise the virtualenvs with: python3 virtualenv.pyz <envname> However it is important to note that the virtual environment gets created in whichever directory you are working in. It is possible to create a single directory and create all venvs in that directory. To do this we can create a bash script to run venv setup, venv list and venv activate. These commands create a virtual environment in a dedicated directory such as a repos directory. list will show all venvs available. When you run venv activate it will source the particular environment. The script can be seen in my scripts folder in my dotfiles.","title":"Working with virtual Environments correctly."},{"location":"zet/20210903102800/#working-with-virtual-environments-correctly","text":"Python has many different virtual environment packagers including venv, virtualenv, conda and pipenv. All of which do very similar things. Venv is shipped with python from 3.3 onwards so is available at all times. The disadvantage to venv is that it is python version specific, ie you are only able to make virtual environments for python3.8 in the 3.8 version. This zet outlines how to use virtualenv on your system There are numerous ways to setup a virtual environment configuration. We can download from pip and install globally or user space. If we want to remove it from any system conflicts we can use a bootstraper from bootstrap . This can be called with curl -O https://bootstrap.pypa.io/virtualenv.pyz We can then initialise the virtualenvs with: python3 virtualenv.pyz <envname> However it is important to note that the virtual environment gets created in whichever directory you are working in. It is possible to create a single directory and create all venvs in that directory. To do this we can create a bash script to run venv setup, venv list and venv activate. These commands create a virtual environment in a dedicated directory such as a repos directory. list will show all venvs available. When you run venv activate it will source the particular environment. The script can be seen in my scripts folder in my dotfiles.","title":"Working with virtual Environments correctly."},{"location":"zet/20210903122409/","text":"Using pytest and capsys for checking errors and running tests ** To begin capsys.readouterr() can capture out and error then we can use assert out == 'whatever' and asseer err == '' We can also send print statements directly to locations for example sending the string error to sys.stderr will allow you to catch the errors on the terminal. We can do this with: print(\"Error message\", file=sys.stderr)","title":"Using pytest and capsys for checking errors and running tests"},{"location":"zet/20210903122409/#using-pytest-and-capsys-for-checking-errors-and-running-tests","text":"** To begin capsys.readouterr() can capture out and error then we can use assert out == 'whatever' and asseer err == '' We can also send print statements directly to locations for example sending the string error to sys.stderr will allow you to catch the errors on the terminal. We can do this with: print(\"Error message\", file=sys.stderr)","title":"Using pytest and capsys for checking errors and running tests"},{"location":"zet/20210903125103/","text":"Python Decorator overview","title":"Python Decorator overview"},{"location":"zet/20210903125103/#python-decorator-overview","text":"","title":"Python Decorator overview"},{"location":"zet/20210905102051/","text":"Why use the @property The @property is a decorator that turns a function into an attribute of a class. The use of the property decorator can be seen below. The issue seen without the @property shows that whilst self.x and self.y change when moving however the repr of the class doesn't update. To fix this one could add an update function however this is then required to be added to all of the functions. Sample Code Class Location: def __init__(self, x=0, y=0) self.x = x self.y = y self.loc = [self.x, self.y] def move_left(self): self.x -= 1 def move_right(self): self.x += 1 def move_down(self): self.y -= 1 def move_up(self): self.y += 1 def __repr__(self): return f'{type(self).__name__}(x={self.x}, y={self.y})' To fix this issue we can add the @property to a loc method. This is seen here. @property def loc(self): return [self.x, self.y] The resulting function can then be called as an attribute with loc.loc where loc is the name of the class instance. We can create our own property class as seen: class MyProperty: def __init__(self, func): self._func = func def __get__(self, inst, owner=None): return self._func(inst) Property decorators also have functionality such as the setters and deleters. These are applied to the function you decorate with the property decorator. This is done with: @loc.setter def loc(self, loc): self.x, self.y = loc Which can be called with loc.loc = [1000, 50] for example. The deleter is seen below: @loc.deleter def loc(self): self.x = self.y = 0 In python 3.8 in the functools library we have a cached_property decorator. This function only gets the value once and stores it for later, @cached_property . #python #decorators #@property","title":"Why use the `@property`"},{"location":"zet/20210905102051/#why-use-the-property","text":"The @property is a decorator that turns a function into an attribute of a class. The use of the property decorator can be seen below. The issue seen without the @property shows that whilst self.x and self.y change when moving however the repr of the class doesn't update. To fix this one could add an update function however this is then required to be added to all of the functions. Sample Code Class Location: def __init__(self, x=0, y=0) self.x = x self.y = y self.loc = [self.x, self.y] def move_left(self): self.x -= 1 def move_right(self): self.x += 1 def move_down(self): self.y -= 1 def move_up(self): self.y += 1 def __repr__(self): return f'{type(self).__name__}(x={self.x}, y={self.y})' To fix this issue we can add the @property to a loc method. This is seen here. @property def loc(self): return [self.x, self.y] The resulting function can then be called as an attribute with loc.loc where loc is the name of the class instance. We can create our own property class as seen: class MyProperty: def __init__(self, func): self._func = func def __get__(self, inst, owner=None): return self._func(inst) Property decorators also have functionality such as the setters and deleters. These are applied to the function you decorate with the property decorator. This is done with: @loc.setter def loc(self, loc): self.x, self.y = loc Which can be called with loc.loc = [1000, 50] for example. The deleter is seen below: @loc.deleter def loc(self): self.x = self.y = 0 In python 3.8 in the functools library we have a cached_property decorator. This function only gets the value once and stores it for later, @cached_property . #python #decorators #@property","title":"Why use the @property"},{"location":"zet/20210905120522/","text":"Use ctrl \\ to close python script if blocking Ctrl C If you ever find your self in an infinite loop being unable to close the program running using ctrl \\`` will send sigQuit to the program causing it to core dump. This can occur when you have a blank except: statement in python. To over come this error using the except Exception:` instead you wont cause this issue in your python program. #python #sigquit","title":"Use ctrl \\ to close python script if blocking Ctrl C"},{"location":"zet/20210905120522/#use-ctrl-to-close-python-script-if-blocking-ctrl-c","text":"If you ever find your self in an infinite loop being unable to close the program running using ctrl \\`` will send sigQuit to the program causing it to core dump. This can occur when you have a blank except: statement in python. To over come this error using the except Exception:` instead you wont cause this issue in your python program. #python #sigquit","title":"Use ctrl \\ to close python script if blocking Ctrl C"},{"location":"zet/20210905123017/","text":"Use @contextlib.contextmanager to make your own context manager If you have a function that needs to be gracefully shutdown or quit and it doesn't work inside a with statement. We can use the @contextlib.contextmanager from contextlib. This is a nifty little tool. #python #contextlib","title":"Use @contextlib.contextmanager to make your own context manager"},{"location":"zet/20210905123017/#use-contextlibcontextmanager-to-make-your-own-context-manager","text":"If you have a function that needs to be gracefully shutdown or quit and it doesn't work inside a with statement. We can use the @contextlib.contextmanager from contextlib. This is a nifty little tool. #python #contextlib","title":"Use @contextlib.contextmanager to make your own context manager"},{"location":"zet/20210905124739/","text":"Bling strings with $ If you place a $ outside the string in echo statements it will expand the escape characters so if you need to show a tab in the echo response you can use: echo $'foo\\tbar' This will expand to foo bar without the $ symbol you would need to use -e . #bash #escapechars","title":"Bling strings with $"},{"location":"zet/20210905124739/#bling-strings-with","text":"If you place a $ outside the string in echo statements it will expand the escape characters so if you need to show a tab in the echo response you can use: echo $'foo\\tbar' This will expand to foo bar without the $ symbol you would need to use -e . #bash #escapechars","title":"Bling strings with $"},{"location":"zet/20210906054839/","text":"Ctrl P and Ctrl Q to detach a docker image","title":"Ctrl P and Ctrl Q to detach a docker image"},{"location":"zet/20210906054839/#ctrl-p-and-ctrl-q-to-detach-a-docker-image","text":"","title":"Ctrl P and Ctrl Q to detach a docker image"},{"location":"zet/20210906124243/","text":"The difference between Process Substitution and Command Substitution The process substitution <(echo 'Hi') and command substitution $(echo 'Hi') are used in many shell scripts and i could never understand what the use of a process substitution does. A terminalforlife video Terminalforlife has given me more understanding of the differences between these and why you would use one over the other. The command substitution is seen often in many programs often used to get the output of a command and send it stdout . Using process substitution correctly can be seen below. cat <(echo hi) #equivalent #cat FILE #FILE = 'hi' Using command substitution: cat <<< \"$(echo hi)\" However it is often seen where people will send the result of the process substitution to stdin with the < such as cat < <(echo hi) . If you want to send the pretend file to stdin this is the incorrect way to do it. Instead we can use the command substitution with a here string <<< with a command such as cat <<< \"$(echo hi)\" which translates to sending echo hi | cat where cat reads stdin . However if a program is unable to use stdin into the program but is able to read a file we can use process substitution to make the process look like a file. This is because under the hood the process substitution creates a FIFO or file descriptor. The file descriptor or named pipe is a file found in /dev/fd<n> where n is a number. One thing to note that was mentioned in the comments is that the process substitution is useful when we are reading line by line of the file and will process each line as it arrives whilst the here string with command substitution waits on reading the whole file. #bash #process-substitution #command-substitution","title":"The difference between Process Substitution and Command Substitution"},{"location":"zet/20210906124243/#the-difference-between-process-substitution-and-command-substitution","text":"The process substitution <(echo 'Hi') and command substitution $(echo 'Hi') are used in many shell scripts and i could never understand what the use of a process substitution does. A terminalforlife video Terminalforlife has given me more understanding of the differences between these and why you would use one over the other. The command substitution is seen often in many programs often used to get the output of a command and send it stdout . Using process substitution correctly can be seen below. cat <(echo hi) #equivalent #cat FILE #FILE = 'hi' Using command substitution: cat <<< \"$(echo hi)\" However it is often seen where people will send the result of the process substitution to stdin with the < such as cat < <(echo hi) . If you want to send the pretend file to stdin this is the incorrect way to do it. Instead we can use the command substitution with a here string <<< with a command such as cat <<< \"$(echo hi)\" which translates to sending echo hi | cat where cat reads stdin . However if a program is unable to use stdin into the program but is able to read a file we can use process substitution to make the process look like a file. This is because under the hood the process substitution creates a FIFO or file descriptor. The file descriptor or named pipe is a file found in /dev/fd<n> where n is a number. One thing to note that was mentioned in the comments is that the process substitution is useful when we are reading line by line of the file and will process each line as it arrives whilst the here string with command substitution waits on reading the whole file. #bash #process-substitution #command-substitution","title":"The difference between Process Substitution and Command Substitution"},{"location":"zet/20210906231427/","text":"jtbl to print JSON data as a table on the terminal kellyjonbrazil has an excellent blog discussing minimalist UNIX programs and always outputting data into a useful format. One of his big requests is to make every tool have a JSON output by using a -j flag or similar. jtbl developed by him takes any JSON data and plots it as a table on the terminal. This is useful if you need to view some JSON file in a more readable format. The program reads from stdin and is written in python. I have taken the stdin reader and added it to my snippets library. The jtbl tool can be installed by pip and the repository is seen here: jtbl #json #jtbl #unix_philosophy","title":"jtbl to print JSON data as a table on the terminal"},{"location":"zet/20210906231427/#jtbl-to-print-json-data-as-a-table-on-the-terminal","text":"kellyjonbrazil has an excellent blog discussing minimalist UNIX programs and always outputting data into a useful format. One of his big requests is to make every tool have a JSON output by using a -j flag or similar. jtbl developed by him takes any JSON data and plots it as a table on the terminal. This is useful if you need to view some JSON file in a more readable format. The program reads from stdin and is written in python. I have taken the stdin reader and added it to my snippets library. The jtbl tool can be installed by pip and the repository is seen here: jtbl #json #jtbl #unix_philosophy","title":"jtbl to print JSON data as a table on the terminal"},{"location":"zet/20210906235035/","text":"Quick start with docker Docker is an incredible tool useful for testing and experimenting with new programs without breaking your main machine. Docker works fundamentally by making use of cgroups , namespaces and chroot . Along with additional functionality through networking and port forwarding. One can start their journey in docker with a couple of simple commands. To start a docker container we can either pull down and image and then start or simply run. Below are the three main ways to get up and running with a container. The first pulls down the docker image using docker pull where image is the path to the docker image from docker hub or similar sites. Then we can run the image with docker run . docker pull <image> docker run -it <--rm> <--name test> <image> The switches to note in this command are: -i #attach stdin to container (if this is not added we can't interact with the terminal) -t #allocates a pseudo tty (Same interface as terminal on linux) --rm #Removes the container after exiting (Optional and will remove any files or configurations made to the container. --name #Optional name to use when attaching and reattaching to the container The next command is similar to above and can be run alternatively to the above command. The docker run command by itself will look for the image locally otherwise will look on docker hub. The -it is outlined above. docker run -it <image> This next command can be used if we have pulled down an image and created a container with a --name switch. This command allows us to attach and reattach to the container. docker start -ai test The switches here are similar to the run command: -a #attach to STDIN, STDOUT and/or STDERR (can attach to one or all of these) -i #keep STDIN open even if not attached. Extensions There are many additional switches one can use when running a docker container. Below outlines a few of these and the syntax for them. -d #detached mode -p 80:80 #port forward port 80 from the container to port 80 on the host. If we do not add values to the port it will publish all ports. * format - hostport:containerport (can be a range of values) --privileged #enables access to all devices on the host. --device=/dev/** #Where ** can be any device, enables a single device access to the container. --entrypoint #sets an alternative entrypoint to the container. (Specify what executable to run when container starts.) --tmpfs #Create a tmpfs mount with options * --tmpfs /run:rw,noexec,nosuid,size=65536k #mounts the tmpfs onto run with additional options. -v #Bind mount a volume * host-src:container-dest --mount #similar to v. Additional advantages are simplicity in reading * --mount type=bind,source=\"$(pwd)\"/target,target=/app,readonly --init #Adds an init system to the container (default:docker-init which is https://github.com/krallin/tini) ## Useful as it reaps zombie processes --network=\"host\" #Gives the container full access to the local system services such as dbus **use with caution** We can attach to a running container with docker attach <name> We can also stop containers that are detached with docker stop <name> This is just the basics of running docker containers. There will be another post about how to create your own containers and building multistage builds. #docker","title":"Quick start with docker"},{"location":"zet/20210906235035/#quick-start-with-docker","text":"Docker is an incredible tool useful for testing and experimenting with new programs without breaking your main machine. Docker works fundamentally by making use of cgroups , namespaces and chroot . Along with additional functionality through networking and port forwarding. One can start their journey in docker with a couple of simple commands. To start a docker container we can either pull down and image and then start or simply run. Below are the three main ways to get up and running with a container. The first pulls down the docker image using docker pull where image is the path to the docker image from docker hub or similar sites. Then we can run the image with docker run . docker pull <image> docker run -it <--rm> <--name test> <image> The switches to note in this command are: -i #attach stdin to container (if this is not added we can't interact with the terminal) -t #allocates a pseudo tty (Same interface as terminal on linux) --rm #Removes the container after exiting (Optional and will remove any files or configurations made to the container. --name #Optional name to use when attaching and reattaching to the container The next command is similar to above and can be run alternatively to the above command. The docker run command by itself will look for the image locally otherwise will look on docker hub. The -it is outlined above. docker run -it <image> This next command can be used if we have pulled down an image and created a container with a --name switch. This command allows us to attach and reattach to the container. docker start -ai test The switches here are similar to the run command: -a #attach to STDIN, STDOUT and/or STDERR (can attach to one or all of these) -i #keep STDIN open even if not attached. Extensions There are many additional switches one can use when running a docker container. Below outlines a few of these and the syntax for them. -d #detached mode -p 80:80 #port forward port 80 from the container to port 80 on the host. If we do not add values to the port it will publish all ports. * format - hostport:containerport (can be a range of values) --privileged #enables access to all devices on the host. --device=/dev/** #Where ** can be any device, enables a single device access to the container. --entrypoint #sets an alternative entrypoint to the container. (Specify what executable to run when container starts.) --tmpfs #Create a tmpfs mount with options * --tmpfs /run:rw,noexec,nosuid,size=65536k #mounts the tmpfs onto run with additional options. -v #Bind mount a volume * host-src:container-dest --mount #similar to v. Additional advantages are simplicity in reading * --mount type=bind,source=\"$(pwd)\"/target,target=/app,readonly --init #Adds an init system to the container (default:docker-init which is https://github.com/krallin/tini) ## Useful as it reaps zombie processes --network=\"host\" #Gives the container full access to the local system services such as dbus **use with caution** We can attach to a running container with docker attach <name> We can also stop containers that are detached with docker stop <name> This is just the basics of running docker containers. There will be another post about how to create your own containers and building multistage builds. #docker","title":"Quick start with docker"},{"location":"zet/20210907105047/","text":"Go - 100daysofCode -- Day 1 I have decided that i am going to try the #100daysofX challenge and learn Go over the next 100 days. These posts will sit in the zet folder but Will follow the same guide as the title of this zet. This way it is searchable and easily used as a reference. The notes seen here will come from many sources and some days may have very little if i am looking at specific code or tutorials. The following notes come from golang Getting Started When you import packages into your Go program, these are managed in a go.mod file. This tracks the modules that provide the packages. To set up a module we use go mod init github.com/hello The fundamental hello world code is completed by declaring that your package is main with package main , following this as is similar in most languages are our import statements. Go doesn't provide Println, or print functionality in the main go binaries but it is provided with the package fmt. We can import a function and then use it. If we don't use an imported function the program throws an error. The basic hello world is seen below. package main import \"fmt\" func main() { fmt.Println(\"Hello, World!\") } We can find go help on the command line with go help Extending on the hello world program we can add an external package such as rsc.io/quote. If we add this into the program and modify the print statement, when we run the script with go run . we get an error saying no module available, this can be remedied with either go get rsc.io/quote or the more prefered method of go mod tidy . This will find the package and create a go.sum file for use in authenticating the module. Go Modules In this example we build a module to communicate with a main package. The code is seen here: package greetings import \"fmt\" // Hello returns a greeting for the named person. func Hello(name string) string { // Return a greeting that embeds the name in a message. message := fmt.Sprintf(\"Hi, %v. Welcome!\", name) return message } Of note in this code is the package name is now greetings. We also have a new design for the function, the name is now Hello followed by the input parameter name and type and outside the brackets is the return type. Inside the code we use := which is a go shortcut to declare and initialize a variable in one go. Alternatively we can declare then initialise with var message string message = fmt.Sprintf(\"Hi, %v. Welcome!\", name) We also make use of the Sprintf function, which has a format string with %v is the format type being a verb we then substitute the second parameter in it's place. We can then call the package from another module. To do this we add to the import the module name. In this example case it is example.com/greetings . However as Go tries to search the web for the package we need to modify the mod file to explicitely indicate the location of the package. This is done with: go mod edit --replace example.com/greetings=../greetings This will modify the mod file to indicate the parallel directory named greetings. This appends to the end of the mod file a pointer to indicate the replacement. replace example.com/greetings => ../greetings . When using published modules we can change replace to require and define a particular version of the package. Adding Return and error handling We can modify for the greetings package to add in error handling as seen below: package greetings import ( \"errors\" \"fmt\" ) // Hello returns a greeting for the named person. func Hello(name string) (string, error) { // If no name was given, return an error with a message. if name == \"\" { return \"\", errors.New(\"empty name\") } // If a name was received, return a value that embeds the name // in a greeting message. message := fmt.Sprintf(\"Hi, %v. Welcome!\", name) return message, nil } The code above shows the error being handled by an if statement. It also returns early with an error provided by the errors.New command. We can then catch these errors and log to file with the log package. Modifying the main hello package we can check for errors from the greetings package and if the program has an error log it and quit, the code is below. package main import ( \"fmt\" \"log\" \"example.com/greetings\" ) func main() { // Set properties of the predefined Logger, including // the log entry prefix and a flag to disable printing // the time, source file, and line number. log.SetPrefix(\"greetings: \") log.SetFlags(0) // Request a greeting message. message, err := greetings.Hello(\"\") // If an error was returned, print it to the console and // exit the program. if err != nil { log.Fatal(err) } // If no error was returned, print the returned message // to the console. fmt.Println(message) } The log.SetPrefix begins the log file and the SetFlags command would configure other attributes. We catch the error in the assignment from greetings and run the exit logic. Tomorrow i will expand on this and finish off the second tutorial on the golang.org website #100daysofGo #Go","title":"Go - 100daysofCode -- Day 1"},{"location":"zet/20210907105047/#go-100daysofcode-day-1","text":"I have decided that i am going to try the #100daysofX challenge and learn Go over the next 100 days. These posts will sit in the zet folder but Will follow the same guide as the title of this zet. This way it is searchable and easily used as a reference. The notes seen here will come from many sources and some days may have very little if i am looking at specific code or tutorials. The following notes come from golang Getting Started When you import packages into your Go program, these are managed in a go.mod file. This tracks the modules that provide the packages. To set up a module we use go mod init github.com/hello The fundamental hello world code is completed by declaring that your package is main with package main , following this as is similar in most languages are our import statements. Go doesn't provide Println, or print functionality in the main go binaries but it is provided with the package fmt. We can import a function and then use it. If we don't use an imported function the program throws an error. The basic hello world is seen below. package main import \"fmt\" func main() { fmt.Println(\"Hello, World!\") } We can find go help on the command line with go help Extending on the hello world program we can add an external package such as rsc.io/quote. If we add this into the program and modify the print statement, when we run the script with go run . we get an error saying no module available, this can be remedied with either go get rsc.io/quote or the more prefered method of go mod tidy . This will find the package and create a go.sum file for use in authenticating the module. Go Modules In this example we build a module to communicate with a main package. The code is seen here: package greetings import \"fmt\" // Hello returns a greeting for the named person. func Hello(name string) string { // Return a greeting that embeds the name in a message. message := fmt.Sprintf(\"Hi, %v. Welcome!\", name) return message } Of note in this code is the package name is now greetings. We also have a new design for the function, the name is now Hello followed by the input parameter name and type and outside the brackets is the return type. Inside the code we use := which is a go shortcut to declare and initialize a variable in one go. Alternatively we can declare then initialise with var message string message = fmt.Sprintf(\"Hi, %v. Welcome!\", name) We also make use of the Sprintf function, which has a format string with %v is the format type being a verb we then substitute the second parameter in it's place. We can then call the package from another module. To do this we add to the import the module name. In this example case it is example.com/greetings . However as Go tries to search the web for the package we need to modify the mod file to explicitely indicate the location of the package. This is done with: go mod edit --replace example.com/greetings=../greetings This will modify the mod file to indicate the parallel directory named greetings. This appends to the end of the mod file a pointer to indicate the replacement. replace example.com/greetings => ../greetings . When using published modules we can change replace to require and define a particular version of the package. Adding Return and error handling We can modify for the greetings package to add in error handling as seen below: package greetings import ( \"errors\" \"fmt\" ) // Hello returns a greeting for the named person. func Hello(name string) (string, error) { // If no name was given, return an error with a message. if name == \"\" { return \"\", errors.New(\"empty name\") } // If a name was received, return a value that embeds the name // in a greeting message. message := fmt.Sprintf(\"Hi, %v. Welcome!\", name) return message, nil } The code above shows the error being handled by an if statement. It also returns early with an error provided by the errors.New command. We can then catch these errors and log to file with the log package. Modifying the main hello package we can check for errors from the greetings package and if the program has an error log it and quit, the code is below. package main import ( \"fmt\" \"log\" \"example.com/greetings\" ) func main() { // Set properties of the predefined Logger, including // the log entry prefix and a flag to disable printing // the time, source file, and line number. log.SetPrefix(\"greetings: \") log.SetFlags(0) // Request a greeting message. message, err := greetings.Hello(\"\") // If an error was returned, print it to the console and // exit the program. if err != nil { log.Fatal(err) } // If no error was returned, print the returned message // to the console. fmt.Println(message) } The log.SetPrefix begins the log file and the SetFlags command would configure other attributes. We catch the error in the assignment from greetings and run the exit logic. Tomorrow i will expand on this and finish off the second tutorial on the golang.org website #100daysofGo #Go","title":"Go - 100daysofCode -- Day 1"},{"location":"zet/20210908114824/","text":"Go - 100daysofCode -- Day 2 Continuing https://golang.org/doc/tutorial currently at: Return a random greeting The first thing to look at in this new section is the Go slice. Much like arrays in python. The code will create a slice of strings and return a message randomly. package greetings import ( \"errors\" \"fmt\" \"math/rand\" \"time\" ) // Hello returns a greeting for the named person. func Hello(name string) (string, error) { // If no name was given, return an error with a message. if name == \"\" { return name, errors.New(\"empty name\") } // Create a message using a random format. message := fmt.Sprintf(randomFormat(), name) return message, nil } // init sets initial values for variables used in the function. func init() { rand.Seed(time.Now().UnixNano()) } // randomFormat returns one of a set of greeting messages. The returned // message is selected at random. func randomFormat() string { // A slice of message formats. formats := []string{ \"Hi, %v. Welcome!\", \"Great to see you, %v!\", \"Hail, %v! Well met!\", } // Return a randomly selected message format by specifying // a random index for the slice of formats. return formats[rand.Intn(len(formats))] } We also find some other interesting things in this script we have a time.Now().UnixNano() which looks like it gets the current time in nanoseconds. We have a rand.Seed and rand.Intn giving access to random generators. The tutorial outlines a couple of important notes. The first being that if you start a function with a lowercase letter the code is only accessible inside the package. Slices omit the size of the array which is handy. init functions are executed automatically on startup, after global variables are initialised. effective go Return greetings for multiple people Adding support to great multiple people. In other words handle a multi value input. It is important to not break functionality of your code by changing the base functions and so to update with the new functionality we create a new function. // Hellos returns a map that associates each of the named people // with a greeting message. func Hellos(names []string) (map[string]string, error) { // A map to associate names with messages. messages := make(map[string]string) // Loop through the received slice of names, calling // the Hello function to get a message for each name. for _, name := range names { message, err := Hello(name) if err != nil { return nil, err } // In the map, associate the retrieved message with // the name. messages[name] = message } return messages, nil } This function introduces a for loop with error checking. The range names must be similar to pythons range functionality here. Also of note is the input is a string slice. with a return of a map of the string string and the error. We create a messages map to associate each of the received names (as a key) with a generated message (as a value). In Go, you initialize a map with the following syntax: make(map[key-type]value-type). See Go maps in action . Finally we edit the hello code to get a slice of names and feed it to the greetings.Hellos function. Tomorrow i will finish off the rest of this tutorial and read through the links i have provided today.","title":"Go - 100daysofCode -- Day 2"},{"location":"zet/20210908114824/#go-100daysofcode-day-2","text":"Continuing https://golang.org/doc/tutorial currently at: Return a random greeting The first thing to look at in this new section is the Go slice. Much like arrays in python. The code will create a slice of strings and return a message randomly. package greetings import ( \"errors\" \"fmt\" \"math/rand\" \"time\" ) // Hello returns a greeting for the named person. func Hello(name string) (string, error) { // If no name was given, return an error with a message. if name == \"\" { return name, errors.New(\"empty name\") } // Create a message using a random format. message := fmt.Sprintf(randomFormat(), name) return message, nil } // init sets initial values for variables used in the function. func init() { rand.Seed(time.Now().UnixNano()) } // randomFormat returns one of a set of greeting messages. The returned // message is selected at random. func randomFormat() string { // A slice of message formats. formats := []string{ \"Hi, %v. Welcome!\", \"Great to see you, %v!\", \"Hail, %v! Well met!\", } // Return a randomly selected message format by specifying // a random index for the slice of formats. return formats[rand.Intn(len(formats))] } We also find some other interesting things in this script we have a time.Now().UnixNano() which looks like it gets the current time in nanoseconds. We have a rand.Seed and rand.Intn giving access to random generators. The tutorial outlines a couple of important notes. The first being that if you start a function with a lowercase letter the code is only accessible inside the package. Slices omit the size of the array which is handy. init functions are executed automatically on startup, after global variables are initialised. effective go Return greetings for multiple people Adding support to great multiple people. In other words handle a multi value input. It is important to not break functionality of your code by changing the base functions and so to update with the new functionality we create a new function. // Hellos returns a map that associates each of the named people // with a greeting message. func Hellos(names []string) (map[string]string, error) { // A map to associate names with messages. messages := make(map[string]string) // Loop through the received slice of names, calling // the Hello function to get a message for each name. for _, name := range names { message, err := Hello(name) if err != nil { return nil, err } // In the map, associate the retrieved message with // the name. messages[name] = message } return messages, nil } This function introduces a for loop with error checking. The range names must be similar to pythons range functionality here. Also of note is the input is a string slice. with a return of a map of the string string and the error. We create a messages map to associate each of the received names (as a key) with a generated message (as a value). In Go, you initialize a map with the following syntax: make(map[key-type]value-type). See Go maps in action . Finally we edit the hello code to get a slice of names and feed it to the greetings.Hellos function. Tomorrow i will finish off the rest of this tutorial and read through the links i have provided today.","title":"Go - 100daysofCode -- Day 2"},{"location":"zet/20210909111906/","text":"Go - 100daysofCode -- Day 3 Finishing off the tutorial from yesterday Adding Tests Go has built in support for unit testing to make it simple to test as you go. To create a test code block we create a file with _test to tell the Go test command to execute the test functions. We create a greetings_test.go inside the greetings directory, the code is seen here. package greetings import ( \"testing\" \"regexp\" ) // TestHelloName calls greetings.Hello with a name, checking // for a valid return value. func TestHelloName(t *testing.T) { name := \"Gladys\" want := regexp.MustCompile(`\\b`+name+`\\b`) msg, err := Hello(\"Gladys\") if !want.MatchString(msg) || err != nil { t.Fatalf(`Hello(\"Gladys\") = %q, %v, want match for %#q, nil`, msg, err, want) } } // TestHelloEmpty calls greetings.Hello with an empty string, // checking for an error. func TestHelloEmpty(t *testing.T) { msg, err := Hello(\"\") if msg != \"\" || err == nil { t.Fatalf(`Hello(\"\") = %q, %v, want \"\", error`, msg, err) } } The code uses the Test name at the start to show go test which functions to run which is really neat. Test functions always take a pointer to the testing.T type. We can add verbosity to the test with -v Compile and install the application We have two commands to use once the program is completed, we have go build and go install . Running the go build command will convert the program into an executable and be able to be run with ./ . The alternative method go install will create an installed package on your system in the directory that is set with GOBIN , to set GOBIN we can use go env -w GOBIN=/path/to/your/bin Running go install will create a program that is able to be run by calling the name.","title":"Go - 100daysofCode -- Day 3"},{"location":"zet/20210909111906/#go-100daysofcode-day-3","text":"Finishing off the tutorial from yesterday Adding Tests Go has built in support for unit testing to make it simple to test as you go. To create a test code block we create a file with _test to tell the Go test command to execute the test functions. We create a greetings_test.go inside the greetings directory, the code is seen here. package greetings import ( \"testing\" \"regexp\" ) // TestHelloName calls greetings.Hello with a name, checking // for a valid return value. func TestHelloName(t *testing.T) { name := \"Gladys\" want := regexp.MustCompile(`\\b`+name+`\\b`) msg, err := Hello(\"Gladys\") if !want.MatchString(msg) || err != nil { t.Fatalf(`Hello(\"Gladys\") = %q, %v, want match for %#q, nil`, msg, err, want) } } // TestHelloEmpty calls greetings.Hello with an empty string, // checking for an error. func TestHelloEmpty(t *testing.T) { msg, err := Hello(\"\") if msg != \"\" || err == nil { t.Fatalf(`Hello(\"\") = %q, %v, want \"\", error`, msg, err) } } The code uses the Test name at the start to show go test which functions to run which is really neat. Test functions always take a pointer to the testing.T type. We can add verbosity to the test with -v Compile and install the application We have two commands to use once the program is completed, we have go build and go install . Running the go build command will convert the program into an executable and be able to be run with ./ . The alternative method go install will create an installed package on your system in the directory that is set with GOBIN , to set GOBIN we can use go env -w GOBIN=/path/to/your/bin Running go install will create a program that is able to be run by calling the name.","title":"Go - 100daysofCode -- Day 3"},{"location":"zet/20210910140643/","text":"Getting started with minikube Minikube is a container management and orchestration tool. Useful for learning kubernetes and containers. I thought it would be worthwhile to install minikube on my computer. Installation is simple with curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube Getting it up and running. Once running we can start minikube with minikube start Many things work better if we have kubectl installed. To do this we curl the latest version and install curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl To interact with the cluster we can run kubectl get po -A #or minikube dashboard To deploy a container we use kubectl kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=NodePort --port=8080 The expose command will expose the port to port 8080 in this case. The image can come from any location. To access the above container we can use kubectl port-forward service/hello-minikube 7080:8080 There are many more things we can do with minikube. Below are a couple from the quickstart minikube pause minikube unpause minikube delete I plan on trying many things with this tool including: Running pihole, duckdns, vpn etc. Installing on rpi cluster","title":"Getting started with minikube"},{"location":"zet/20210910140643/#getting-started-with-minikube","text":"Minikube is a container management and orchestration tool. Useful for learning kubernetes and containers. I thought it would be worthwhile to install minikube on my computer. Installation is simple with curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube Getting it up and running. Once running we can start minikube with minikube start Many things work better if we have kubectl installed. To do this we curl the latest version and install curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl To interact with the cluster we can run kubectl get po -A #or minikube dashboard To deploy a container we use kubectl kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=NodePort --port=8080 The expose command will expose the port to port 8080 in this case. The image can come from any location. To access the above container we can use kubectl port-forward service/hello-minikube 7080:8080 There are many more things we can do with minikube. Below are a couple from the quickstart minikube pause minikube unpause minikube delete I plan on trying many things with this tool including: Running pihole, duckdns, vpn etc. Installing on rpi cluster","title":"Getting started with minikube"},{"location":"zet/20210911014132/","text":"Command substitution There are two methods fo commands substitution the old way using backticks ` or the new method using $() . There are times when you would like to use one over the other. For example using the $() is beneficial when we have multiple nested commands. The use of the backticks is great for when we want to quickly produce a result for instance in \\ which FILE``. An interesting bit of information seen in the video from terminalforlife is: Data=`< \"$HOME\"/.bashrc` This command will read the contents of the bashrc into a variable.","title":"Command substitution"},{"location":"zet/20210911014132/#command-substitution","text":"There are two methods fo commands substitution the old way using backticks ` or the new method using $() . There are times when you would like to use one over the other. For example using the $() is beneficial when we have multiple nested commands. The use of the backticks is great for when we want to quickly produce a result for instance in \\ which FILE``. An interesting bit of information seen in the video from terminalforlife is: Data=`< \"$HOME\"/.bashrc` This command will read the contents of the bashrc into a variable.","title":"Command substitution"},{"location":"zet/20210911033459/","text":"printf and its abilities in bash printf has some excellent features to expand on your output capabilities. The printf function has the ability to print out the date with the command printf '%(%F)T' which will output the current date. We can also use the -v flag to output to a variable. Where the first value is the variable name. Whilst the second value is the input into the variable. This is similar to sprintf . printf '%s\\n' \"$Data\" places the second value in place of the %s similar to many other languages. s can be replaced by other values such as d for number. We can also use %'d to output the decimal value with a comma.","title":"`printf` and its abilities in bash"},{"location":"zet/20210911033459/#printf-and-its-abilities-in-bash","text":"printf has some excellent features to expand on your output capabilities. The printf function has the ability to print out the date with the command printf '%(%F)T' which will output the current date. We can also use the -v flag to output to a variable. Where the first value is the variable name. Whilst the second value is the input into the variable. This is similar to sprintf . printf '%s\\n' \"$Data\" places the second value in place of the %s similar to many other languages. s can be replaced by other values such as d for number. We can also use %'d to output the decimal value with a comma.","title":"printf and its abilities in bash"},{"location":"zet/20210911065503/","text":"The absolute power of chroot Working on the setting up the raspberry pi with my son today. The computer froze mid update and no longer would boot. Rather than reflashing and starting again i grabbed the sd card and plugged it into my main computer. From there i mounted the rpi into /mnt/rpi bind mounted the dev and mounted sys and proc into the rpi os structure. The code to do this is: mount --bind /dev dev mount -t proc proc proc mount -t sysfs sysfs sys Once done i used chroot to change the root to the rpi with sudo chroot . bash , from there i was able to fix up the system by updating apt and fixing the errors that occurred. I then updated all the system as needed and unmounted the system. I am now a total convert to using chroot to fix any issues i have with installations.","title":"The absolute power of chroot"},{"location":"zet/20210911065503/#the-absolute-power-of-chroot","text":"Working on the setting up the raspberry pi with my son today. The computer froze mid update and no longer would boot. Rather than reflashing and starting again i grabbed the sd card and plugged it into my main computer. From there i mounted the rpi into /mnt/rpi bind mounted the dev and mounted sys and proc into the rpi os structure. The code to do this is: mount --bind /dev dev mount -t proc proc proc mount -t sysfs sysfs sys Once done i used chroot to change the root to the rpi with sudo chroot . bash , from there i was able to fix up the system by updating apt and fixing the errors that occurred. I then updated all the system as needed and unmounted the system. I am now a total convert to using chroot to fix any issues i have with installations.","title":"The absolute power of chroot"},{"location":"zet/20210916040516/","text":"test new zet This post is testing the latest zet program. Testing to see if it works without modifications. #zet","title":"test new zet"},{"location":"zet/20210916040516/#test-new-zet","text":"This post is testing the latest zet program. Testing to see if it works without modifications. #zet","title":"test new zet"},{"location":"zet/20210927115721/","text":"subclasses in python for creating overloaded classes Watching mcoding new vs init he showed a useful setup for creating a single class overlaying multiple underlying subclasses to process varying input types. The main function called. EncryptedFile().read() which contained a plain text, rot13 and otp files to read in. For each of these file types a class is created with a read method. The classes each run varying decryption functionality. The main idea is having the class contain the EncryptedFile parent class. The main EncryptedFile class contains a _registry = {} which maps the prefixs to classes. The registry is populated by using an __init_subclass__ hook. def __init_subclass__(cls, prefix, **kwargs): super().__init_subclass__(**kwargs) cls._registry[prefix] = cls The prefix and subclass are stored in the reqistry. Then when the __new__ method is called at class creation the code calls the below __new__ method. def __new__(cls, path, key=None): prefix, sep, suffix = path.partition(:///') ##specific to the setup of the video. if sep: file = suffix else: file = prefix prefix = \"file\" subclass = cls._registry[prefix] obj = object.__new__(subclass) obj.file = file obj.key = key return obj The prefix is pulled out of the registry and stored in the subclass. Then the subclass is initialised with the __new__ command. This design could be incredibly useful when loading in data from different datasets for deep learning and other projects.","title":"subclasses in python for creating overloaded classes"},{"location":"zet/20210927115721/#subclasses-in-python-for-creating-overloaded-classes","text":"Watching mcoding new vs init he showed a useful setup for creating a single class overlaying multiple underlying subclasses to process varying input types. The main function called. EncryptedFile().read() which contained a plain text, rot13 and otp files to read in. For each of these file types a class is created with a read method. The classes each run varying decryption functionality. The main idea is having the class contain the EncryptedFile parent class. The main EncryptedFile class contains a _registry = {} which maps the prefixs to classes. The registry is populated by using an __init_subclass__ hook. def __init_subclass__(cls, prefix, **kwargs): super().__init_subclass__(**kwargs) cls._registry[prefix] = cls The prefix and subclass are stored in the reqistry. Then when the __new__ method is called at class creation the code calls the below __new__ method. def __new__(cls, path, key=None): prefix, sep, suffix = path.partition(:///') ##specific to the setup of the video. if sep: file = suffix else: file = prefix prefix = \"file\" subclass = cls._registry[prefix] obj = object.__new__(subclass) obj.file = file obj.key = key return obj The prefix is pulled out of the registry and stored in the subclass. Then the subclass is initialised with the __new__ command. This design could be incredibly useful when loading in data from different datasets for deep learning and other projects.","title":"subclasses in python for creating overloaded classes"},{"location":"zet/20210927122736/","text":"Using pytest to improve testing Watching mcoding and he showed a really good setup for automated testing using pytest, tox and github actions. Automated testing One of the focuses was really on how to use pytest efficiently. The issue with standard testing is that if one test fails none of the others run. Using the parameterize decorator we can run the multiple tests with a single function @pytest.mark.parameterize(\"test_input,expected\", [(.., ..), (.., ..)]) def test_multi(test_input, expected): assert function('test_expected, input) is expected We can also skip functionality testing with @pytest.mark.skip if we haven't implemented functions yet We can create what are known as fixtures which are boiler plate code for setup and teardown of functionality before testing. The method is set in a conftest.py file and use the @pytest.fixture(scope=\"session\") and wrap the function. This setup is then able to be used across all tests. Another thing to look into for testing is the inbuilt function called monkeytest We can add test coverage with pytest as well.","title":"Using pytest to improve testing"},{"location":"zet/20210927122736/#using-pytest-to-improve-testing","text":"Watching mcoding and he showed a really good setup for automated testing using pytest, tox and github actions. Automated testing One of the focuses was really on how to use pytest efficiently. The issue with standard testing is that if one test fails none of the others run. Using the parameterize decorator we can run the multiple tests with a single function @pytest.mark.parameterize(\"test_input,expected\", [(.., ..), (.., ..)]) def test_multi(test_input, expected): assert function('test_expected, input) is expected We can also skip functionality testing with @pytest.mark.skip if we haven't implemented functions yet We can create what are known as fixtures which are boiler plate code for setup and teardown of functionality before testing. The method is set in a conftest.py file and use the @pytest.fixture(scope=\"session\") and wrap the function. This setup is then able to be used across all tests. Another thing to look into for testing is the inbuilt function called monkeytest We can add test coverage with pytest as well.","title":"Using pytest to improve testing"},{"location":"zet/20210930022826/","text":"Separate single run bash profile from run each new shell with .profile The .profile and .bash_profile files were originally intended to be run on startup to process single setup environment variables that in the past would have taken a long time to run. This is not much of an issue with current computers but does in fact present a good workflow habit. Separating single run scripts such as starting the ssh-agent or other scripts remove issues of spawning multiple agents or other such daemons. Other benefits of this design is to implement overwriting scripts such as language settings by changing this in the bash_profile on user login. For more information and an overview i found this link to be very useful. dotfiles This approach is probably an excellent way to source programs such as ROS and run one time configurations on startup..","title":"Separate single run bash profile from run each new shell with .profile"},{"location":"zet/20210930022826/#separate-single-run-bash-profile-from-run-each-new-shell-with-profile","text":"The .profile and .bash_profile files were originally intended to be run on startup to process single setup environment variables that in the past would have taken a long time to run. This is not much of an issue with current computers but does in fact present a good workflow habit. Separating single run scripts such as starting the ssh-agent or other scripts remove issues of spawning multiple agents or other such daemons. Other benefits of this design is to implement overwriting scripts such as language settings by changing this in the bash_profile on user login. For more information and an overview i found this link to be very useful. dotfiles This approach is probably an excellent way to source programs such as ROS and run one time configurations on startup..","title":"Separate single run bash profile from run each new shell with .profile"},{"location":"zet/20210930231139/","text":"Using vipe and vidir from the moreutils package I found out today about the vipe and vidir tools from the moreutils package. These too tools allow vim integration in pipe flows. The first tool vipe allows you to send a pipe into an editable vim buffer which can then be used to save as a file on close. For example: ls -a | grep .bash | vipe > bash_dotfiles allows the output of a grepped ls command searching for bash files to be piped into a vim buffer where any non .bash specific files could be removed before saving as a log file or something else. The above example is silly and mostly pointless but shows the flow. The second tool vidir allows you to edit the output of a find search and modify filenames or remove files from the directory. This is useful when you are looking at bulk renaming files in a folder. An example of using vidir is: find -type f | vidir - - Edit all files under the current directory and subdirectories. This tool could be useful when working with old folders.","title":"Using vipe and vidir from the moreutils package"},{"location":"zet/20210930231139/#using-vipe-and-vidir-from-the-moreutils-package","text":"I found out today about the vipe and vidir tools from the moreutils package. These too tools allow vim integration in pipe flows. The first tool vipe allows you to send a pipe into an editable vim buffer which can then be used to save as a file on close. For example: ls -a | grep .bash | vipe > bash_dotfiles allows the output of a grepped ls command searching for bash files to be piped into a vim buffer where any non .bash specific files could be removed before saving as a log file or something else. The above example is silly and mostly pointless but shows the flow. The second tool vidir allows you to edit the output of a find search and modify filenames or remove files from the directory. This is useful when you are looking at bulk renaming files in a folder. An example of using vidir is: find -type f | vidir - - Edit all files under the current directory and subdirectories. This tool could be useful when working with old folders.","title":"Using vipe and vidir from the moreutils package"},{"location":"zet/20211005120018/","text":"Use the -L option in curl to download github releases I often want to download and install packages from github to test out a program and often these packages arent available from apt or other managers. Sometimes i pull down the full package and compile it but this isnt always an option especially in debian based systems. With arch i can run git repositories through any AUR helper and build the package. Whilst i do not know of a way to do this is Ubuntu. Instead i found today i can use the -L on the releases link and it will pull it down with curl. This saves me from downloading the file to my downloads folder moving the file to somewhere else and running the installation. I can now do it all from the command line.","title":"Use the -L option in curl to download github releases"},{"location":"zet/20211005120018/#use-the-l-option-in-curl-to-download-github-releases","text":"I often want to download and install packages from github to test out a program and often these packages arent available from apt or other managers. Sometimes i pull down the full package and compile it but this isnt always an option especially in debian based systems. With arch i can run git repositories through any AUR helper and build the package. Whilst i do not know of a way to do this is Ubuntu. Instead i found today i can use the -L on the releases link and it will pull it down with curl. This saves me from downloading the file to my downloads folder moving the file to somewhere else and running the installation. I can now do it all from the command line.","title":"Use the -L option in curl to download github releases"},{"location":"zet/20211006002017/","text":"Using xev and awk to view keypress values Today i was working on fixing up my sxhkd configuration for keybindings and i wanted to have a help menu display when the user runs super ? , this binding is simple and makes it easy to remember however sxhkd did not use ? and could not understand what i was trying to send. The xev program saved my time and gave me an excellent setup for future keybindings. Linked with awk i was able to strip away all of the unneeded information and provide only the key number and the string output of the keypress. This allowed me to find out that the ? is question for use in sxhkd. The script is very simple which i have added here. On Ubuntu xev is already installed which makes it super simple to configure. xev | awk -F'[ )]+' '/^KeyPress/ { a[NR+2] } NR in a { printf \"%-3s %s\\n\", $5, $8 }'","title":"Using xev and awk to view keypress values"},{"location":"zet/20211006002017/#using-xev-and-awk-to-view-keypress-values","text":"Today i was working on fixing up my sxhkd configuration for keybindings and i wanted to have a help menu display when the user runs super ? , this binding is simple and makes it easy to remember however sxhkd did not use ? and could not understand what i was trying to send. The xev program saved my time and gave me an excellent setup for future keybindings. Linked with awk i was able to strip away all of the unneeded information and provide only the key number and the string output of the keypress. This allowed me to find out that the ? is question for use in sxhkd. The script is very simple which i have added here. On Ubuntu xev is already installed which makes it super simple to configure. xev | awk -F'[ )]+' '/^KeyPress/ { a[NR+2] } NR in a { printf \"%-3s %s\\n\", $5, $8 }'","title":"Using xev and awk to view keypress values"},{"location":"zet/20211006002421/","text":"Using a keybinding program across multiple systems Many window managers come prepackaged with a different keybinding manager. I found a very useful and simple keybinding manager known as sxhkd which is configured with very a very simple protocol. I am aiming to configure this to become my default keybinding program across all my systems overwriting the inbuilt systems in Ubuntu and Arch window managers. Once configured hopefully i will be able to use it across all of my systems","title":"Using a keybinding program across multiple systems"},{"location":"zet/20211006002421/#using-a-keybinding-program-across-multiple-systems","text":"Many window managers come prepackaged with a different keybinding manager. I found a very useful and simple keybinding manager known as sxhkd which is configured with very a very simple protocol. I am aiming to configure this to become my default keybinding program across all my systems overwriting the inbuilt systems in Ubuntu and Arch window managers. Once configured hopefully i will be able to use it across all of my systems","title":"Using a keybinding program across multiple systems"},{"location":"zet/20211006002810/","text":"Use yad to output a display for script help yad is a useful tool that i have used to create a script output of my keybindings for sxhkd. The program takes in the result of an awk or sed command and displays it in a pretty way. In my helpsxhkd script the awk program scans for # to find the comment on the command and outputs the command below the comment followed by a tab separated comment. The script is below. awk '/^[a-z]/ && last {print $0,\"\\t\",last} {last=\"\"} /^#/{last=$0}' ~/.config/sxhkd/*config | column -t -s $'\\t' | yad --text-info --back=#282c34 --fore=#46d9ff --geometry=1200x800 This simple script is then bound to a keybinding in my skhxd config to be able to show the keybindings set for sxhkd.","title":"Use yad to output a display for script help"},{"location":"zet/20211006002810/#use-yad-to-output-a-display-for-script-help","text":"yad is a useful tool that i have used to create a script output of my keybindings for sxhkd. The program takes in the result of an awk or sed command and displays it in a pretty way. In my helpsxhkd script the awk program scans for # to find the comment on the command and outputs the command below the comment followed by a tab separated comment. The script is below. awk '/^[a-z]/ && last {print $0,\"\\t\",last} {last=\"\"} /^#/{last=$0}' ~/.config/sxhkd/*config | column -t -s $'\\t' | yad --text-info --back=#282c34 --fore=#46d9ff --geometry=1200x800 This simple script is then bound to a keybinding in my skhxd config to be able to show the keybindings set for sxhkd.","title":"Use yad to output a display for script help"},{"location":"zet/20211012095840/","text":"Use pv to add a progress bar to any pipe. If you want a progress bar in any linux command pipe the file into pv and then send the output to the required location. For example if we want to copy an iso onto a drive we can use the below command. cat arch.iso | pv > /dev/sdc This above command is also the better way to write isos to drives rather than using dd .","title":"Use pv to add a progress bar to any pipe."},{"location":"zet/20211012095840/#use-pv-to-add-a-progress-bar-to-any-pipe","text":"If you want a progress bar in any linux command pipe the file into pv and then send the output to the required location. For example if we want to copy an iso onto a drive we can use the below command. cat arch.iso | pv > /dev/sdc This above command is also the better way to write isos to drives rather than using dd .","title":"Use pv to add a progress bar to any pipe."},{"location":"zet/20211012234810/","text":"Running random commands is always bad I found this cool command online toda","title":"Running random commands is always bad"},{"location":"zet/20211012234810/#running-random-commands-is-always-bad","text":"I found this cool command online toda","title":"Running random commands is always bad"},{"location":"zet/20211013120536/","text":"Found this test from a company looking to hire someone. The task was to find the f1 score of tuesdays in the dataset. The file was labeled as a .psv file. I decided ill just run cat to see if there was anything in it. Turns out psv is pipe separated variable. Once i found out that there was data in the file in a \"csv\" like file i moved to python. Using the datetime library, csv and sklearn i was able to filter out all lines that were tuesdays and then run the f1 score across them. The code took 20 minutes to write which i was quite happy with. The code is shown below. I iteratively tested using ipython which was easy to work with. import datetime import csv from sklearn.metrics import f1_score with open('test_v2.psv', newline='') as csvf: file = csv.reader(csvf, delimiter='|') tuesdays=[] for i, row in enumerate(file): if i == 0: pass elif i == 1: pass else: col1=row[0].split('-') d = datetime(int(col1[0]), int(col1[1]), int(col1[2])) if d.weekday() == 1: tuesdays.append(row) print(f'Days : {tuesdays}') y = [] y_hat = [] for item in tuesdays: y.append(int(item[1])) y_hat.append(int(item[2])) res = f1_score(y,y_hat) I wasn't certain of the answer i got having thrown the solution together and so i thought i would manually run the f1 measurement. I needed to clean up the data to feed it into libre office and this was where my bash learning came in handy. Using a while loop, parameter substitution, process substitution and sending the results to a new file i was able to convert the psv into a csv file. Admittedly it looked for all | and if there was any outside of the main data it would change that too. But i wasn't worried. The shell line i wrote is seen below. while read line; do string=${line//|/,}; echo $string; echo $string >> test3.csv; done < <(cat test_v2.psv) Once this was done i opened it up in libreoffice and calculated the result. I like that i was able to use my bash skills to run a fast file modification to allow the program to run in libreoffice and am keen to try more of these. #python #bash #funcoding","title":"Found this test from a company looking to hire someone."},{"location":"zet/20211013120536/#found-this-test-from-a-company-looking-to-hire-someone","text":"The task was to find the f1 score of tuesdays in the dataset. The file was labeled as a .psv file. I decided ill just run cat to see if there was anything in it. Turns out psv is pipe separated variable. Once i found out that there was data in the file in a \"csv\" like file i moved to python. Using the datetime library, csv and sklearn i was able to filter out all lines that were tuesdays and then run the f1 score across them. The code took 20 minutes to write which i was quite happy with. The code is shown below. I iteratively tested using ipython which was easy to work with. import datetime import csv from sklearn.metrics import f1_score with open('test_v2.psv', newline='') as csvf: file = csv.reader(csvf, delimiter='|') tuesdays=[] for i, row in enumerate(file): if i == 0: pass elif i == 1: pass else: col1=row[0].split('-') d = datetime(int(col1[0]), int(col1[1]), int(col1[2])) if d.weekday() == 1: tuesdays.append(row) print(f'Days : {tuesdays}') y = [] y_hat = [] for item in tuesdays: y.append(int(item[1])) y_hat.append(int(item[2])) res = f1_score(y,y_hat) I wasn't certain of the answer i got having thrown the solution together and so i thought i would manually run the f1 measurement. I needed to clean up the data to feed it into libre office and this was where my bash learning came in handy. Using a while loop, parameter substitution, process substitution and sending the results to a new file i was able to convert the psv into a csv file. Admittedly it looked for all | and if there was any outside of the main data it would change that too. But i wasn't worried. The shell line i wrote is seen below. while read line; do string=${line//|/,}; echo $string; echo $string >> test3.csv; done < <(cat test_v2.psv) Once this was done i opened it up in libreoffice and calculated the result. I like that i was able to use my bash skills to run a fast file modification to allow the program to run in libreoffice and am keen to try more of these. #python #bash #funcoding","title":"Found this test from a company looking to hire someone."},{"location":"zet/20211019053929/","text":"Using rich in python","title":"Using rich in python"},{"location":"zet/20211019053929/#using-rich-in-python","text":"","title":"Using rich in python"},{"location":"zet/20211024220524/","text":"Reinforcing what i have learnt by using it in the terminal I have learnt a huge amount in bash and am beginning to see how powerfull the bash knowledge can be for working with linux. However i often write a script to do something and then just run the script. This is great for having goto commands at my disposal but some times i should write the command out rather than putting it in a script. For example if i need to find out how many files are in a directory rather than having a script i should write something like: Count=0; for File in *; { [ -f \"$File\" ] && let Count++; } echo \"$Count\" This neat little script reinforces a couple of functions that help make linux integration with bash so powerful. The use of a for loop and a simple test -f to find whether the item is a file are super useful commands.","title":"Reinforcing what i have learnt by using it in the terminal"},{"location":"zet/20211024220524/#reinforcing-what-i-have-learnt-by-using-it-in-the-terminal","text":"I have learnt a huge amount in bash and am beginning to see how powerfull the bash knowledge can be for working with linux. However i often write a script to do something and then just run the script. This is great for having goto commands at my disposal but some times i should write the command out rather than putting it in a script. For example if i need to find out how many files are in a directory rather than having a script i should write something like: Count=0; for File in *; { [ -f \"$File\" ] && let Count++; } echo \"$Count\" This neat little script reinforces a couple of functions that help make linux integration with bash so powerful. The use of a for loop and a simple test -f to find whether the item is a file are super useful commands.","title":"Reinforcing what i have learnt by using it in the terminal"},{"location":"zet/20211025023420/","text":"New programs to play with using fzf Recently i have been looking at the use case of fzf. Whilst it is an excellent tool for changing my reverse search to be more interactive i have found a couple of amazing tools written around fzf and i have added some of these to my workflow. The main one i found and integrated was fzfgit fzfgit This program is not complete however it has some very useful features and maybe one day i might extend it to add the other features it needs. The program uses fzf and git diff to interactively add and commit git repos. The system is intuitive and reduces that one step in having to check diffs before adding and commiting. fzmv A simple fuzzy finder move tool to help you choose the files you wish to move to a new directory. This is helpful to reduce the time it takes to move a bunch of files and PeepOpenrovides a little interactiveness. fv A simple terminal file manager for the current directory. Super light simple and provides folder contents and a file previewer. sshget A command line rsync manager using fzf. This tool is extremely useful when working with my Jetson boards.","title":"New programs to play with using fzf"},{"location":"zet/20211025023420/#new-programs-to-play-with-using-fzf","text":"Recently i have been looking at the use case of fzf. Whilst it is an excellent tool for changing my reverse search to be more interactive i have found a couple of amazing tools written around fzf and i have added some of these to my workflow. The main one i found and integrated was fzfgit fzfgit This program is not complete however it has some very useful features and maybe one day i might extend it to add the other features it needs. The program uses fzf and git diff to interactively add and commit git repos. The system is intuitive and reduces that one step in having to check diffs before adding and commiting. fzmv A simple fuzzy finder move tool to help you choose the files you wish to move to a new directory. This is helpful to reduce the time it takes to move a bunch of files and PeepOpenrovides a little interactiveness. fv A simple terminal file manager for the current directory. Super light simple and provides folder contents and a file previewer. sshget A command line rsync manager using fzf. This tool is extremely useful when working with my Jetson boards.","title":"New programs to play with using fzf"},{"location":"zet/20211025030316/","text":"Developing a userguide to remember the programs i installed and configured in a certain way I have found recently that i am learning so many things in linux and the command line that i forget certain programs some of these are incredibly cool. I have decided that on top of my slowly evolving idea for a blog site i am also going to jot down programs that i have found really useful. This guide will include links to ansible playbooks that i have developed along with guides on installation. Ultimately i will fork Chris Titus Tech arch installer and modify it with all the programs i wish to have on my system.","title":"Developing a userguide to remember the programs i installed and configured in a certain way"},{"location":"zet/20211025030316/#developing-a-userguide-to-remember-the-programs-i-installed-and-configured-in-a-certain-way","text":"I have found recently that i am learning so many things in linux and the command line that i forget certain programs some of these are incredibly cool. I have decided that on top of my slowly evolving idea for a blog site i am also going to jot down programs that i have found really useful. This guide will include links to ansible playbooks that i have developed along with guides on installation. Ultimately i will fork Chris Titus Tech arch installer and modify it with all the programs i wish to have on my system.","title":"Developing a userguide to remember the programs i installed and configured in a certain way"},{"location":"zet/20211025030953/","text":"vim command shortcut to save readonly files. I found this excellent vimrc snippet today. command! W execute 'w !sudo tee % > /dev/null' <bar> edit! This command does the sudo tee command that i have been doing to save readonly files but rather than typng it out i can just :W which makes sense in a force write kind of way. I still need to make sure i use the command every now and then so i don't forget it.","title":"vim command shortcut to save readonly files."},{"location":"zet/20211025030953/#vim-command-shortcut-to-save-readonly-files","text":"I found this excellent vimrc snippet today. command! W execute 'w !sudo tee % > /dev/null' <bar> edit! This command does the sudo tee command that i have been doing to save readonly files but rather than typng it out i can just :W which makes sense in a force write kind of way. I still need to make sure i use the command every now and then so i don't forget it.","title":"vim command shortcut to save readonly files."},{"location":"zet/20211025104638/","text":"Mental Note for running jupyter lab Run jupyter lab with jupyter lab 2>/dev/null & to clear output and free up the terminal. This is helpful when not running tmux and not wanting to open another terminal. To kill the jupyter lab we can run kill -9 $(ps aux | grep [j]upyter | awk '{print $2}') which basically runs the process search with PeepOpens, greps for jupyter without grepping for the grep process, this is done with the [j]. Then the awk statement is used to show the process that is killed. Finally this is all wrapped in a process substitution which is then sent to the kill command with a sigterm -9 For fun i wrapped it in a program to kill jupyter and other programs if needed. The script is below. #!/bin/sh # Description: Simple program to kill a specific process from the command line. This uses ps, grep, awk and substring # searching. Not the most elegent way to kill a program. file=$1 first=${file:0:1} #get first letter grepfirst=[\"$first\"] #used to remove grep command from ps last=${file#$first*} ## alternative last=$(cut -d $first -f2 <<< $file) #get everythin after the first letter res=$grepfirst$last echo \"$res\" kill -9 $(ps u | grep $res | awk '{print $2}')","title":"Mental Note for running jupyter lab"},{"location":"zet/20211025104638/#mental-note-for-running-jupyter-lab","text":"Run jupyter lab with jupyter lab 2>/dev/null & to clear output and free up the terminal. This is helpful when not running tmux and not wanting to open another terminal. To kill the jupyter lab we can run kill -9 $(ps aux | grep [j]upyter | awk '{print $2}') which basically runs the process search with PeepOpens, greps for jupyter without grepping for the grep process, this is done with the [j]. Then the awk statement is used to show the process that is killed. Finally this is all wrapped in a process substitution which is then sent to the kill command with a sigterm -9 For fun i wrapped it in a program to kill jupyter and other programs if needed. The script is below. #!/bin/sh # Description: Simple program to kill a specific process from the command line. This uses ps, grep, awk and substring # searching. Not the most elegent way to kill a program. file=$1 first=${file:0:1} #get first letter grepfirst=[\"$first\"] #used to remove grep command from ps last=${file#$first*} ## alternative last=$(cut -d $first -f2 <<< $file) #get everythin after the first letter res=$grepfirst$last echo \"$res\" kill -9 $(ps u | grep $res | awk '{print $2}')","title":"Mental Note for running jupyter lab"},{"location":"zet/20211103003842/","text":"Configuring Docker and using shortcut commands Today i was trying to configure my new system with docker so i could continue some docker fun. I was trying to pull down an image and had issues connecting to docker hub. Always make sure that dockerd is running if not run systemctl enable docker and systemctl start docker . Also if you haven't done so add docker to the user group sudo usermod -aG docker $USER When this is done the system should enable you to pull images down. Once down you can use a simple program to start an image. To begin we need to pull the image down. docker pull <image> Once pulled we can run the container for the first time giving it a name to be used later. docker run -it --name my_container -v src:dest <image> Once this is created we are able to exit and reattach to the container whenever we wish, i wrote a simple program that starts and attaches to the container or sends an error saying the container name is not there. The script is below. #!/bin/bash container_name=\"$@\" [ ! `docker inspect --format='{{.Name}}' $(sudo docker ps -aq --no-trunc) | grep \"$container_name\"` ] && echo \"Container doesn't exist exiting\" && exit echo \"Container $container_name found attaching\" if [ ` docker container inspect -f '{{.State.Running}}' \"$container_name\" ` == \"true\" ]; then docker attach \"$container_name\" else docker start \"$container_name\" docker attach \"$container_name\" fi Simply create the script in your path with a name such as da then when you need to run the container just da my_container and you are straight into the container. If we did not configure the container correctly to begin with we can run docker rm my_container to remove the container however all information will be lost. If information needs to be moved out of the container we can use a script to retroactively mount a volume into a container. The script is found at my github. (bootstrapdockervol)[github.com/qwertimer/.dotfiles/scripts/bootstrapdockervol)","title":"Configuring Docker and using shortcut commands"},{"location":"zet/20211103003842/#configuring-docker-and-using-shortcut-commands","text":"Today i was trying to configure my new system with docker so i could continue some docker fun. I was trying to pull down an image and had issues connecting to docker hub. Always make sure that dockerd is running if not run systemctl enable docker and systemctl start docker . Also if you haven't done so add docker to the user group sudo usermod -aG docker $USER When this is done the system should enable you to pull images down. Once down you can use a simple program to start an image. To begin we need to pull the image down. docker pull <image> Once pulled we can run the container for the first time giving it a name to be used later. docker run -it --name my_container -v src:dest <image> Once this is created we are able to exit and reattach to the container whenever we wish, i wrote a simple program that starts and attaches to the container or sends an error saying the container name is not there. The script is below. #!/bin/bash container_name=\"$@\" [ ! `docker inspect --format='{{.Name}}' $(sudo docker ps -aq --no-trunc) | grep \"$container_name\"` ] && echo \"Container doesn't exist exiting\" && exit echo \"Container $container_name found attaching\" if [ ` docker container inspect -f '{{.State.Running}}' \"$container_name\" ` == \"true\" ]; then docker attach \"$container_name\" else docker start \"$container_name\" docker attach \"$container_name\" fi Simply create the script in your path with a name such as da then when you need to run the container just da my_container and you are straight into the container. If we did not configure the container correctly to begin with we can run docker rm my_container to remove the container however all information will be lost. If information needs to be moved out of the container we can use a script to retroactively mount a volume into a container. The script is found at my github. (bootstrapdockervol)[github.com/qwertimer/.dotfiles/scripts/bootstrapdockervol)","title":"Configuring Docker and using shortcut commands"},{"location":"zet/20211103013656/","text":"Broken Zet test My zet is broken in commits. Hopefully i have fixed it. Ignore this zet","title":"Broken Zet test"},{"location":"zet/20211103013656/#broken-zet-test","text":"My zet is broken in commits. Hopefully i have fixed it. Ignore this zet","title":"Broken Zet test"},{"location":"zet/20211103032632/","text":"TIL -r can be added after cp and rm When i have gone to run a command on a folder and forgotten to add -r i have always gone back to the previous command and added the -r in the middle of the command. However i realised today i can add -r after the directories and it will still work. This saves me so much time and i should have realised this years ago.","title":"TIL -r can be added after cp and rm"},{"location":"zet/20211103032632/#til-r-can-be-added-after-cp-and-rm","text":"When i have gone to run a command on a folder and forgotten to add -r i have always gone back to the previous command and added the -r in the middle of the command. However i realised today i can add -r after the directories and it will still work. This saves me so much time and i should have realised this years ago.","title":"TIL -r can be added after cp and rm"},{"location":"zet/20211105213546/","text":"Interesting article on sideloading software into a docker container https://www.randori.com/blog/side-loading-software-in-a-running-docker-container/ Talks about how using nsenter can give us container access from the host to sideload applications into the container.","title":"Interesting article on sideloading software into a docker container"},{"location":"zet/20211105213546/#interesting-article-on-sideloading-software-into-a-docker-container","text":"https://www.randori.com/blog/side-loading-software-in-a-running-docker-container/ Talks about how using nsenter can give us container access from the host to sideload applications into the container.","title":"Interesting article on sideloading software into a docker container"},{"location":"zet/20211107231021/","text":"Useful Vim commands Watching the","title":"Useful Vim commands"},{"location":"zet/20211107231021/#useful-vim-commands","text":"Watching the","title":"Useful Vim commands"},{"location":"zet/20211110121809/","text":"Configuring sxhkd for gnome and bsp is fun I was trying to get a more unified gnome and bsp experience with my keybindings and started delving into the simple keybinding tool sxhkd. I was struggling to get gnome configured correctly then i found that i could use the xdotool and pretty much do what ever i needed to. At the moment i have two configurations with many similarities between bspwm and gnome. Ultimately i plan on having a launcher script that checks wm and runs whichever config is needed. I also found out that the sxhkdrc will take precedence over other bindings and so have to figure out a configuration that might utilise this. I also found people using mappings to run different 'modes' for sxhkd, which i might look into.","title":"Configuring sxhkd for gnome and bsp is fun"},{"location":"zet/20211110121809/#configuring-sxhkd-for-gnome-and-bsp-is-fun","text":"I was trying to get a more unified gnome and bsp experience with my keybindings and started delving into the simple keybinding tool sxhkd. I was struggling to get gnome configured correctly then i found that i could use the xdotool and pretty much do what ever i needed to. At the moment i have two configurations with many similarities between bspwm and gnome. Ultimately i plan on having a launcher script that checks wm and runs whichever config is needed. I also found out that the sxhkdrc will take precedence over other bindings and so have to figure out a configuration that might utilise this. I also found people using mappings to run different 'modes' for sxhkd, which i might look into.","title":"Configuring sxhkd for gnome and bsp is fun"},{"location":"zet/20211110223954/","text":"Get good at git One thing i am slowly learning is how to work well in git. Today i needed to clean up my main development that i had stuffed up. To do this i ran git reset --hard origin/main This killed the current modifications. On the directory. If some things aren't removed i can then run git clean -sdf To remove these files. One issue i find is that i haven't always labelled the remote branch as origin. To find out what this branch is called, i can either call git remote show or git branch -vv Which will give me information on my current remote upstream. I have also been looking at how to do breaking changes in development and started looking into git worktree . There are a couple of good articles out there including (git-worktree)[https://opensource.com/article/21/4/git-worktree] (best git features)[https://levelup.gitconnected.com/git-worktrees-the-best-git-feature-youve-never-heard-of-9cd21df67baf] (never switch branches)[https://www.theserverside.com/blog/Coffee-Talk-Java-News-Stories-and-Opinions/Use-this-git-worktree-add-example-and-never-switch-branches-again] There is so much more i need to learn about git but i am slowly getting there.","title":"Get good at git"},{"location":"zet/20211110223954/#get-good-at-git","text":"One thing i am slowly learning is how to work well in git. Today i needed to clean up my main development that i had stuffed up. To do this i ran git reset --hard origin/main This killed the current modifications. On the directory. If some things aren't removed i can then run git clean -sdf To remove these files. One issue i find is that i haven't always labelled the remote branch as origin. To find out what this branch is called, i can either call git remote show or git branch -vv Which will give me information on my current remote upstream. I have also been looking at how to do breaking changes in development and started looking into git worktree . There are a couple of good articles out there including (git-worktree)[https://opensource.com/article/21/4/git-worktree] (best git features)[https://levelup.gitconnected.com/git-worktrees-the-best-git-feature-youve-never-heard-of-9cd21df67baf] (never switch branches)[https://www.theserverside.com/blog/Coffee-Talk-Java-News-Stories-and-Opinions/Use-this-git-worktree-add-example-and-never-switch-branches-again] There is so much more i need to learn about git but i am slowly getting there.","title":"Get good at git"},{"location":"zet/20211110230335/","text":"Using realpath when creating symlinks If i have some crazy nested folder location like my .dotfiles folder rather than writing out the whole directory i can use ln -s \"$(realpath <filename>)\" <dest dir> Whats great about this is it becomes super fast to set up all my symlinks. It doesn't work for scripts though.","title":"Using realpath when creating symlinks"},{"location":"zet/20211110230335/#using-realpath-when-creating-symlinks","text":"If i have some crazy nested folder location like my .dotfiles folder rather than writing out the whole directory i can use ln -s \"$(realpath <filename>)\" <dest dir> Whats great about this is it becomes super fast to set up all my symlinks. It doesn't work for scripts though.","title":"Using realpath when creating symlinks"},{"location":"zet/20211122210703/","text":"Mimic cat in pure bash Watching terminalforlife video he showed how you can mimic cat with pure bash. while read; do printf '%s\\n' \"$REPLY\" done < 'file' Where 'file' is any file on the system. Useful method to reduce calls to other programs such as cat when you dont need to. #bash","title":"Mimic cat in pure bash"},{"location":"zet/20211122210703/#mimic-cat-in-pure-bash","text":"Watching terminalforlife video he showed how you can mimic cat with pure bash. while read; do printf '%s\\n' \"$REPLY\" done < 'file' Where 'file' is any file on the system. Useful method to reduce calls to other programs such as cat when you dont need to. #bash","title":"Mimic cat in pure bash"},{"location":"zet/20211122210928/","text":"Mimic cut in pure bash Watching terminalforlife video https://www.youtube.com/watch?v=UwlbOGn0cW4 . He shoed that you can replace cut with a while read loop. To do so you ned to change the IFS to which ever delimiter you want to cut on. For example inthe video he read the passwd file in which is delimited by :. while IFS=':' read -a Line; do printf '%s\\n' \"${Line[2]}\" done < /etc/passwd The code above reads into an array the lines from passwd. Then based on the value in {} displays that column.","title":"Mimic cut in pure bash"},{"location":"zet/20211122210928/#mimic-cut-in-pure-bash","text":"Watching terminalforlife video https://www.youtube.com/watch?v=UwlbOGn0cW4 . He shoed that you can replace cut with a while read loop. To do so you ned to change the IFS to which ever delimiter you want to cut on. For example inthe video he read the passwd file in which is delimited by :. while IFS=':' read -a Line; do printf '%s\\n' \"${Line[2]}\" done < /etc/passwd The code above reads into an array the lines from passwd. Then based on the value in {} displays that column.","title":"Mimic cut in pure bash"},{"location":"zet/20211122211310/","text":"Get wc using pure bash To get the count of words in a file we can use bash's readarray function and the # sine in the array call along with the '@' to get the amount of words in a file: readarray Lines < /etc/passwd printf '%d\\n' ${#Lines[@]} To get the length of a string we can do: read <<< \"$USER\" printf '%d\\n' ${#REPLY}","title":"Get wc using pure bash"},{"location":"zet/20211122211310/#get-wc-using-pure-bash","text":"To get the count of words in a file we can use bash's readarray function and the # sine in the array call along with the '@' to get the amount of words in a file: readarray Lines < /etc/passwd printf '%d\\n' ${#Lines[@]} To get the length of a string we can do: read <<< \"$USER\" printf '%d\\n' ${#REPLY}","title":"Get wc using pure bash"},{"location":"zet/20211122211712/","text":"Replace grep with bash We can use bash to do a simple grep for a line. This saves us running grep for a simple check. while read; do if [[ $REPLY =~ ^ichy ]]; then printf '%s\\n' \"$REPLY\" break fi done < /etc/passwd","title":"Replace grep with bash"},{"location":"zet/20211122211712/#replace-grep-with-bash","text":"We can use bash to do a simple grep for a line. This saves us running grep for a simple check. while read; do if [[ $REPLY =~ ^ichy ]]; then printf '%s\\n' \"$REPLY\" break fi done < /etc/passwd","title":"Replace grep with bash"},{"location":"zet/20211123105419/","text":"Using sys folder to access system information Inside the sys folder you can get access to a large amount of system information, for example we can find out how much storage exists in the system. declare -i Sectors=0 for Dir in /sys/block/*; { [ -d \"$Dir\" ] || continue Sectors+=`< \"$Dir\"/size` } Result=$(( (Sectors * 512) / 1024 / 1024 )) printf \"Total: %'d GiB\\n\" $Result This script scans all the files in the block folder and adds the size of the directory to the sectors. This is then converted to Bytes and then to GiB. We can also access CPU core temperature in the sys folder. This is found in class and hwmon. Inside this folder we can find the core temperature of the CPU as well as each core. We can also access the wireless transmission and recieve information in /sys/class/net/wlan0/statistics allowing us to monitor data io over the network. There is so much inside this that it is worthwhile spending some time looking through it.","title":"Using sys folder to access system information"},{"location":"zet/20211123105419/#using-sys-folder-to-access-system-information","text":"Inside the sys folder you can get access to a large amount of system information, for example we can find out how much storage exists in the system. declare -i Sectors=0 for Dir in /sys/block/*; { [ -d \"$Dir\" ] || continue Sectors+=`< \"$Dir\"/size` } Result=$(( (Sectors * 512) / 1024 / 1024 )) printf \"Total: %'d GiB\\n\" $Result This script scans all the files in the block folder and adds the size of the directory to the sectors. This is then converted to Bytes and then to GiB. We can also access CPU core temperature in the sys folder. This is found in class and hwmon. Inside this folder we can find the core temperature of the CPU as well as each core. We can also access the wireless transmission and recieve information in /sys/class/net/wlan0/statistics allowing us to monitor data io over the network. There is so much inside this that it is worthwhile spending some time looking through it.","title":"Using sys folder to access system information"},{"location":"zet/20211123111358/","text":"We can modify wget and curl to use different browser agents For example if we want to pretend we are downloading something with firefox. We can use the -U flag with the 'Mozilla/5.0' to pretend to be firefox. We can also do this with curl by using the -A flag to do the same thing.","title":"We can modify wget and curl to use different browser agents"},{"location":"zet/20211123111358/#we-can-modify-wget-and-curl-to-use-different-browser-agents","text":"For example if we want to pretend we are downloading something with firefox. We can use the -U flag with the 'Mozilla/5.0' to pretend to be firefox. We can also do this with curl by using the -A flag to do the same thing.","title":"We can modify wget and curl to use different browser agents"},{"location":"zet/20211123111630/","text":"Use -C in wget or -c - in curl to pick up where you left off If your connection dies whilst downloading a file you can use the -C flag in wget or the -c - in curl to pick up where you left off. Super useful if downloading a large file.","title":"Use -C in wget or -c - in curl to pick up where you left off"},{"location":"zet/20211123111630/#use-c-in-wget-or-c-in-curl-to-pick-up-where-you-left-off","text":"If your connection dies whilst downloading a file you can use the -C flag in wget or the -c - in curl to pick up where you left off. Super useful if downloading a large file.","title":"Use -C in wget or -c - in curl to pick up where you left off"},{"location":"zet/20211123112016/","text":"Instead of using cut, grep or awk to get things like UID We can you pure bash to get things like UID from the passwd file with a simple while read loop with a comparison search. This way we don't need to invoke a subshell or anything else. while IFS=':' read -a Line; do if [ \"${Line[0]}\" == 'qwertimer' ]; then printf '%d\\n' ${Line[2]} break fi done < /etc/passwd","title":"Instead of using cut, grep or awk to get things like UID"},{"location":"zet/20211123112016/#instead-of-using-cut-grep-or-awk-to-get-things-like-uid","text":"We can you pure bash to get things like UID from the passwd file with a simple while read loop with a comparison search. This way we don't need to invoke a subshell or anything else. while IFS=':' read -a Line; do if [ \"${Line[0]}\" == 'qwertimer' ]; then printf '%d\\n' ${Line[2]} break fi done < /etc/passwd","title":"Instead of using cut, grep or awk to get things like UID"},{"location":"zet/20211123112736/","text":"Pure bash implementation of the wc command with -w flag We can emulate wc with the -w flag in bash with two lines of code removing the need to invoke wc in our script. read -a String <<< 'This is a string with multiple words' printf '%d\\n' ${#String[@]}","title":"Pure bash implementation of the wc command with -w flag"},{"location":"zet/20211123112736/#pure-bash-implementation-of-the-wc-command-with-w-flag","text":"We can emulate wc with the -w flag in bash with two lines of code removing the need to invoke wc in our script. read -a String <<< 'This is a string with multiple words' printf '%d\\n' ${#String[@]}","title":"Pure bash implementation of the wc command with -w flag"},{"location":"zet/20211123113320/","text":"Bash has sed like substitution capabilities built in We can use // in a variable to get substitution capability. For example if we want to globally substitute parts of a username we can do this with printf '%s\\n' \"${USER//er/a}\" This in my case will remove er from qwertimer and replace it with a giving me the username of qwertima. Extremely useful in many situations.","title":"Bash has sed like substitution capabilities built in"},{"location":"zet/20211123113320/#bash-has-sed-like-substitution-capabilities-built-in","text":"We can use // in a variable to get substitution capability. For example if we want to globally substitute parts of a username we can do this with printf '%s\\n' \"${USER//er/a}\" This in my case will remove er from qwertimer and replace it with a giving me the username of qwertima. Extremely useful in many situations.","title":"Bash has sed like substitution capabilities built in"},{"location":"zet/20211123231426/","text":"Use nmap to check network and poll for ssh port nmap is a super powerful tool using nmap -sP <network> i can scan my network for addresses. Then i can use nmap -sv -p- <ip-addr> to get the port my ip is using for ssh. This is super useful when i have not setup or configured my ssh configs.","title":"Use nmap to check network and poll for ssh port"},{"location":"zet/20211123231426/#use-nmap-to-check-network-and-poll-for-ssh-port","text":"nmap is a super powerful tool using nmap -sP <network> i can scan my network for addresses. Then i can use nmap -sv -p- <ip-addr> to get the port my ip is using for ssh. This is super useful when i have not setup or configured my ssh configs.","title":"Use nmap to check network and poll for ssh port"},{"location":"zet/20211126055005/","text":"Use !$ to expand last parameter of most recent command When running commands in the terminal we can use !$ to expand the last parameter, this is useful if you're running a command and then wish to use it in the next line. An example of this is when we create script then need to chmod it. vi coolscript chmod +x !$ Where !$ will expand out to coolscript.","title":"Use !$ to expand last parameter of most recent command"},{"location":"zet/20211126055005/#use-to-expand-last-parameter-of-most-recent-command","text":"When running commands in the terminal we can use !$ to expand the last parameter, this is useful if you're running a command and then wish to use it in the next line. An example of this is when we create script then need to chmod it. vi coolscript chmod +x !$ Where !$ will expand out to coolscript.","title":"Use !$ to expand last parameter of most recent command"},{"location":"zet/20211130011908/","text":"Incorporating fastcore into everday python work Whist fastcore is designed for use with the fastai library for deep learning there are some very useful tools for use in normal development frameworks. Some of these i have outlined below. Setting instance attributes Rather than having to write: class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c the library provides a helper function. store_attr() which reduces code duplication. We can also use a keyword but to exclude any attribute. Type Dispatch Or multiple dispatch allows 'function overloading' where by the function dispatches to different methods based on the type passed. The only issue in this regards is the requirement to type annotate your code. To implement this we can use the @typedispatch basic_repr Functionality to add a simple repr based on the attributions in the __init__ . patch and patch_to Decorator functions to monkey patch classes using type annotations. You can use as_prop and cls_method with the decorator. as_prop allows the method to act as an attribute. The cls_method will add the classmethod decorator to the function. fast scripting tools fastcore.script provides wrappers around argparse to make cli tooling fast. just decorate your function with @call_parse , provide type annotation followed by a comment and the program will autoscript with help and the prewriiten switches. from fastcore.script import * @call_parse def main(msg:str, # The message upper:bool): # Convert to uppercase? \"Print `msg`, optionally converting to uppercase\" print(msg.upper() if upper else msg)","title":"Incorporating fastcore into everday python work"},{"location":"zet/20211130011908/#incorporating-fastcore-into-everday-python-work","text":"Whist fastcore is designed for use with the fastai library for deep learning there are some very useful tools for use in normal development frameworks. Some of these i have outlined below. Setting instance attributes Rather than having to write: class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c the library provides a helper function. store_attr() which reduces code duplication. We can also use a keyword but to exclude any attribute. Type Dispatch Or multiple dispatch allows 'function overloading' where by the function dispatches to different methods based on the type passed. The only issue in this regards is the requirement to type annotate your code. To implement this we can use the @typedispatch basic_repr Functionality to add a simple repr based on the attributions in the __init__ . patch and patch_to Decorator functions to monkey patch classes using type annotations. You can use as_prop and cls_method with the decorator. as_prop allows the method to act as an attribute. The cls_method will add the classmethod decorator to the function. fast scripting tools fastcore.script provides wrappers around argparse to make cli tooling fast. just decorate your function with @call_parse , provide type annotation followed by a comment and the program will autoscript with help and the prewriiten switches. from fastcore.script import * @call_parse def main(msg:str, # The message upper:bool): # Convert to uppercase? \"Print `msg`, optionally converting to uppercase\" print(msg.upper() if upper else msg)","title":"Incorporating fastcore into everday python work"},{"location":"zet/20211130213831/","text":"smug - a simple tmux session manager Smug sets up tmux sessions using a simple yaml configuration file. The simplicity of this configuration abstracts away from the tmux default config tools. To get smug we can download it from https://github.com/ivaaaan/smug.git. To run this we set up a configuration file which can be stored in ~/.config/smug or pointed to with the -f switch. We can use smug new to create a new configuration file. Then we can start the tmux ession with smug start `.","title":"smug - a simple tmux session manager"},{"location":"zet/20211130213831/#smug-a-simple-tmux-session-manager","text":"Smug sets up tmux sessions using a simple yaml configuration file. The simplicity of this configuration abstracts away from the tmux default config tools. To get smug we can download it from https://github.com/ivaaaan/smug.git. To run this we set up a configuration file which can be stored in ~/.config/smug or pointed to with the -f switch. We can use smug new to create a new configuration file. Then we can start the tmux ession with smug start `.","title":"smug - a simple tmux session manager"},{"location":"zet/20211208030540/","text":"Useful curl Note - Getting the latest version number from a github project If we need to find the latest version of any project we can use curl and jq to check for the latest curl -sSL https://api.github.com/repos/<package>/releases/latest | jq -r .name","title":"Useful curl Note - Getting the latest version number from a github project"},{"location":"zet/20211208030540/#useful-curl-note-getting-the-latest-version-number-from-a-github-project","text":"If we need to find the latest version of any project we can use curl and jq to check for the latest curl -sSL https://api.github.com/repos/<package>/releases/latest | jq -r .name","title":"Useful curl Note - Getting the latest version number from a github project"},{"location":"zet/20211208104628/","text":"Parallelising a process in bash If we have a process that we want to parallelise we can use a simple script to parallelise the process and limit the number of processes with the N variable This allows us to spawn multiple versions of the program but limit resource usage with the variable. If we want to run all the items at once we can just use the & symbol to send each process to the background. #!/bin/bash N=6 ( for item in \"$@\"; do ((i=i%N)); ((i++==0)) && wait python3 safaribooks.py \"$item\" & done )","title":"Parallelising a process in bash"},{"location":"zet/20211208104628/#parallelising-a-process-in-bash","text":"If we have a process that we want to parallelise we can use a simple script to parallelise the process and limit the number of processes with the N variable This allows us to spawn multiple versions of the program but limit resource usage with the variable. If we want to run all the items at once we can just use the & symbol to send each process to the background. #!/bin/bash N=6 ( for item in \"$@\"; do ((i=i%N)); ((i++==0)) && wait python3 safaribooks.py \"$item\" & done )","title":"Parallelising a process in bash"},{"location":"zet/20211209205047/","text":"Vim tricks: Using gf to open file below cursor and extending it with a new mapping gf is a fast tool to open a file that is being used in the current vim buffer. For example if i am working on a python program and i have a config file that i am accessing, if i hover over the file directory, gf will open the file. If the file doesn't exist gf fails. To make it open non existent files for editing we add to the vimrc map gf :edit <cfile><cr> .","title":"Vim tricks: Using gf to open file below cursor and extending it with a new mapping"},{"location":"zet/20211209205047/#vim-tricks-using-gf-to-open-file-below-cursor-and-extending-it-with-a-new-mapping","text":"gf is a fast tool to open a file that is being used in the current vim buffer. For example if i am working on a python program and i have a config file that i am accessing, if i hover over the file directory, gf will open the file. If the file doesn't exist gf fails. To make it open non existent files for editing we add to the vimrc map gf :edit <cfile><cr> .","title":"Vim tricks: Using gf to open file below cursor and extending it with a new mapping"},{"location":"zet/20220111030349/","text":"forward a docker program to the display with -v If you want to have access to a containers GUI program you can forward it using -v /tmp/.X11-unix:/tmp/.X11-unix and -e DISPLAY=unix$DISPLAY These will forward the containers GUI to the host display. If it doesn't work we can run xhost local:root to make sure it is forwarding.","title":"forward a docker program to the display with -v"},{"location":"zet/20220111030349/#forward-a-docker-program-to-the-display-with-v","text":"If you want to have access to a containers GUI program you can forward it using -v /tmp/.X11-unix:/tmp/.X11-unix and -e DISPLAY=unix$DISPLAY These will forward the containers GUI to the host display. If it doesn't work we can run xhost local:root to make sure it is forwarding.","title":"forward a docker program to the display with -v"},{"location":"zet/20220112032139/","text":"Cleaning up docker images with awk and xargs I had a huge amount of docker images that had been accumulating during development. I decided to clean them up using a linux pipeline docker image ls | head -n 28 | awk '{ print 3 }' | xargs -l docker image rm -f This zet is basically a reminder that for many things a linux pipeline is the simplest solution. I basically searched all my docker images, got the top x number of images. In this case 28, for the iterations i went through in development. Sent this to awk to get the third row and sent the result of this to xargs, there is a little hiccup in this code as it tried to remove the images with the labels of the columns. But apart from that it worked well. #pipelines #docker","title":"Cleaning up docker images with awk and xargs"},{"location":"zet/20220112032139/#cleaning-up-docker-images-with-awk-and-xargs","text":"I had a huge amount of docker images that had been accumulating during development. I decided to clean them up using a linux pipeline docker image ls | head -n 28 | awk '{ print 3 }' | xargs -l docker image rm -f This zet is basically a reminder that for many things a linux pipeline is the simplest solution. I basically searched all my docker images, got the top x number of images. In this case 28, for the iterations i went through in development. Sent this to awk to get the third row and sent the result of this to xargs, there is a little hiccup in this code as it tried to remove the images with the labels of the columns. But apart from that it worked well. #pipelines #docker","title":"Cleaning up docker images with awk and xargs"},{"location":"zet/20220117033343/","text":"Rob had a zet about vim K If you use K in normal mode in vim it will search the man page for the command under the cursor. I need to configure this to work with python.","title":"Rob had a zet about vim `K`"},{"location":"zet/20220117033343/#rob-had-a-zet-about-vim-k","text":"If you use K in normal mode in vim it will search the man page for the command under the cursor. I need to configure this to work with python.","title":"Rob had a zet about vim K"},{"location":"zet/20220126232651/","text":"Creating a better homepage with github After reading rwxrobs zet on github design and looking at TechnoTims new design for his github front page i decided that i would work through creating a new home page for my github. What the page needs is to be simple and readable with links to my main resources available at the touch of a button. So following robs post i have decided to set up a couple of specific repositories for my github. dotfiles - My currently maintained dotfiles. zet - My zettlekasten repo learning-labs - My place for all my dabblings with code. Website - Github hosted blog. These repositories will be pinned and accessible in the about me section.","title":"Creating a better homepage with github"},{"location":"zet/20220126232651/#creating-a-better-homepage-with-github","text":"After reading rwxrobs zet on github design and looking at TechnoTims new design for his github front page i decided that i would work through creating a new home page for my github. What the page needs is to be simple and readable with links to my main resources available at the touch of a button. So following robs post i have decided to set up a couple of specific repositories for my github. dotfiles - My currently maintained dotfiles. zet - My zettlekasten repo learning-labs - My place for all my dabblings with code. Website - Github hosted blog. These repositories will be pinned and accessible in the about me section.","title":"Creating a better homepage with github"},{"location":"zet/20220127200825/","text":"Ensuring docker reruns updated code Docker uses the cached images each time you rerun a build. IF you have changed the underlying shell script or other script docker doesn't know this and will use the cached version instead. To overcome this we can do a couple of different things. The first is to use --no-cached which will force a rebuild from the start. This is frustrating as it removes all the basic scripting that you would do in the container and forces the system to rerun apt update etc. The second setup is to encapsulate your changing script with a run script that sources the host file. This will ensure the latest version is copied into the container at build time. The final design that i use most frequently is a no-op command. This can be commented in and out at each new build which forces the build system to rerun on each build from the no-op point on. The simplest no-op is RUN : .","title":"Ensuring docker reruns updated code"},{"location":"zet/20220127200825/#ensuring-docker-reruns-updated-code","text":"Docker uses the cached images each time you rerun a build. IF you have changed the underlying shell script or other script docker doesn't know this and will use the cached version instead. To overcome this we can do a couple of different things. The first is to use --no-cached which will force a rebuild from the start. This is frustrating as it removes all the basic scripting that you would do in the container and forces the system to rerun apt update etc. The second setup is to encapsulate your changing script with a run script that sources the host file. This will ensure the latest version is copied into the container at build time. The final design that i use most frequently is a no-op command. This can be commented in and out at each new build which forces the build system to rerun on each build from the no-op point on. The simplest no-op is RUN : .","title":"Ensuring docker reruns updated code"},{"location":"zet/20220127205826/","text":"Running github actions locally with act There is a neat tool to run your github actions locally to test the development pipeline before committing it to github. The package is called act (https://github.com/nektos/act) and provides a cli to quickly check your github actions as you develop.","title":"Running github actions locally with act"},{"location":"zet/20220127205826/#running-github-actions-locally-with-act","text":"There is a neat tool to run your github actions locally to test the development pipeline before committing it to github. The package is called act (https://github.com/nektos/act) and provides a cli to quickly check your github actions as you develop.","title":"Running github actions locally with act"},{"location":"zet/20220217101827/","text":"# Use tree to show a structured view of a folder. Simple command to get a structured tree of the files in the directory.","title":"Use tree to show a structured view of a folder."},{"location":"zet/20220313001804/","text":"Use sudo update-alternatives --config editor to change default editor Quick note to say always set up the computer with this config to change default editor to vim.","title":"Use `sudo update-alternatives --config editor` to change default"},{"location":"zet/20220313001804/#use-sudo-update-alternatives-config-editor-to-change-default","text":"editor Quick note to say always set up the computer with this config to change default editor to vim.","title":"Use sudo update-alternatives --config editor to change default"},{"location":"zet/20220319013625/","text":"Use newgrp docker to remove the need to restart computer When updating usermod, many examples tell you to restart the computer for the changes to take effect. If you don't want to do that you can force an update by changing your user to the group by running `newgrp ` allowing you to run the commands. Ultimately you want to restart to get the configurations working but to initially run tests just use the `newgrp` command.","title":"Use `newgrp docker` to remove the need to restart computer"},{"location":"zet/20220319013625/#use-newgrp-docker-to-remove-the-need-to-restart-computer","text":"When updating usermod, many examples tell you to restart the computer for the changes to take effect. If you don't want to do that you can force an update by changing your user to the group by running `newgrp ` allowing you to run the commands. Ultimately you want to restart to get the configurations working but to initially run tests just use the `newgrp` command.","title":"Use newgrp docker to remove the need to restart computer"},{"location":"zet/20220320105415/","text":"Use run-parts with a folder to run all the iterable files When you have multiple run scripts such as 00- 10- 20- 35- use the run-parts with the folder and you can run all the scripts inside the folder in iterable order. run-parts /etc/update-motd.d/ Will run all of the below scripts. /etc/update-motd.d//00-header /etc/update-motd.d//10-help-text /etc/update-motd.d//50-motd-news /etc/update-motd.d//85-fwupd /etc/update-motd.d//88-esm-announce /etc/update-motd.d//90-updates-available /etc/update-motd.d//91-contract-ua-esm-status /etc/update-motd.d//91-release-upgrade /etc/update-motd.d//92-unattended-upgrades /etc/update-motd.d//95-hwe-eol /etc/update-motd.d//98-fsck-at-reboot /etc/update-motd.d//98-reboot-required","title":"Use run-parts with a folder to run all the iterable files"},{"location":"zet/20220320105415/#use-run-parts-with-a-folder-to-run-all-the-iterable-files","text":"When you have multiple run scripts such as 00- 10- 20- 35- use the run-parts with the folder and you can run all the scripts inside the folder in iterable order. run-parts /etc/update-motd.d/ Will run all of the below scripts. /etc/update-motd.d//00-header /etc/update-motd.d//10-help-text /etc/update-motd.d//50-motd-news /etc/update-motd.d//85-fwupd /etc/update-motd.d//88-esm-announce /etc/update-motd.d//90-updates-available /etc/update-motd.d//91-contract-ua-esm-status /etc/update-motd.d//91-release-upgrade /etc/update-motd.d//92-unattended-upgrades /etc/update-motd.d//95-hwe-eol /etc/update-motd.d//98-fsck-at-reboot /etc/update-motd.d//98-reboot-required","title":"Use run-parts with a folder to run all the iterable files"},{"location":"zet/20220327203445/","text":"Default configurations for ufw It is always useful to configure ufw upon a new system install. Below outlines the quick configuration steps for the system. To begin we install ufw. Once installed we enable and start ufw. This is seperate to the ufw daemon which is started with systemctl. The start commands are sudo ufw enable sudo ufw start Next we setup the systemd configuration with: sudo systemctl enable --now ufw The basic configurations that we set are to allow ssh and http/https with sudo ufw allow ssh sudo ufw allow proto tcp from any to any port 80,443 There are many other things we can allow or deny in this system but this is the bare necessities when configuring ufw.","title":"Default configurations for ufw"},{"location":"zet/20220327203445/#default-configurations-for-ufw","text":"It is always useful to configure ufw upon a new system install. Below outlines the quick configuration steps for the system. To begin we install ufw. Once installed we enable and start ufw. This is seperate to the ufw daemon which is started with systemctl. The start commands are sudo ufw enable sudo ufw start Next we setup the systemd configuration with: sudo systemctl enable --now ufw The basic configurations that we set are to allow ssh and http/https with sudo ufw allow ssh sudo ufw allow proto tcp from any to any port 80,443 There are many other things we can allow or deny in this system but this is the bare necessities when configuring ufw.","title":"Default configurations for ufw"},{"location":"zet/20220329022928/","text":"Having multiple git accounts and pushing code When using multiple git accounts the best thing to do is create individual ssh keys for each account and on a per repository basis set the git config --local configuration of ssh to the required key. git config core.sshCommand \"ssh -i ~/.ssh/id_rsa_example -F /dev/null\"","title":"Having multiple git accounts and pushing code"},{"location":"zet/20220329022928/#having-multiple-git-accounts-and-pushing-code","text":"When using multiple git accounts the best thing to do is create individual ssh keys for each account and on a per repository basis set the git config --local configuration of ssh to the required key. git config core.sshCommand \"ssh -i ~/.ssh/id_rsa_example -F /dev/null\"","title":"Having multiple git accounts and pushing code"},{"location":"zet/20220329114125/","text":"editing a remote file directly in local vim Found out you can edit a file on a remote host using vim with vim scp://user@host//etc/fstab where //etc/fstab is the remote file. This is very useful to work with programs on a remote system.","title":"editing a remote file directly in local vim"},{"location":"zet/20220329114125/#editing-a-remote-file-directly-in-local-vim","text":"Found out you can edit a file on a remote host using vim with vim scp://user@host//etc/fstab where //etc/fstab is the remote file. This is very useful to work with programs on a remote system.","title":"editing a remote file directly in local vim"},{"location":"zet/20220419050313/","text":"Using ansible-vault encrypt is the simplest way to configure encryption When working with ansible individual files can be enrypted using ansible-vault encrypt foo.yml bar.yml baz.yml This is a simple way to configure encrypted files to work with an ansible setup","title":"Using ansible-vault encrypt is the simplest way to configure"},{"location":"zet/20220419050313/#using-ansible-vault-encrypt-is-the-simplest-way-to-configure","text":"encryption When working with ansible individual files can be enrypted using ansible-vault encrypt foo.yml bar.yml baz.yml This is a simple way to configure encrypted files to work with an ansible setup","title":"Using ansible-vault encrypt is the simplest way to configure"},{"location":"zet/20220429131823/","text":"Using Ansible block function allows grouping tasks In Ansible 2.0 and above you can join tasks together with a block level, allowing multiple tasks to be run based on a conditional. For example running tasks: - block: - debug: msg='Task 1' - debug: msg='Task 2' - debug: msg='Task 3' when: ansible_distribution == 'Ubuntu' This means the tasks only run if the system is ubuntu","title":"Using Ansible block function allows grouping tasks"},{"location":"zet/20220429131823/#using-ansible-block-function-allows-grouping-tasks","text":"In Ansible 2.0 and above you can join tasks together with a block level, allowing multiple tasks to be run based on a conditional. For example running tasks: - block: - debug: msg='Task 1' - debug: msg='Task 2' - debug: msg='Task 3' when: ansible_distribution == 'Ubuntu' This means the tasks only run if the system is ubuntu","title":"Using Ansible block function allows grouping tasks"},{"location":"zet/20220429132715/","text":"Use --start-at-task and --step to move through playbooks in dev Often i have needed to rerun only a certain part of a playbook during development. Whilst Ansible is Idempotent and wont change anything that doesnt need changing some of my plays or tasks take a while. I found if i need to start at a specific task i can use the --start-at-task=\"task name\" to start at a specific point. Further to this i can use the --step flag to step through the playbook. This is interactive and so i can skip plays and tasks that i don't need to do.","title":"Use --start-at-task and --step to move through playbooks in dev"},{"location":"zet/20220429132715/#use-start-at-task-and-step-to-move-through-playbooks-in-dev","text":"Often i have needed to rerun only a certain part of a playbook during development. Whilst Ansible is Idempotent and wont change anything that doesnt need changing some of my plays or tasks take a while. I found if i need to start at a specific task i can use the --start-at-task=\"task name\" to start at a specific point. Further to this i can use the --step flag to step through the playbook. This is interactive and so i can skip plays and tasks that i don't need to do.","title":"Use --start-at-task and --step to move through playbooks in dev"},{"location":"zet/20220502132007/","text":"Quick tutorial on chroot to fix a broken install The first thing to do is configure the drive to be mounted in the live disk or host computer. This may be through numerous methods. If there is no encryption you can just mount /dev/<deviceID> into a folder like /mnt/faulty_drive . If there is encryption on the drive you first must decrypt the device, this is done using cryptsetup open followed by the device and an identifier such as cryptsetup open /dev/sda ubuntu Once this is donw you can mount the ubuntu to your /mnt location using mount /dev/mapper/ubuntu /mnt/faulty_drive . If you are also using lvm it becomes a little more difficult and you need to have lvm2 installed. From there run.... Once the drive is mounted. Bind mount the dev folder and mount proc, pts and sysfs with mount -t proc proc /rescue/proc mount -t sysfs sys /rescue/sys mount -o bind /dev /rescue/dev mount -t devpts pts /rescue/dev/pts This allows the system to interact with the processor correctly. Next we make a backup of the /etc/resolv.conf file and create a newfile with echo \"nameserver 4.2.2.2\" > /etc/resolv.conf This will provide dns resolution allowing you to connect to the internet in the chroot. Finally you are able to chroot into the drive and run commands as if you were in the system.","title":"Quick tutorial on chroot to fix a broken install"},{"location":"zet/20220502132007/#quick-tutorial-on-chroot-to-fix-a-broken-install","text":"The first thing to do is configure the drive to be mounted in the live disk or host computer. This may be through numerous methods. If there is no encryption you can just mount /dev/<deviceID> into a folder like /mnt/faulty_drive . If there is encryption on the drive you first must decrypt the device, this is done using cryptsetup open followed by the device and an identifier such as cryptsetup open /dev/sda ubuntu Once this is donw you can mount the ubuntu to your /mnt location using mount /dev/mapper/ubuntu /mnt/faulty_drive . If you are also using lvm it becomes a little more difficult and you need to have lvm2 installed. From there run.... Once the drive is mounted. Bind mount the dev folder and mount proc, pts and sysfs with mount -t proc proc /rescue/proc mount -t sysfs sys /rescue/sys mount -o bind /dev /rescue/dev mount -t devpts pts /rescue/dev/pts This allows the system to interact with the processor correctly. Next we make a backup of the /etc/resolv.conf file and create a newfile with echo \"nameserver 4.2.2.2\" > /etc/resolv.conf This will provide dns resolution allowing you to connect to the internet in the chroot. Finally you are able to chroot into the drive and run commands as if you were in the system.","title":"Quick tutorial on chroot to fix a broken install"},{"location":"zet/20220511124942/","text":"Interesting tool for getting only an incremental update of ISO While looking into QEMU and QuickEMU tool. A youtube video utilised zsync to bring only the changed bits of an iso across. This simple tool is a great idea for using iso's when testing images.","title":"Interesting tool for getting only an incremental update of ISO"},{"location":"zet/20220511124942/#interesting-tool-for-getting-only-an-incremental-update-of-iso","text":"While looking into QEMU and QuickEMU tool. A youtube video utilised zsync to bring only the changed bits of an iso across. This simple tool is a great idea for using iso's when testing images.","title":"Interesting tool for getting only an incremental update of ISO"},{"location":"zet/20220512092746/","text":"New vim commands to remember Moving inside a long wrapped line use gj and gk Copy to system clipboard use \"+y","title":"New vim commands to remember"},{"location":"zet/20220512092746/#new-vim-commands-to-remember","text":"Moving inside a long wrapped line use gj and gk Copy to system clipboard use \"+y","title":"New vim commands to remember"},{"location":"zet/20220603120853/","text":"Check out rclone for cloud storage backup tool This tool is very similar to rsync but provides configurations for uploading to a cloud storage. This might be useful for when we are sending data to an S3 bucket.","title":"Check out rclone for cloud storage backup tool"},{"location":"zet/20220603120853/#check-out-rclone-for-cloud-storage-backup-tool","text":"This tool is very similar to rsync but provides configurations for uploading to a cloud storage. This might be useful for when we are sending data to an S3 bucket.","title":"Check out rclone for cloud storage backup tool"},{"location":"zet/20220603131741/","text":"Useful python profile tool chain When trying to figure out where i was spending the most time in my code today i was running simple timeits all over the code. Looking at mcoding on youtube tonight i found a useful design that may allow me to get better insights into the code. The first is to run the basic python profiler which is called with importing cProfile. Then using a context manager we run the code block that we want to analyse. Once done the code outputs statistics that we can view with pstats by importing pstats and sorting the data by TIME. import cProfile import pstats with cProfile.Profile() as pr: cool_func() stats = pstats.Stats(pr) stats.sort_stats(pstats.SortKey.TIME) stats.print_stats() We can then dump this data to a file and use a super useful web based ui to view this data, called snakeviz. This will help improve many of my coding issues.","title":"Useful python profile tool chain"},{"location":"zet/20220603131741/#useful-python-profile-tool-chain","text":"When trying to figure out where i was spending the most time in my code today i was running simple timeits all over the code. Looking at mcoding on youtube tonight i found a useful design that may allow me to get better insights into the code. The first is to run the basic python profiler which is called with importing cProfile. Then using a context manager we run the code block that we want to analyse. Once done the code outputs statistics that we can view with pstats by importing pstats and sorting the data by TIME. import cProfile import pstats with cProfile.Profile() as pr: cool_func() stats = pstats.Stats(pr) stats.sort_stats(pstats.SortKey.TIME) stats.print_stats() We can then dump this data to a file and use a super useful web based ui to view this data, called snakeviz. This will help improve many of my coding issues.","title":"Useful python profile tool chain"},{"location":"zet/20220620021735/","text":"Use paramiko if you need to run ssh in python paramiko has an excellent interface to connect to a client through ssh interface. This is useful if you want to programmatically communicate with a server and run a program remotely.","title":"Use paramiko if you need to run ssh in python"},{"location":"zet/20220620021735/#use-paramiko-if-you-need-to-run-ssh-in-python","text":"paramiko has an excellent interface to connect to a client through ssh interface. This is useful if you want to programmatically communicate with a server and run a program remotely.","title":"Use paramiko if you need to run ssh in python"},{"location":"zet/20220620133522/","text":"Interfacing plex with tailscale in docker I have often wanted to configure plex to be accessible everywhere but not having to worry about port forwarding on the router and dealing with all those issues. I found i often lost the network connection or something failed on me. I then moved to using plex in a container to remove the issues i had with installing plex locally. Once this was configured i needed a way to configure it for use everywhere. I found that if i run a tailscale node in the plex docker-compose i am able to connect to the server remotely. This design is super simple and will allow me to play around with having a dedicated code server or hosted service connected to the tailscale service.","title":"Interfacing plex with tailscale in docker"},{"location":"zet/20220620133522/#interfacing-plex-with-tailscale-in-docker","text":"I have often wanted to configure plex to be accessible everywhere but not having to worry about port forwarding on the router and dealing with all those issues. I found i often lost the network connection or something failed on me. I then moved to using plex in a container to remove the issues i had with installing plex locally. Once this was configured i needed a way to configure it for use everywhere. I found that if i run a tailscale node in the plex docker-compose i am able to connect to the server remotely. This design is super simple and will allow me to play around with having a dedicated code server or hosted service connected to the tailscale service.","title":"Interfacing plex with tailscale in docker"},{"location":"zet/20220717073209/","text":"Extracting app images and moving the Application to /usr/bin Looking at nvim installation they suggest running an app image. Im not a fan of app images i prefer the package to be installable via apt. If however there is no apt candidate i will use appimages over snap packages. I found out today nvim has a neat setup to extract the appimage and mv it somewhere usable and create a symlink in the user path. This way i have a full application being called with the standard command. For example, nvim works directly from the terminal in all locations. - nvim example ./nvim.appimage --appimage-extract ./squashfs-root/AppRun --version # Optional: exposing nvim globally. sudo mv squashfs-root / sudo ln -s /squashfs-root/AppRun /usr/bin/nvim nvim This could probably also be done with a direct ln of the appimage but i like this solution better.","title":"Extracting app images and moving the Application to /usr/bin"},{"location":"zet/20220717073209/#extracting-app-images-and-moving-the-application-to-usrbin","text":"Looking at nvim installation they suggest running an app image. Im not a fan of app images i prefer the package to be installable via apt. If however there is no apt candidate i will use appimages over snap packages. I found out today nvim has a neat setup to extract the appimage and mv it somewhere usable and create a symlink in the user path. This way i have a full application being called with the standard command. For example, nvim works directly from the terminal in all locations. - nvim example ./nvim.appimage --appimage-extract ./squashfs-root/AppRun --version # Optional: exposing nvim globally. sudo mv squashfs-root / sudo ln -s /squashfs-root/AppRun /usr/bin/nvim nvim This could probably also be done with a direct ln of the appimage but i like this solution better.","title":"Extracting app images and moving the Application to /usr/bin"},{"location":"zet/20220813111811/","text":"rich inspect module is amazing I just found out that rich has an inspect module which will pretty print all the methods available for a module when you are running the repl. Whats great here is that it provides docstrings for all of the methods.","title":"rich inspect module is amazing"},{"location":"zet/20220813111811/#rich-inspect-module-is-amazing","text":"I just found out that rich has an inspect module which will pretty print all the methods available for a module when you are running the repl. Whats great here is that it provides docstrings for all of the methods.","title":"rich inspect module is amazing"},{"location":"zet/20220813111934/","text":"Using libtmux to build tmux sessions I have always wanted a simpler way to implement tmux sessions for long running code or for running deep learning training sessions. I have found that i can use tmux in python and script up all the commands. This is a game changer.","title":"Using libtmux to build tmux sessions"},{"location":"zet/20220813111934/#using-libtmux-to-build-tmux-sessions","text":"I have always wanted a simpler way to implement tmux sessions for long running code or for running deep learning training sessions. I have found that i can use tmux in python and script up all the commands. This is a game changer.","title":"Using libtmux to build tmux sessions"},{"location":"zet/20220813112106/","text":"The power of Typer I have been using typer for many things recently and am amazed with it's capabilities and what i am able to do with it. Whilst on the face typer just wraps click commands and makes it pretty, i have found that i can use typer to test functions with a simple wrapping of the app decorator. This allows me to test functionality on the fly of a written function without having to copy into a repl or use jupyter. Further to this i have found that typer becomes an excellent tool for creating simple usable system checking tools. This is useful when i have a suite of tests that are functional and not easily automated in a real world system i am able to quickly create a testing environment to for example check the operation of a camera or a sensor. There is so much that i can do with typer that i need to delve into further.","title":"The power of Typer"},{"location":"zet/20220813112106/#the-power-of-typer","text":"I have been using typer for many things recently and am amazed with it's capabilities and what i am able to do with it. Whilst on the face typer just wraps click commands and makes it pretty, i have found that i can use typer to test functions with a simple wrapping of the app decorator. This allows me to test functionality on the fly of a written function without having to copy into a repl or use jupyter. Further to this i have found that typer becomes an excellent tool for creating simple usable system checking tools. This is useful when i have a suite of tests that are functional and not easily automated in a real world system i am able to quickly create a testing environment to for example check the operation of a camera or a sensor. There is so much that i can do with typer that i need to delve into further.","title":"The power of Typer"},{"location":"zet/20220813112523/","text":"Simple terminal tools for python I have been using the simple-term-menu library recently to add user interaction to pipeline tools. The simplicity of this tool to view and create lists of folders or files is excellent and provides a great way to quickly create terminal based ui's for applications.","title":"Simple terminal tools for python"},{"location":"zet/20220813112523/#simple-terminal-tools-for-python","text":"I have been using the simple-term-menu library recently to add user interaction to pipeline tools. The simplicity of this tool to view and create lists of folders or files is excellent and provides a great way to quickly create terminal based ui's for applications.","title":"Simple terminal tools for python"},{"location":"zet/20220820043840/","text":"Getting started with FARM (FastAPI, React, MongoDB) I love the capability of fastAPI and so i am going to look at building a farm stack. This stack uses MongoDB as the database portion. React for the frontend and FastAPI for the backend.","title":"Getting started with FARM (FastAPI, React, MongoDB)"},{"location":"zet/20220820043840/#getting-started-with-farm-fastapi-react-mongodb","text":"I love the capability of fastAPI and so i am going to look at building a farm stack. This stack uses MongoDB as the database portion. React for the frontend and FastAPI for the backend.","title":"Getting started with FARM (FastAPI, React, MongoDB)"},{"location":"zet/20220824013036/","text":"testing delete","title":"testing delete"},{"location":"zet/20220824013036/#testing-delete","text":"","title":"testing delete"},{"location":"zet/20220827112635/","text":"Simple quick way to remove .env from git history The other night i did it. I accidently pushed an env file to github and i was trying to figure out the best way to remove the file without leaving anything in the commit history. I finally found this simple solution. git filter-branch --index-filter \"git rm -rf --cached --ignore-unmatch .env\" HEAD","title":"Simple quick way to remove .env from git history"},{"location":"zet/20220827112635/#simple-quick-way-to-remove-env-from-git-history","text":"The other night i did it. I accidently pushed an env file to github and i was trying to figure out the best way to remove the file without leaving anything in the commit history. I finally found this simple solution. git filter-branch --index-filter \"git rm -rf --cached --ignore-unmatch .env\" HEAD","title":"Simple quick way to remove .env from git history"},{"location":"zet/20220827114114/","text":"Using !! on cli with string replacements Often times i am running a command that either i need to modify or i have made a mistake and want to rerun. To fix this i can either go back up in the history and modify the command or i can use a useful trick of adding substitution or replacement with the !! and :s/<item>/<replacement> command. For example i can change systemctl start to status with !!:s/start/status and i can rerun with status instead of start whilst it isn't as easy as tapping up and using the arrow key it is probably a nicer way to change things.","title":"Using !! on cli with string replacements"},{"location":"zet/20220827114114/#using-on-cli-with-string-replacements","text":"Often times i am running a command that either i need to modify or i have made a mistake and want to rerun. To fix this i can either go back up in the history and modify the command or i can use a useful trick of adding substitution or replacement with the !! and :s/<item>/<replacement> command. For example i can change systemctl start to status with !!:s/start/status and i can rerun with status instead of start whilst it isn't as easy as tapping up and using the arrow key it is probably a nicer way to change things.","title":"Using !! on cli with string replacements"},{"location":"zet/20220827132613/","text":"Use sudo install as a way of installing binaries. I found today that you can use the install command to copy files into bin directories, whilst it is no different from the cp command it looks nicer when you are producing a tool for someone.","title":"Use sudo install <file> as a way of installing binaries."},{"location":"zet/20220827132613/#use-sudo-install-as-a-way-of-installing-binaries","text":"I found today that you can use the install command to copy files into bin directories, whilst it is no different from the cp command it looks nicer when you are producing a tool for someone.","title":"Use sudo install  as a way of installing binaries."},{"location":"zet/20220919121213/","text":"Useful tool in Go I like the functionality in Go of giving the return types and names at the top of a small function and not having to give the values at the end. This implicit output gives conciseness to the code block. (Only works in small code functions.","title":"Useful tool in Go"},{"location":"zet/20220919121213/#useful-tool-in-go","text":"I like the functionality in Go of giving the return types and names at the top of a small function and not having to give the values at the end. This implicit output gives conciseness to the code block. (Only works in small code functions.","title":"Useful tool in Go"},{"location":"zet/20220920110247/","text":"Why has it taken so long to figure out git restore I just found out that i can run git restore to remove staged items from the commit. The previous way i was doing it was removing the cached items and recommitting everything else. I will be adding this one into my tooling now.","title":"Why has it taken so long to figure out git restore"},{"location":"zet/20220920110247/#why-has-it-taken-so-long-to-figure-out-git-restore","text":"I just found out that i can run git restore to remove staged items from the commit. The previous way i was doing it was removing the cached items and recommitting everything else. I will be adding this one into my tooling now.","title":"Why has it taken so long to figure out git restore"},{"location":"zet/20220925060111/","text":"Some awesome Go CLI tools I found some awesome Go CLI tools. The first of these is great for creating pretty cli. * https://github.com/charmbracelet/gum The charmbracelet repo has many great repositories including * https://github.com/charmbracelet/glow -- pretty markdown in the cli * https://github.com/charmbracelet/soft-serve -- git server * https://github.com/charmbracelet/skate -- a key value storer Another tool i found for secure file sharing * https://github.com/SpatiumPortae/portal","title":"Some awesome Go CLI tools"},{"location":"zet/20220925060111/#some-awesome-go-cli-tools","text":"I found some awesome Go CLI tools. The first of these is great for creating pretty cli. * https://github.com/charmbracelet/gum The charmbracelet repo has many great repositories including * https://github.com/charmbracelet/glow -- pretty markdown in the cli * https://github.com/charmbracelet/soft-serve -- git server * https://github.com/charmbracelet/skate -- a key value storer Another tool i found for secure file sharing * https://github.com/SpatiumPortae/portal","title":"Some awesome Go CLI tools"},{"location":"zet/20220925061557/","text":"Use lipgloss for go tui styling A simple tool for creating prettier cli tools in Go. https://github.com/charmbracelet/lipgloss","title":"Use lipgloss for go tui styling"},{"location":"zet/20220925061557/#use-lipgloss-for-go-tui-styling","text":"A simple tool for creating prettier cli tools in Go. https://github.com/charmbracelet/lipgloss","title":"Use lipgloss for go tui styling"},{"location":"zet/20220929224751/","text":"Learning about rauc files https://buildmedia.readthedocs.org/media/pdf/rauc/latest/rauc.pdf Rauc should be installed on SSD or other such devices rather than raw NAND. Rauc can update just the file system content and leave the rootfs untouched. Rauc can work with GRUB, Barebox, U-boot etc it is an application layer but provides an interface to these bootloaders.","title":"Learning about rauc files"},{"location":"zet/20220929224751/#learning-about-rauc-files","text":"https://buildmedia.readthedocs.org/media/pdf/rauc/latest/rauc.pdf Rauc should be installed on SSD or other such devices rather than raw NAND. Rauc can update just the file system content and leave the rootfs untouched. Rauc can work with GRUB, Barebox, U-boot etc it is an application layer but provides an interface to these bootloaders.","title":"Learning about rauc files"},{"location":"zet/20221001114802/","text":"Use ithub.com/pkg/errors to wrap errors up the call stack Often errors are lost in Go as the error is propagated up the call stack using errors.Wrapf one can add context to the error being wrapped to provide information at a later point in the code when errors are handled.","title":"Use ithub.com/pkg/errors to wrap errors up the call stack"},{"location":"zet/20221001114802/#use-ithubcompkgerrors-to-wrap-errors-up-the-call-stack","text":"Often errors are lost in Go as the error is propagated up the call stack using errors.Wrapf one can add context to the error being wrapped to provide information at a later point in the code when errors are handled.","title":"Use ithub.com/pkg/errors to wrap errors up the call stack"},{"location":"zet/20221010070312/","text":"Test file This is a test code","title":"Test file"},{"location":"zet/20221010070312/#test-file","text":"This is a test code","title":"Test file"},{"location":"zet/20221011035050/","text":"Using netstat to find port usage To quickly identify the ports that are in use we can use netstat --tcpn to get a quick overview of running processes related to the port. The tcp flag allows the system to search only tcp ports whilst udp can be called with -u/--udp . We can further constrain the command with -l for listening devices, and -p for showing the PID of the program. From here we can grep for our port and undertake further processing.","title":"Using netstat to find port usage"},{"location":"zet/20221011035050/#using-netstat-to-find-port-usage","text":"To quickly identify the ports that are in use we can use netstat --tcpn to get a quick overview of running processes related to the port. The tcp flag allows the system to search only tcp ports whilst udp can be called with -u/--udp . We can further constrain the command with -l for listening devices, and -p for showing the PID of the program. From here we can grep for our port and undertake further processing.","title":"Using netstat to find port usage"},{"location":"zet/20221024224210/","text":"vim shortcuts vip Use vip which will visual mode the whole paragraph. We can then run :norm and add stuff to the paragraph. Useful for commentingout code blocks global :g/^@/m50 move all lines starting with @ to 50 lines ahead earlier Earlier allows you to go back to earlier points in time. You can use the command :earlier and :later to skip back and forward through time.","title":"vim shortcuts"},{"location":"zet/20221024224210/#vim-shortcuts","text":"","title":"vim shortcuts"},{"location":"zet/20221024224210/#vip","text":"Use vip which will visual mode the whole paragraph. We can then run :norm and add stuff to the paragraph. Useful for commentingout code blocks","title":"vip"},{"location":"zet/20221024224210/#global","text":":g/^@/m50 move all lines starting with @ to 50 lines ahead","title":"global"},{"location":"zet/20221024224210/#earlier","text":"Earlier allows you to go back to earlier points in time. You can use the command :earlier and :later to skip back and forward through time.","title":"earlier"},{"location":"zet/20221102093334/","text":"gRPC and protobuf - the world of microservices Recently i have been looking into gRPC and protobuf, learning about how and why they are used. Below are some notes i have collated on this topic. gRPC - gRPC Remote Procedure Call. Protobuf - protocol buffer gRPC can use protocol buffers as its interface language and its message interchange format. The idea of the gRPC is that a client can make a call to a servers method directly on a different machine as if it were a local object. The goal is to define a service specifying the methods that can be called with their parameters and return types. The client has a stub that provides the same methods as the server. gRPC can use json, but defaults to protobuf. The protocol is defined in a .proto file. It is structured as messages. Where each message is a logical record of information containing key value pairs called fields. Once defined you use protoc to generate data access classes. Including field access and serialisers for the data. Core Concepts Based around the idea of services. Defining methods that can be called remotely. We can use four service types. - Unary - client and servre send single message response. * Server streamer - where client requests and server streams response. * Client streamer - where the client writes sequence of messages and server sends a response. * Bidirectional - Both streaming. Users generate client and server side code and typically call the API on the client side and implement the server side. gRPC uses DEADLINE_EXCEEDED error for timeouts.","title":"gRPC and protobuf - the world of microservices"},{"location":"zet/20221102093334/#grpc-and-protobuf-the-world-of-microservices","text":"Recently i have been looking into gRPC and protobuf, learning about how and why they are used. Below are some notes i have collated on this topic. gRPC - gRPC Remote Procedure Call. Protobuf - protocol buffer gRPC can use protocol buffers as its interface language and its message interchange format. The idea of the gRPC is that a client can make a call to a servers method directly on a different machine as if it were a local object. The goal is to define a service specifying the methods that can be called with their parameters and return types. The client has a stub that provides the same methods as the server. gRPC can use json, but defaults to protobuf. The protocol is defined in a .proto file. It is structured as messages. Where each message is a logical record of information containing key value pairs called fields. Once defined you use protoc to generate data access classes. Including field access and serialisers for the data.","title":"gRPC and protobuf - the world of microservices"},{"location":"zet/20221102093334/#core-concepts","text":"Based around the idea of services. Defining methods that can be called remotely. We can use four service types. - Unary - client and servre send single message response. * Server streamer - where client requests and server streams response. * Client streamer - where the client writes sequence of messages and server sends a response. * Bidirectional - Both streaming. Users generate client and server side code and typically call the API on the client side and implement the server side. gRPC uses DEADLINE_EXCEEDED error for timeouts.","title":"Core Concepts"},{"location":"zet/20221130111603/","text":"Working with os/exec in Go One of the interesting things about Go is in some situations we have output which is counter to my understanding in python. For example running the ping command in python i would run the command with subprocess.Run or similar and then i would parse the result. In Go however if i want to view the result of the command that was run i need to attach the Stdout and stdErr to variables. This makes sense once you think about it but the logical code looks different. For example pingExecutable, _ := exec.LookPath(\"ping\") pingCommand := exec.Command(pingExecutable, ip, \"-c\", \"1\", \"-w\", \"3\") out, _ := pingCommand.CombinedOutput() pingCommand.Run() Runs the ping command on an ip address but the CombinedOutput command is attachhing to stdout and stderr.","title":"Working with os/exec in Go"},{"location":"zet/20221130111603/#working-with-osexec-in-go","text":"One of the interesting things about Go is in some situations we have output which is counter to my understanding in python. For example running the ping command in python i would run the command with subprocess.Run or similar and then i would parse the result. In Go however if i want to view the result of the command that was run i need to attach the Stdout and stdErr to variables. This makes sense once you think about it but the logical code looks different. For example pingExecutable, _ := exec.LookPath(\"ping\") pingCommand := exec.Command(pingExecutable, ip, \"-c\", \"1\", \"-w\", \"3\") out, _ := pingCommand.CombinedOutput() pingCommand.Run() Runs the ping command on an ip address but the CombinedOutput command is attachhing to stdout and stderr.","title":"Working with os/exec in Go"},{"location":"zet/20221211111835/","text":"Go Concurrency Waitgroups primitives or channels A waitgroup is part of sync package, starting point where goroutines start and a point further along that waits on the completion of goroutines. To add to a waitgroup wg.Add() Don't make deadlocks, this can be fixed by making sure we have the same amount of Adds as Done. If we have too many Dones we cause go to panic. waitgroups should always pass as a pointer rather than a value.","title":"Go Concurrency"},{"location":"zet/20221211111835/#go-concurrency","text":"Waitgroups primitives or channels A waitgroup is part of sync package, starting point where goroutines start and a point further along that waits on the completion of goroutines. To add to a waitgroup wg.Add() Don't make deadlocks, this can be fixed by making sure we have the same amount of Adds as Done. If we have too many Dones we cause go to panic. waitgroups should always pass as a pointer rather than a value.","title":"Go Concurrency"},{"location":"zet/20221211113110/","text":"Anonymous functions in Go The syntax for anonymous functions in go is interesting. The main idea of the anonymous function is to run a function as a closure or part of a goroutine. The syntax of a basic anonymous function is: func(){ //Do something }() We can also assign a function to a variable which doesn't include the () at the end. We can then call the variable later with the () at the end. We are also able to pass variables into the function.","title":"Anonymous functions in Go"},{"location":"zet/20221211113110/#anonymous-functions-in-go","text":"The syntax for anonymous functions in go is interesting. The main idea of the anonymous function is to run a function as a closure or part of a goroutine. The syntax of a basic anonymous function is: func(){ //Do something }() We can also assign a function to a variable which doesn't include the () at the end. We can then call the variable later with the () at the end. We are also able to pass variables into the function.","title":"Anonymous functions in Go"},{"location":"zet/20221212110758/","text":"Some useful makefile configurations Using a cookiecutter go config today i found an excellent makefile with some useful configurations. The most useful of these is configuring the makefile to have a help setup. This is done with two commands. The first means the help is the default call when make is run without any subcommand. The command is .DEFAULT_GOAL := help The second is to actually display help by outputting the results of each commands comment string. A useful config is to have double # to distinguish from normal comments and use the below setup .PHONY: help ## `help`: Generates this help dialog for the Makefile help: Makefile echo echo \" Commands available in \\`\"$(PROJECTNAME)\"\\`:\" echo sed -n 's/^[ \\t]*##//p' $< | column -t -s ':' | sed -e 's/^//' echo The other two cool configurations are for undertaking linting which can be done with .PHONY: codestyle ## : ## `codestyle`: Run code formatter(s) codestyle: golangci-lint run --fix .PHONY: lint ## `lint`: Run linters and check code-style lint: golangci-lint run","title":"Some useful makefile configurations"},{"location":"zet/20221212110758/#some-useful-makefile-configurations","text":"Using a cookiecutter go config today i found an excellent makefile with some useful configurations. The most useful of these is configuring the makefile to have a help setup. This is done with two commands. The first means the help is the default call when make is run without any subcommand. The command is .DEFAULT_GOAL := help The second is to actually display help by outputting the results of each commands comment string. A useful config is to have double # to distinguish from normal comments and use the below setup .PHONY: help ## `help`: Generates this help dialog for the Makefile help: Makefile echo echo \" Commands available in \\`\"$(PROJECTNAME)\"\\`:\" echo sed -n 's/^[ \\t]*##//p' $< | column -t -s ':' | sed -e 's/^//' echo The other two cool configurations are for undertaking linting which can be done with .PHONY: codestyle ## : ## `codestyle`: Run code formatter(s) codestyle: golangci-lint run --fix .PHONY: lint ## `lint`: Run linters and check code-style lint: golangci-lint run","title":"Some useful makefile configurations"},{"location":"zet/20221212114728/","text":"Useful ansible guard for playbooks Sometimes ansible can be a little to powerful and can wipe out the infrastructure that you weren't expecting. We can use a short yaml to create a check before running the dangerous commands. The yaml is shown below. This simple script will allow you to double check the hosts you are running the playbooks against. # vim: set ft=yaml: --- - hosts: all gather_facts: false tasks: - local_action: module: ansible.builtin.debug msg: 'Run affects {{ansible_play_hosts|length}} hosts' run_once: true - local_action: module: ansible.builtin.pause prompt: Confirm number of hosts affected register: prompt when: not ansible_check_mode - name: Verify number of affected hosts local_action: module: ansible.builtin.assert that: - '{{prompt.user_input|int}} == {{ansible_play_hosts|length}}' quiet: true run_once: true when: not ansible_check_mode","title":"Useful ansible guard for playbooks"},{"location":"zet/20221212114728/#useful-ansible-guard-for-playbooks","text":"Sometimes ansible can be a little to powerful and can wipe out the infrastructure that you weren't expecting. We can use a short yaml to create a check before running the dangerous commands. The yaml is shown below. This simple script will allow you to double check the hosts you are running the playbooks against. # vim: set ft=yaml: --- - hosts: all gather_facts: false tasks: - local_action: module: ansible.builtin.debug msg: 'Run affects {{ansible_play_hosts|length}} hosts' run_once: true - local_action: module: ansible.builtin.pause prompt: Confirm number of hosts affected register: prompt when: not ansible_check_mode - name: Verify number of affected hosts local_action: module: ansible.builtin.assert that: - '{{prompt.user_input|int}} == {{ansible_play_hosts|length}}' quiet: true run_once: true when: not ansible_check_mode","title":"Useful ansible guard for playbooks"},{"location":"zet/20221212120432/","text":"Run arbitrary ansible roles without tags and main yaml I find a simple ansible playbook that allows the running of single role tasks from your ansible code base. This is useful when you don't want to have a large number of arbitrary playbooks with random roles. The playbook is : --- - hosts: all remote_user: \"{{remote_user|default('root')}}\" roles: - '{{role}}' where to use the playbook we run ansible-playbook -l localhost -e role=<role_name> runrole.yaml","title":"Run arbitrary ansible roles without tags and main yaml"},{"location":"zet/20221212120432/#run-arbitrary-ansible-roles-without-tags-and-main-yaml","text":"I find a simple ansible playbook that allows the running of single role tasks from your ansible code base. This is useful when you don't want to have a large number of arbitrary playbooks with random roles. The playbook is : --- - hosts: all remote_user: \"{{remote_user|default('root')}}\" roles: - '{{role}}' where to use the playbook we run ansible-playbook -l localhost -e role=<role_name> runrole.yaml","title":"Run arbitrary ansible roles without tags and main yaml"},{"location":"zet/20221214064600/","text":"Argh why does uniq not work as i want it Today i figured out that uniq will only compare lines next to each other and to use it correctly with input data we need to sort first.","title":"Argh why does uniq not work as i want it"},{"location":"zet/20221214064600/#argh-why-does-uniq-not-work-as-i-want-it","text":"Today i figured out that uniq will only compare lines next to each other and to use it correctly with input data we need to sort first.","title":"Argh why does uniq not work as i want it"},{"location":"zet/20230115004436/","text":"Go structs and struct methods To create OO like code in Go we attach methods to a struct. An example of this is when you have the below struct type Student struct { name string grades []int age int } We can then attach methods to the struct and this allows us to use the variables in the struct and build functionality around it. For example we have the below method to get the age of the student. func (s Student) getAge() int { return s.age } We can look at the first brackets as the method acts on the object student. To access the method we first need to instantiate the struct to then be able to act on it. To do this we can call. s1 := Student{\"Tim\", []int{20,30,50}, 33} If we want to act on the variables in the struct when we access the struct in a method call we need to use a pointer to be able to change the actual values in the instantiated struct. Below is a simple example of this: func (s *Student) setAge(age int) { s.age = age To use the above method we can call s1.setAge(80) If we don't use the pointer we only change the value whilst inside the method and on returning from the method the field of the struct remains unchanged.","title":"Go structs and struct methods"},{"location":"zet/20230115004436/#go-structs-and-struct-methods","text":"To create OO like code in Go we attach methods to a struct. An example of this is when you have the below struct type Student struct { name string grades []int age int } We can then attach methods to the struct and this allows us to use the variables in the struct and build functionality around it. For example we have the below method to get the age of the student. func (s Student) getAge() int { return s.age } We can look at the first brackets as the method acts on the object student. To access the method we first need to instantiate the struct to then be able to act on it. To do this we can call. s1 := Student{\"Tim\", []int{20,30,50}, 33} If we want to act on the variables in the struct when we access the struct in a method call we need to use a pointer to be able to change the actual values in the instantiated struct. Below is a simple example of this: func (s *Student) setAge(age int) { s.age = age To use the above method we can call s1.setAge(80) If we don't use the pointer we only change the value whilst inside the method and on returning from the method the field of the struct remains unchanged.","title":"Go structs and struct methods"},{"location":"zet/20230131110949/","text":"awk foo There is so much we can do with awk on top of the standard getting of a certain value with { print $3 } . For example we can use regular expressions to search for a value inside the file and only print a column if that line contains the regex. For example awk '/label/ { print $1 }' will print the first column if the line contains the label. We can also send data from a file into awk by placing the file at the end of the command items to input the file rather than piping it from cat. Awk also has the ability to write functions. We can do that with the func keyword.","title":"awk foo"},{"location":"zet/20230131110949/#awk-foo","text":"There is so much we can do with awk on top of the standard getting of a certain value with { print $3 } . For example we can use regular expressions to search for a value inside the file and only print a column if that line contains the regex. For example awk '/label/ { print $1 }' will print the first column if the line contains the label. We can also send data from a file into awk by placing the file at the end of the command items to input the file rather than piping it from cat. Awk also has the ability to write functions. We can do that with the func keyword.","title":"awk foo"},{"location":"zet/20230304231507/","text":"Build conditions in Go We can actually utilise a build condition using the suffix name of a file to build for specific operating systems. For example we can build a file for linux with _linux and mac with _darwin . We can also utilise build tags at the top of a file to build only for specific OS and architectures. For example we can use //+build darwin,!arm64 to build for mac and amd64 chipsets or use arm64 to build for arm.","title":"Build conditions in Go"},{"location":"zet/20230304231507/#build-conditions-in-go","text":"We can actually utilise a build condition using the suffix name of a file to build for specific operating systems. For example we can build a file for linux with _linux and mac with _darwin . We can also utilise build tags at the top of a file to build only for specific OS and architectures. For example we can use //+build darwin,!arm64 to build for mac and amd64 chipsets or use arm64 to build for arm.","title":"Build conditions in Go"},{"location":"zet/20230403075015/","text":"Working with git worktree git worktree has so many useful features.","title":"Working with git worktree"},{"location":"zet/20230403075015/#working-with-git-worktree","text":"git worktree has so many useful features.","title":"Working with git worktree"},{"location":"zet/20230403075141/","text":"Use xvfb for display in CI When running UI code in CI the system will often fail without a display. To fix this we can use xvfb to fake a UI for CI builds. For this to work we need to run export DISPLAY=:99 Xvfb -ac :99 -screen 0 1280x1024x24 > /dev/null 2>&1 & to export the display to a number which is then referenced in the call to xvfb.","title":"Use xvfb for display in CI"},{"location":"zet/20230403075141/#use-xvfb-for-display-in-ci","text":"When running UI code in CI the system will often fail without a display. To fix this we can use xvfb to fake a UI for CI builds. For this to work we need to run export DISPLAY=:99 Xvfb -ac :99 -screen 0 1280x1024x24 > /dev/null 2>&1 & to export the display to a number which is then referenced in the call to xvfb.","title":"Use xvfb for display in CI"},{"location":"zet/20230505123336/","text":"Dynaconf is an awesome configuration manager for python I just found today dynaconf. It has a cli tool to initialise a configuration file with dynaconf init -f toml initialising a toml based config file. When the tool initialises all you need to do is import settings from config and you have access to all your configurations in your application. The great thing about this design is you don't need to pass around your configs into each function.","title":"Dynaconf is an awesome configuration manager for python"},{"location":"zet/20230505123336/#dynaconf-is-an-awesome-configuration-manager-for-python","text":"I just found today dynaconf. It has a cli tool to initialise a configuration file with dynaconf init -f toml initialising a toml based config file. When the tool initialises all you need to do is import settings from config and you have access to all your configurations in your application. The great thing about this design is you don't need to pass around your configs into each function.","title":"Dynaconf is an awesome configuration manager for python"},{"location":"zet/20230512233755/","text":"Update vim to shortcut a zet creation A nice little tool is to bind leader z to auto open a zet create tool. This way i can quickly jot down a zet without having to open up a new terminal or tmux window. map <leader>z :!zet create<cr>","title":"Update vim to shortcut a zet creation"},{"location":"zet/20230512233755/#update-vim-to-shortcut-a-zet-creation","text":"A nice little tool is to bind leader z to auto open a zet create tool. This way i can quickly jot down a zet without having to open up a new terminal or tmux window. map <leader>z :!zet create<cr>","title":"Update vim to shortcut a zet creation"},{"location":"zet/20230512235555/","text":"Using registers in vim and vscode properly Saving to register To save to a register we use the \" symbol followed by a letter or number and y to yank into that register Loading from register To load from a register we use \" followed by the above letter and p . Viewing the register In vim we can view the register with :reg or this can be mapped to something like <leader>r . Macros are stored in vim registers One thing to note is if you use a lot of macros you can overwrite them by saving to the register with your yank.","title":"Using registers in vim and vscode properly"},{"location":"zet/20230512235555/#using-registers-in-vim-and-vscode-properly","text":"","title":"Using registers in vim and vscode properly"},{"location":"zet/20230512235555/#saving-to-register","text":"To save to a register we use the \" symbol followed by a letter or number and y to yank into that register","title":"Saving to register"},{"location":"zet/20230512235555/#loading-from-register","text":"To load from a register we use \" followed by the above letter and p .","title":"Loading from register"},{"location":"zet/20230512235555/#viewing-the-register","text":"In vim we can view the register with :reg or this can be mapped to something like <leader>r .","title":"Viewing the register"},{"location":"zet/20230512235555/#macros-are-stored-in-vim-registers","text":"One thing to note is if you use a lot of macros you can overwrite them by saving to the register with your yank.","title":"Macros are stored in vim registers"},{"location":"zet/20230717100116/","text":"Semaphores in programming The semaphore in programming is used as a method to avoid critical sections of code having a race condition. Semaphores can be seen as a kind of synchronisation primitive. The semaphore keeps track of how many resources are free and not which resources are free. We can also think of semaphores as flags marking something as available for use when the flag is set to true.","title":"Semaphores in programming"},{"location":"zet/20230717100116/#semaphores-in-programming","text":"The semaphore in programming is used as a method to avoid critical sections of code having a race condition. Semaphores can be seen as a kind of synchronisation primitive. The semaphore keeps track of how many resources are free and not which resources are free. We can also think of semaphores as flags marking something as available for use when the flag is set to true.","title":"Semaphores in programming"}]}